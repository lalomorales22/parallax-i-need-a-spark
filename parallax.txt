
================================================================================
File: .gitattributes
Size: 302 B
================================================================================


* text=auto

*.js text eol=lf
*.py text eol=lf
*.ts text eol=lf
*.html text eol=lf
*.css text eol=lf
*.json text eol=lf
*.md text eol=lf
*.sh text eol=lf

*.png binary
*.jpg binary
*.jpeg binary
*.gif binary
*.ico binary
*.mov binary
*.mp4 binary
*.pdf binary

*.bat text eol=crlf
*.cmd text eol=crlf


================================================================================
File: .github/workflows/build-images.yaml
Size: 1.39 kB
================================================================================

name: Build Images

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:


env:
  IMAGE_NAME: parallax
  NAMESPACE: gradientservice

jobs:
  build:
    runs-on: arc-public-parallax
    permissions:
      contents: read
      packages: write
      id-token: write
      attestations: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver: remote
        endpoint: tcp://buildkit-buildkit-service.arc-systems:1234


    - name: Log in to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.NAMESPACE }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64


================================================================================
File: .github/workflows/build-spark-image.yaml
Size: 1.55 kB
================================================================================

name: Build Spark Image

on:
  schedule:
    - cron: '0 3 * * *'
  workflow_dispatch:


env:
  IMAGE_NAME: parallax
  NAMESPACE: gradientservice

jobs:
  build:
    runs-on: arc-public-parallax
    permissions:
      contents: read
      packages: write
      id-token: write
      attestations: write
    strategy:
      matrix:
        variant: [spark]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver: remote
        endpoint: tcp://buildkit-buildkit-service.arc-systems:1234


    - name: Log in to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.NAMESPACE }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch,suffix=-${{ matrix.variant }}
          type=ref,event=pr,suffix=-${{ matrix.variant }}
          type=raw,value=latest-${{ matrix.variant }},enable={{is_default_branch}}

    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile.${{ matrix.variant }}
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/arm64


================================================================================
File: .github/workflows/ci.yml
Size: 4.71 kB
================================================================================

name: CI

on:
  pull_request:
    branches: [ main ]

jobs:
  test:
    name: Test on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            python-version: '3.11'
            # Install gpu (sglang) and dev dependencies for Linux
            extras: 'gpu, dev'
          - os: macos-latest
            python-version: '3.11'
            # Install mac (mlx) and dev dependencies for macOS
            extras: 'mac, dev'

    steps:
    - name: Free Disk Space (Ubuntu)
      if: runner.os == 'Linux'
      uses: jlumbroso/free-disk-space@v1.3.1
      with:
        tool-cache: false
        android: true
        dotnet: true
        haskell: true
        large-packages: true
        docker-images: true
        swap-storage: false

    - name: Checkout code
      uses: actions/checkout@v4

    - name: Check for file changes
      id: changes
      uses: dorny/paths-filter@v2
      with:
        filters: |
          src:
            - 'src/**'
            - 'tests/**'
            - 'pyproject.toml'

    - name: Set up Python ${{ matrix.python-version }}
      if: steps.changes.outputs.src == 'true'
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      if: steps.changes.outputs.src == 'true'
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      if: steps.changes.outputs.src == 'true'
      run: |
        python -m pip install --upgrade pip
        # Install extras dependencies based on matrix variable
        pip install -e ".[${{ matrix.extras }}]"

    - name: Run Unit Tests (macOS only)
      if: steps.changes.outputs.src == 'true' && runner.os == 'macOS'
      shell: bash
      run: |
        pytest tests/ -v --cov=src/parallax --cov-report=xml

    - name: Upload coverage to Codecov
      if: steps.changes.outputs.src == 'true' && runner.os == 'macOS'
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
        token: ${{ secrets.CODECOV_TOKEN }}

    - name: Run E2E tests (macOS only)
      if: steps.changes.outputs.src == 'true' && runner.os == 'macOS'
      shell: bash
      run: |
        # Start the server
        python src/parallax/launch.py \
          --model-path Qwen/Qwen3-0.6B \
          --max-num-tokens-per-batch 16384 \
          --kv-block-size 1024 \
          --max-batch-size 128 \
          --start-layer 0 \
          --end-layer 28 &
        PID=$!

        echo "Waiting for server to start..."
        # Poll to check if the port is ready (wait up to 60 seconds)
        for i in {1..30}; do
          # If curl succeeds (200) or returns 405 (Method Not Allowed), the port is open
          if curl -s -o /dev/null -w "%{http_code}" http://localhost:3000/v1/chat/completions | grep -qE "200|400|405"; then
             echo "Server is up!"
             break
          fi

          # Check if the process is still alive
          if ! kill -0 $PID 2>/dev/null; then
             echo "Server process died prematurely"
             exit 1
          fi

          if [ $i -eq 30 ]; then
             echo "Server failed to start within 60 seconds"
             kill $PID 2>/dev/null
             exit 1
          fi
          sleep 2
        done

        echo "Sending test request..."
        # Capture the response
        RESPONSE=$(curl --fail --silent --show-error --location 'http://localhost:3000/v1/chat/completions' \
          --header 'Content-Type: application/json' \
          --data '{
            "messages": [
              {
                "role": "user",
                "content": "What is the capital of France"
              }
            ],
            "stream": false,
            "max_tokens": 1024,
            "chat_template_kwargs": {"enable_thinking": false},
            "sampling_params": {
                "top_k": 3
            }
        }')

        echo "Response received:"
        echo "$RESPONSE"

        # Check if the response contains "Paris" (case-insensitive)
        if echo "$RESPONSE" | grep -iq "Paris"; then
            echo "Test passed: Response contains 'Paris'"
        else
            echo "Test failed: Response does not contain 'Paris'"
            kill $PID 2>/dev/null || true
            exit 1
        fi

        # Clean up process
        kill $PID 2>/dev/null || true


================================================================================
File: .github/workflows/commit-check.yaml
Size: 187 B
================================================================================


name: Commit Message Lint

on:
  pull_request:
    types: [opened, synchronize, edited, reopened]

jobs:
  commitlint:
    uses: GradientHQ/.github/.github/workflows/commitlint.yml@main


================================================================================
File: .github/workflows/pre-commit.yml
Size: 973 B
================================================================================

name: Pre-commit Checks

on:
  pull_request:

jobs:
  pre-commit:
    runs-on: macos-15
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Cache pre-commit
      uses: actions/cache@v3
      with:
        path: ~/.cache/pre-commit
        key: ${{ runner.os }}-pre-commit-${{ hashFiles('.pre-commit-config.yaml') }}
        restore-keys: |
          ${{ runner.os }}-pre-commit-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pre-commit
        pre-commit install

    - name: Run pre-commit on all files
      run: pre-commit run --all-files


================================================================================
File: .gitignore
Size: 105 B
================================================================================

.idea/
*.iml
# .vscode/
*~
__pycache__/
*.egg-info/
build/
# dist/
.venv
.DS_Store
*.key
.cache
.vscode/


================================================================================
File: .pre-commit-config.yaml
Size: 929 B
================================================================================

repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0
    hooks:
      - id: check-yaml
      - id: end-of-file-fixer
        exclude: '^src/frontend/dist/|\.svg$|\.png$|\.jpg$|\.jpeg$|\.gif$|\.webp$'
      - id: trailing-whitespace
      - id: mixed-line-ending
        args: ['--fix=lf']

  - repo: https://github.com/PyCQA/autoflake
    rev: v2.3.1
    hooks:
      - id: autoflake
        args:
          - --remove-all-unused-imports
          - --remove-unused-variables
          - --in-place

  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
        language_version: python3.11
        args: ["--profile", "black"]
        exclude: '^src/parallax/p2p/proto/forward_pb2\.py$'

  - repo: https://github.com/psf/black
    rev: 24.3.0
    hooks:
      - id: black
        language_version: python3.11
        exclude: '^src/parallax/p2p/proto/forward_pb2\.py$'


================================================================================
File: LICENSE
Size: 11.36 kB
================================================================================

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


================================================================================
File: README.md
Size: 8.15 kB
================================================================================

<div align="center">
  <p align="center">
    <img src="docs/images/parallax.png" width="720">
    <div align="center">
      <p style="font-size: 1.3em; font-weight: 600; margin-bottom: 10px;">Trusted by Partners</p>
      <img src="docs/images/qwen.avif" alt="Qwen" height="30" style="margin: 0 20px;">
      <img src="docs/images/sglang.png" alt="SGLang" height="28" style="margin: 0 20px;">
      <img src="docs/images/kimi.png" alt="Kimi" height="30" style="margin: 0 20px;">
      <img src="docs/images/minimax.png" alt="Minimax" height="30" style="margin: 0 10px;">
      <img src="docs/images/zai.svg"za alt="ZAI" height="30" style="margin: 0 10px;"/>
    </div>
  </p>

[![license](https://img.shields.io/github/license/GradientHQ/parallax.svg)](https://github.com/GradientHQ/parallax/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/GradientHQ/parallax)](https://github.com/GradientHQ/parallax/issues)
[![open issues](https://img.shields.io/github/issues-raw/GradientHQ/parallax)](https://github.com/GradientHQ/parallax/issues)

<a href="https://www.producthunt.com/products/parallax-by-gradient?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_source=badge-parallax&#0045;by&#0045;gradient" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=1030922&theme=light&period=daily&t=1761986433128" alt="Parallax&#0032;by&#0032;Gradient - Host&#0032;LLMs&#0032;across&#0032;devices&#0032;sharing&#0032;GPU&#0032;to&#0032;make&#0032;your&#0032;AI&#0032;go&#0032;brrr | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /></a>

</div>

| [**Gradient**](https://gradient.network)
| [**Blog**](https://gradient.network/blog/parallax-the-sovereign-ai-os)
| [**X(Twitter)**](https://x.com/Gradient_HQ)
| [**Discord**](https://discord.gg/parallax)
| [**Arxiv**](https://arxiv.org/pdf/2509.26182v1)

## News
- [2025/10] ðŸ”¥ Parallax won #1 Product of The Day on Product Hunt!
- [2025/10] ðŸ”¥ Parallax version 0.0.1 has been released!

## About
A fully decentralized inference engine developed by [Gradient](https://gradient.network). Parallax lets you build your own AI cluster for model inference onto a set of distributed nodes despite their varying configuration and physical location. Its core features include:

- **Host local LLM on personal devices**
- **Cross-platform support**
- **Pipeline parallel model sharding**
- **Dynamic KV cache management & continuous batching for Mac**
- **Dynamic request scheduling and routing for high performance**

The backend architecture:

* P2P communication powered by [Lattica](https://github.com/GradientHQ/lattica)
* GPU backend powered by [SGLang](https://github.com/sgl-project/sglang)
* MAC backend powered by [MLX LM](https://github.com/ml-explore/mlx-lm)

## User Guide

- [Installation](./docs/user_guide/install.md)
- [Getting Started](./docs/user_guide/quick_start.md)

## Contributing

We warmly welcome contributions of all kinds! For guidelines on how to get involved, please refer to our [Contributing Guide](./docs/CONTRIBUTING.md).

## Supported Models

|              | Provider     | HuggingFace Collection  |  Blog  | Description |
|:-------------|:-------------|:----------------------------:|:----------------------------:|:----------------------------|
|DeepSeek      | Deepseek     | [DeepSeek-V3.1](https://huggingface.co/collections/deepseek-ai/deepseek-v31) <br>[DeepSeek-R1](https://huggingface.co/collections/deepseek-ai/deepseek-r1) <br>[DeepSeek-V3](https://huggingface.co/collections/deepseek-ai/deepseek-v3) <br>[DeepSeek-V2](https://huggingface.co/collections/deepseek-ai/deepseek-v2) | [DeepSeek V3.1: The New Frontier in Artificial Intelligence](https://deepseek.ai/blog/deepseek-v31) | "DeepSeek" is an advanced large language model series from Deepseek AI, offering multiple generations such as DeepSeek-V3.1, DeepSeek-R1, DeepSeek-V2, and DeepSeek-V3. These models are designed for powerful natural language understanding and generation, with various sizes and capabilities for research and production use. |
|MiniMax-M2    | MiniMax AI  | [MiniMax-M2](https://huggingface.co/MiniMaxAI/MiniMax-M2) | [MiniMax M2 & Agent: Ingenious in Simplicity](https://www.minimax.io/news/minimax-m2) | MiniMax-M2 is a compact, fast, and cost-effective MoE model (230B parameters, 10B active) built for advanced coding and agentic workflows. It offers state-of-the-art intelligence and coding abilities, delivering efficient, reliable tool use and strong multi-step reasoning for developers and agents, with high throughput and low latency for easy deployment. |
|GLM-4.6       | Z AI | [GLM-4.6](https://huggingface.co/zai-org/GLM-4.6) | [GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities](https://z.ai/blog/glm-4.6) | GLM-4.6 improves upon GLM-4.5 with a longer 200K token context window, stronger coding and reasoning performance, enhanced tool-use and agent integration, and refined writing quality. Outperforms previous versions and is highly competitive with leading open-source models across coding, reasoning, and agent benchmarks. |
|Kimi-K2       | Moonshot AI  | [Kimi-K2](https://huggingface.co/collections/moonshotai/kimi-k2-6871243b990f2af5ba60617d) | [Kimi K2: Open Agentic Intelligence](https://moonshotai.github.io/Kimi-K2/) | "Kimi-K2" is Moonshot AI's Kimi-K2 model family, including Kimi-K2-Base, Kimi-K2-Instruct and Kimi-K2-Thinking. Kimi K2 Thinking is a state-of-the-art open-source agentic model designed for deep, step-by-step reasoning and dynamic tool use. It features native INT4 quantization and a 256k context window for fast, memory-efficient inference. Uniquely stable in long-horizon tasks, Kimi K2 enables reliable autonomous workflows with consistent performance across hundreds of tool calls.
|Qwen          | Qwen         | [Qwen3-Next](https://huggingface.co/collections/Qwen/qwen3-next-68c25fd6838e585db8eeea9d) <br>[Qwen3](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) <br>[Qwen2.5](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)| [Qwen3-Next: Towards Ultimate Training & Inference Efficiency](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list) | The Qwen series is a family of large language models developed by Alibaba's Qwen team. It includes multiple generations such as Qwen2.5, Qwen3, and Qwen3-Next, which improve upon model architecture, efficiency, and capabilities. The models are available in various sizes and instruction-tuned versions, with support for cutting-edge features like long context and quantization. Suitable for a wide range of language tasks and open-source use cases. |
|gpt-oss       | OpenAI       | [gpt-oss](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) <br>[gpt-oss-safeguard](https://huggingface.co/collections/openai/gpt-oss-safeguard) | [Introducing gpt-oss-safeguard](https://openai.com/index/introducing-gpt-oss-safeguard/) | gpt-oss are OpenAIâ€™s open-weight GPT models (20B & 120B). The gpt-oss-safeguard variants are reasoning-based safety classification models: developers provide their own policy at inference, and the model uses chain-of-thought to classify content and explain its reasoning. This allows flexible, policy-driven moderation in complex or evolving domains, with open weights under Apache 2.0. |
|Meta Llama 3  | Meta         | [Meta Llama 3](https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6) <br>[Llama 3.1](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f) <br>[Llama 3.2](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf) <br>[Llama 3.3](https://huggingface.co/collections/meta-llama/llama-33-67531d5c405ec5d08a852000) | [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/) | "Meta Llama 3" is Meta's third-generation Llama model, available in sizes such as 8B and 70B parameters. Includes instruction-tuned and quantized (e.g., FP8) variants. |


================================================================================
File: docker/Dockerfile
Size: 190 B
================================================================================

FROM lmsysorg/sglang:v0.5.5

ENV SGLANG_ENABLE_JIT_DEEPGEMM=0

WORKDIR /parallax

COPY README.md ./README.md
COPY src ./src
COPY pyproject.toml ./pyproject.toml

RUN pip install -e '.[gpu]'


================================================================================
File: docker/Dockerfile.spark
Size: 552 B
================================================================================

FROM lmsysorg/sglang:spark

ENV SGLANG_ENABLE_JIT_DEEPGEMM=0

WORKDIR /parallax

COPY README.md ./README.md
COPY src ./src
COPY pyproject.toml ./pyproject.toml

RUN apt-get update

RUN git clone --branch v0.29.1 https://github.com/ml-explore/mlx.git && \
    apt-get install libblas-dev liblapack-dev liblapacke-dev -y && \
    cd mlx && \
    PYPI_RELEASE=1 pip install . --break-system-packages && \
    cd ..

# remove sglang[all] from pyproject.toml
RUN sed -i '/sglang\[all\]/d' pyproject.toml

RUN pip install -e '.[gpu]' --break-system-packages


================================================================================
File: docs/CONTRIBUTING.md
Size: 4.74 kB
================================================================================

# Contributing Guide

Welcome to **Parallax**, we're glad you're interested in contributing! We welcome and accept all kinds of contributions, no matter how small or large. Here are some common ways you can contribute to Parallax:
  - Find and report bugs.
  - Request or suggest new features.
  - Enhance documentation and guides.

We encourage everyoneâ€”first-time contributors and experienced developers alikeâ€”to follow these best practices for a smooth collaboration process.

If you have any questions, feel free to ask in our [Discord channel](https://discord.gg/parallax).

## How Can I Contribute?

Before you start contributing, we recommend browsing existing issues to find something to work on. Once youâ€™ve chosen an issue, developed your code, or updated any documents, submit a pull request; maintainers will review and eventually merge your changes. If you want to introduce a new feature or have discovered a bug, itâ€™s a good idea to create an issue first and discuss it with the maintainers before proceeding.

Here is a typical step-by-step process for contributing to Parallax:
- **Fork the repository** to your own GitHub account.
- **Create a new branch** for your changes.
- **Check and format your code** using tools such as `pre-commit` to ensure style consistency.
- **Run the unit tests** and confirm that no tests are broken by your changes.
- **Open a pull request** (PR) with a clear description of your contribution and any relevant context.

By following this process, you help ensure a smooth review and integration of your contributions!

## Set up your dev environment

### Fork and clone the repository

**Note:** As a new contributor, you do **not** have write access to the official Parallax repository. Please fork the [Parallax repository](https://github.com/GradientHQ/parallax) to your own GitHub account. After forking, clone your fork locally for development:

```bash
git clone https://github.com/<your_github_username>/parallax.git
```

### Create a branch
Create a new branch for your work to keep changes separate from the main branch. Use a clear, descriptive name, such as `feat/add-mock-support` or `fix/api-timeout-bug`:

```bash
git checkout -b <your-branch-name>
```

Example:

```bash
git checkout -b feat/add-mock-support
```

Work on your feature in this branch.


### Installation

Refer to [Installation](../README.md#installation).

### Code Formatting with pre-commit

To maintain a consistent code style, we use [pre-commit](https://pre-commit.com/) in this project. Please follow the steps below before submitting your changes:

**1. Install and set up pre-commit:**
```bash
pip3 install pre-commit
pre-commit install
```

**2. Run pre-commit to check and format your code before each commit:**
```bash
pre-commit run --all-files
```

This will help ensure your code adheres to the project's standards and reduces formatting-related review comments.

### Push to your remote branch

After committing your changes, push your branch to your forked remote repository:

```bash
git push origin <your-branch-name>
```

Replace `<your-branch-name>` with the name of the branch you created. This makes your changes available on GitHub so you can open a pull request.

### Unit test
Before submitting your changes, make sure to add or update unit tests as appropriate for your contribution.

- **Add tests:** If you are introducing new features or fixing bugs, add relevant tests to verify the expected behavior.
- **Update tests:** Whenever you modify the existing codebase, ensure affected tests are updated accordingly.
- **Test location:** Place your tests in the `tests/` directory.
- **Test locally:** Run all tests locally to ensure they pass before pushing your branch.

### Create PR

Once your contribution is ready, please open a pull request (PR) following the [standard GitHub workflow](https://help.github.com/en/articles/about-pull-requests). Use the provided PR template to clearly describe your changes and give maintainers the necessary context.

To make it easier for others to understand the nature of your PR, always categorize your changes by adding a suitable prefix to the PR title. Choose one of the following prefixes to indicate the type of contribution:
- feat:   New feature.
- fix:    Bug fix.
- docs:   Documentation only changes.
- refactor: A code change that neither fixes a bug nor adds a feature.
- perf:   Performance improvement.
- test:   Adding missing tests or correcting existing tests.
- chore:  Maintenance tasks (e.g., updating dependencies).

Prefix your pull request title accordingly for easy classification and review.

## Thank You
Thank you for contributing to Parallax!

Your efforts help make this project better for everyone.


================================================================================
File: docs/images/zai.svg
Size: 11.04 kB
================================================================================

<?xml version="1.0" encoding="utf-8"?>
<!-- Generator: Adobe Illustrator 25.3.1, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="0 0 30 30" style="enable-background:new 0 0 30 30;" xml:space="preserve">
<style type="text/css">
	.st0{opacity:0.3;fill:#E2E4E7;}
	.st1{opacity:0.8;fill:#E2E4E7;stroke:#FFFFFF;stroke-width:5;stroke-miterlimit:10;}
	.st2{fill:url(#SVGID_1_);}
	.st3{fill:none;stroke:#E0E4E9;stroke-width:0.25;stroke-miterlimit:10;}
	.st4{fill:none;}
	.st5{fill:#9DA1A5;}
	.st6{fill-rule:evenodd;clip-rule:evenodd;fill:none;}
	.st7{fill-rule:evenodd;clip-rule:evenodd;fill:#DFE2E7;}
	.st8{fill-rule:evenodd;clip-rule:evenodd;fill:#CDD4DA;}
	.st9{fill-rule:evenodd;clip-rule:evenodd;fill:#B3BCC7;}
	.st10{fill-rule:evenodd;clip-rule:evenodd;fill:#9DAAB7;}
	.st11{fill-rule:evenodd;clip-rule:evenodd;fill:#8698A8;}
	.st12{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_2_);}
	.st13{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_3_);}
	.st14{fill:#1F63EC;}
	.st15{fill:#2D2D2D;}
	.st16{fill:none;stroke:#E0E4E9;stroke-width:0.5;stroke-miterlimit:10;}
	.st17{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_4_);}
	.st18{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_5_);}
	.st19{fill:none;stroke:#677380;stroke-width:0.5;stroke-miterlimit:10;}
	.st20{fill:none;stroke:url(#SVGID_6_);stroke-width:2;stroke-miterlimit:10;}
	.st21{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_7_);}
	.st22{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_8_);}
	.st23{fill:#FFFFFF;}
	.st24{fill-rule:evenodd;clip-rule:evenodd;fill:#2D2D2D;}
	.st25{clip-path:url(#SVGID_10_);}
	.st26{clip-path:url(#SVGID_12_);}
	.st27{fill:url(#SVGID_13_);}
	.st28{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_14_);}
	.st29{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_15_);}
	.st30{clip-path:url(#SVGID_17_);}
	.st31{clip-path:url(#SVGID_19_);}
	.st32{fill:url(#SVGID_20_);}
	.st33{fill:none;stroke:url(#SVGID_21_);stroke-width:2;stroke-miterlimit:10;}
	.st34{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_22_);}
	.st35{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_23_);}
	.st36{clip-path:url(#SVGID_25_);}
	.st37{clip-path:url(#SVGID_27_);}
	.st38{fill:url(#SVGID_28_);}
	.st39{clip-path:url(#SVGID_30_);}
	.st40{clip-path:url(#SVGID_32_);}
	.st41{fill:url(#SVGID_33_);}
	.st42{fill-rule:evenodd;clip-rule:evenodd;fill:#126EF6;}
	.st43{fill-rule:evenodd;clip-rule:evenodd;fill:#FFFFFF;}
	.st44{clip-path:url(#SVGID_35_);}
	.st45{clip-path:url(#SVGID_37_);}
	.st46{fill:url(#SVGID_38_);}
	.st47{fill-rule:evenodd;clip-rule:evenodd;fill:#9DA1A5;}
	.st48{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_39_);}
	.st49{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_40_);}
	.st50{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_41_);}
	.st51{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_42_);}
	.st52{fill:none;stroke:url(#SVGID_43_);stroke-width:2;stroke-miterlimit:10;}
	.st53{fill-rule:evenodd;clip-rule:evenodd;fill:none;stroke:#E0E4E9;stroke-width:0.5;stroke-miterlimit:10;}
	.st54{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_44_);}
	.st55{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_45_);}
	.st56{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_46_);}
	.st57{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_47_);}
	.st58{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_48_);}
	.st59{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_49_);}
	.st60{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_50_);}
	.st61{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_51_);}
	.st62{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_52_);}
	.st63{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_53_);}
	.st64{clip-path:url(#SVGID_55_);}
	.st65{clip-path:url(#SVGID_57_);}
	.st66{fill:url(#SVGID_58_);}
	.st67{clip-path:url(#SVGID_60_);}
	.st68{clip-path:url(#SVGID_62_);}
	.st69{fill:url(#SVGID_63_);}
	.st70{fill:none;stroke:url(#SVGID_64_);stroke-width:2;stroke-miterlimit:10;}
	.st71{clip-path:url(#SVGID_66_);}
	.st72{clip-path:url(#SVGID_68_);}
	.st73{fill:url(#SVGID_69_);}
	.st74{clip-path:url(#SVGID_71_);}
	.st75{clip-path:url(#SVGID_73_);}
	.st76{fill:url(#SVGID_74_);}
	.st77{clip-path:url(#SVGID_76_);}
	.st78{clip-path:url(#SVGID_78_);}
	.st79{fill:url(#SVGID_79_);}
	.st80{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_80_);}
	.st81{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_81_);}
	.st82{clip-path:url(#SVGID_83_);}
	.st83{clip-path:url(#SVGID_85_);}
	.st84{fill:url(#SVGID_86_);}
	.st85{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_87_);}
	.st86{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_88_);}
	.st87{clip-path:url(#SVGID_90_);}
	.st88{clip-path:url(#SVGID_92_);}
	.st89{fill:url(#SVGID_93_);}
	.st90{fill:none;stroke:url(#SVGID_94_);stroke-width:2;stroke-miterlimit:10;}
	.st91{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_95_);}
	.st92{fill-rule:evenodd;clip-rule:evenodd;fill:url(#SVGID_96_);}
	.st93{clip-path:url(#SVGID_98_);}
	.st94{clip-path:url(#SVGID_100_);}
	.st95{fill:url(#SVGID_101_);}
	.st96{clip-path:url(#SVGID_103_);}
	.st97{clip-path:url(#SVGID_105_);}
	.st98{fill:url(#SVGID_106_);}
	.st99{clip-path:url(#SVGID_108_);}
	.st100{clip-path:url(#SVGID_110_);}
	.st101{fill:url(#SVGID_111_);}
	.st102{fill:#FFFFFF;stroke:#B3BCC7;stroke-width:0.275;stroke-miterlimit:10;}
	.st103{clip-path:url(#SVGID_113_);}
	.st104{fill:#FDD138;}
	.st105{fill:#FCA62F;}
	.st106{fill:#FB7927;}
	.st107{fill:#F44B22;}
	.st108{fill:#D81915;}
	.st109{fill:#2D2D2D;stroke:#FFFFFF;stroke-width:0.3354;stroke-miterlimit:10;}
	.st110{fill:none;stroke:#65727F;stroke-width:2;stroke-miterlimit:10;}
	.st111{fill:none;stroke:#65727F;stroke-width:0.75;stroke-miterlimit:10;}
	.st112{fill:url(#SVGID_114_);}
	.st113{fill:#D06C50;}
	.st114{fill:#2D2D2D;stroke:#B3BCC7;stroke-width:0.275;stroke-miterlimit:10;}
	.st115{opacity:0.2;}
	.st116{fill:none;stroke:#677380;stroke-width:0.3564;stroke-miterlimit:10;}
	.st117{fill:none;stroke:#677380;stroke-width:0.3564;stroke-miterlimit:10;stroke-dasharray:1.0212,1.0212;}
	.st118{fill:none;stroke:#677380;stroke-width:0.3564;stroke-miterlimit:10;stroke-dasharray:1.0205,1.0205;}
	.st119{opacity:0.2;fill:none;}
	.st120{fill:none;stroke:#677380;stroke-width:0.3689;stroke-miterlimit:10;}
	.st121{fill:none;stroke:#677380;stroke-width:0.3689;stroke-miterlimit:10;stroke-dasharray:1.0509,1.0509;}
	.st122{opacity:0.3;fill:#1F63EC;}
	.st123{fill:#2D2D2D;stroke:#FFFFFF;stroke-width:0.3162;stroke-miterlimit:10;}
	.st124{fill:#FFFFFF;stroke:#B3BCC7;stroke-width:0.3162;stroke-miterlimit:10;}
	.st125{clip-path:url(#SVGID_118_);}
	.st126{fill:url(#SVGID_119_);}
	.st127{fill:none;stroke:#DFE2E7;stroke-width:0.75;stroke-miterlimit:10;}
	.st128{fill:#9DA1A5;stroke:#FFFFFF;stroke-miterlimit:10;}
	.st129{fill:url(#SVGID_120_);}
	.st130{fill:none;stroke:#677380;stroke-width:0.75;stroke-miterlimit:10;}
	.st131{opacity:0.4;}
	.st132{clip-path:url(#SVGID_122_);}
	.st133{clip-path:url(#SVGID_124_);}
	.st134{fill:url(#SVGID_125_);}
	.st135{fill:none;stroke:#8392A3;stroke-width:0.35;stroke-miterlimit:10;}
	.st136{fill:none;stroke:#8392A3;stroke-width:0.35;stroke-miterlimit:10;stroke-dasharray:0.9951,0.9951;}
	.st137{fill:none;stroke:#8392A3;stroke-width:0.35;stroke-miterlimit:10;stroke-dasharray:1.004,1.004;}
	.st138{fill:none;stroke:url(#SVGID_126_);stroke-width:1.5;stroke-miterlimit:10;}
	.st139{fill:url(#SVGID_127_);}
	.st140{fill:none;stroke:#DDE0E4;stroke-width:0.35;stroke-miterlimit:10;}
	.st141{fill:#2D2D2D;stroke:#A9B3BE;stroke-width:0.275;stroke-miterlimit:10;}
	.st142{fill-rule:evenodd;clip-rule:evenodd;fill:#126EF4;}
	.st143{fill:#FFFFFF;stroke:#B1BAC4;stroke-width:0.275;stroke-miterlimit:10;}
	.st144{fill:#CE6C50;}
	.st145{fill:#5B5B5B;}
	.st146{fill:#8392A3;}
	.st147{fill:none;stroke:url(#SVGID_128_);stroke-width:1.5;stroke-miterlimit:10;}
	.st148{fill:url(#SVGID_129_);}
	.st149{fill:none;stroke:#B5BDC4;stroke-width:0.7;stroke-miterlimit:10;}
	.st150{opacity:0.6;fill:none;stroke:#78838E;stroke-width:0.35;stroke-miterlimit:10;}
	.st151{opacity:0.2;fill:none;stroke:#8392A3;stroke-width:0.35;stroke-miterlimit:10;stroke-dasharray:1,1;}
	.st152{fill:none;stroke:#DDE0E4;stroke-width:0.75;stroke-miterlimit:10;}
	.st153{fill:none;stroke:#8392A3;stroke-width:0.5;stroke-miterlimit:10;}
	.st154{opacity:0.2;fill:none;stroke:#677380;stroke-width:0.3564;stroke-miterlimit:10;stroke-dasharray:1.0182,1.0182;}
	.st155{fill:none;stroke:#DDE0E4;stroke-width:0.765;stroke-miterlimit:10;}
	.st156{fill:url(#SVGID_130_);}
	.st157{fill:url(#SVGID_131_);}
	.st158{fill:#B1BAC4;}
	.st159{fill:#CBD1D8;}
	.st160{fill:#0B1B2B;}
	.st161{fill:#91D119;}
	.st162{opacity:0.7;}
	.st163{fill:#FFFFFF;stroke:#000000;stroke-width:0.4418;stroke-miterlimit:10;}
	.st164{fill:none;stroke:#939CAA;stroke-width:0.2209;stroke-miterlimit:10;}
	.st165{fill:none;stroke:#FFFFFF;stroke-width:3.0924;stroke-miterlimit:10;}
	.st166{fill:url(#SVGID_132_);}
	.st167{fill:none;stroke:url(#SVGID_133_);stroke-width:1.714;stroke-miterlimit:10;}
	.st168{fill:url(#SVGID_134_);}
	.st169{fill:url(#SVGID_135_);}
	.st170{fill:url(#SVGID_136_);}
	.st171{fill:url(#SVGID_137_);}
	.st172{fill:url(#SVGID_138_);}
	.st173{fill:url(#SVGID_139_);}
	.st174{fill:url(#SVGID_140_);}
	.st175{fill:url(#SVGID_141_);}
	.st176{fill:url(#SVGID_142_);}
	.st177{fill:url(#SVGID_143_);}
	.st178{fill:url(#SVGID_144_);}
	.st179{fill:none;stroke:#1F63EC;stroke-width:4;stroke-miterlimit:10;}
	.st180{fill:none;stroke:#0B1B2B;stroke-width:4;stroke-miterlimit:10;}
	.st181{fill:none;stroke:#677380;stroke-width:0.3989;stroke-miterlimit:10;}
	.st182{fill:none;stroke:#677380;stroke-width:0.3989;stroke-miterlimit:10;stroke-dasharray:1.14,1.14;}
	.st183{fill:#257AF1;}
	.st184{opacity:0.3;fill:#FFFFFF;}
	.st185{fill:none;stroke:#98A5B2;stroke-width:4;stroke-miterlimit:10;}
	.st186{fill:none;stroke:#65727F;stroke-width:0.3989;stroke-miterlimit:10;}
	.st187{fill:none;stroke:#65727F;stroke-width:0.3989;stroke-miterlimit:10;stroke-dasharray:1.14,1.14;}
	.st188{fill:none;stroke:#DDDFE4;stroke-width:0.75;stroke-miterlimit:10;}
	.st189{fill:#9A9EA2;}
	.st190{fill-rule:evenodd;clip-rule:evenodd;fill:#3267AC;}
	.st191{fill:#FFFFFF;stroke:#AFB8C3;stroke-width:0.275;stroke-miterlimit:10;}
	.st192{fill:#C5694E;}
	.st193{fill:#8192A2;}
	.st194{fill:#2D2D2D;stroke:#FFFFFF;stroke-width:0.6317;stroke-miterlimit:10;}
</style>
<g id="å›¾å±‚_2">
</g>
<g id="å›¾å±‚_1">
	<path class="st15" d="M24.51,28.51H5.49c-2.21,0-4-1.79-4-4V5.49c0-2.21,1.79-4,4-4h19.03c2.21,0,4,1.79,4,4v19.03
		C28.51,26.72,26.72,28.51,24.51,28.51z"/>
	<g>
		<g>
			<g>
				<g>
					<path class="st23" d="M15.47,7.1l-1.3,1.85c-0.2,0.29-0.54,0.47-0.9,0.47h-7.1V7.09C6.16,7.1,15.47,7.1,15.47,7.1z"/>
					<polygon class="st23" points="24.3,7.1 13.14,22.91 5.7,22.91 16.86,7.1 					"/>
					<path class="st23" d="M14.53,22.91l1.31-1.86c0.2-0.29,0.54-0.47,0.9-0.47h7.09v2.33H14.53z"/>
				</g>
			</g>
		</g>
	</g>
</g>
</svg>


================================================================================
File: docs/user_guide/install.md
Size: 4.01 kB
================================================================================


## Installation

### Prerequisites
- Python>=3.11.0,<3.14.0
- Ubuntu-24.04 for Blackwell GPUs

Below are installation methods for different operating systems.

|  Operating System  |  Windows App  |  From Source | Docker |
|:-------------|:----------------------------:|:----------------------------:|:----------------------------:|
|Windows       | âœ…ï¸ | Not recommended | Not recommended |
|Linux | âŒï¸ | âœ…ï¸ | âœ…ï¸ |
|macOS | âŒï¸ | âœ…ï¸ | âŒï¸ |

### From Source
#### For Linux/WSL (GPU):
Note: If you are using DGX Spark, please refer to the Docker installation section
```sh
git clone https://github.com/GradientHQ/parallax.git
cd parallax
pip install -e '.[gpu]'
```

#### For macOS (Apple silicon):

We recommend macOS users to create an isolated Python virtual environment before installation.

```sh
git clone https://github.com/GradientHQ/parallax.git
cd parallax

# Enter Python virtual environment
python3 -m venv ./venv
source ./venv/bin/activate

pip install -e '.[mac]'
```

Next time to re-activate this virtual environment, run ```source ./venv/bin/activate```.

#### Extra step for development:
```sh
pip install -e '.[dev]'
```

### Windows Application
[Click here](https://github.com/GradientHQ/parallax_win_cli/releases/latest/download/Parallax_Win_Setup.exe) to get latest Windows installer.

After installing .exe, right click Windows start button and click ```Windows Terminal(Admin)``` to start a Powershell console as administrator.

â— Make sure you open your terminal with administrator privileges.
<details>
<summary>Ways to run Windows Terminal as administrator</summary>

- Start menu: Rightâ€‘click Start and choose "Windows Terminal (Admin)", or search "Windows Terminal", rightâ€‘click the result, and select "Run as administrator".
- Run dialog: Press Win+R â†’ type `wt` â†’ press Ctrl+Shift+Enter.
- Task Manager: Press Ctrl+Shift+Esc â†’ File â†’ Run new task â†’ enter `wt` â†’ check "Create this task with administrator privileges".
- File Explorer: Open the target folder â†’ hold Ctrl+Shift â†’ rightâ€‘click in the folder â†’ select "Open in Terminal".
</details>
<br>

Start Windows dependencies installation by simply typing this command in console:
```sh
parallax install
```

Installation process may take around 30 minutes.

To see a description of all Parallax Windows configurations you can do:
```sh
parallax --help
```

### Docker
For Linux+GPU devices, Parallax provides a docker environment for quick setup. Choose the docker image according to the device's GPU architechture.

|  GPU Architecture  |  GPU Series  | Image Pull Command |
|:-------------|:----------------------------|:----------------------------|
|Blackwell/Ampere/Hopper| RTX50 series/RTX40 series/B100/B200/A100/H100... |```docker pull gradientservice/parallax:latest```|
|DGX Spark | GB10 |```docker pull gradientservice/parallax:latest-spark```|

Run a docker container as below. Please note that generally the argument ```--gpus all``` is necessary for the docker to run on GPUs.
```sh
# For Blackwell/Ampere/Hopper
docker run -it --gpus all --network host gradientservice/parallax:latest bash
# For DGX Spark
docker run -it --gpus all --network host gradientservice/parallax:latest-spark bash
```
The container starts under parallax workspace and you should be able to run parallax directly.

### Uninstalling Parallax

For macOS or Linux, if you've installed Parallax via pip and want to uninstall it, you can use the following command:

```sh
pip uninstall parallax
```

For Docker installations, remove Parallax images and containers using standard Docker commands:

```sh
docker ps -a               # List running containers
docker stop <container_id> # Stop running containers
docker rm <container_id>   # Remove stopped containers
docker images              # List Docker images
docker rmi <image_id>      # Remove Parallax images
```

For Windows, simply go to Control Panel â†’ Programs â†’ Uninstall a program, find "Gradient" in the list, and uninstall it.


================================================================================
File: docs/user_guide/quick_start.md
Size: 8.17 kB
================================================================================

## Getting Started

We will walk through you the easiest way to quickly set up your own AI cluster.

If you have not installed Parallax yet, please refer to the [installation guide](./install.md) and follow the instructions.

### With Frontend

#### Step 1: Launch scheduler

First launch our scheduler on the main node, we recommend you to use your most convenient computer for this.
- For Linux/macOS:
```sh
parallax run
```

- For Windows, start Powershell console as administrator and run:
```sh
parallax run
```

To allow the API to be accessible from other machines, add the argument `--host 0.0.0.0` when launching scheduler.
```sh
parallax run --host 0.0.0.0
```

When running `parallax run` for the first time or after an update, the code version info might be sent to help improve the project. To disable this, use the `-u` flag:
```sh
parallax run -u
```

#### Step 2: Set cluster and model config

Open http://localhost:3001 and you should see the setup interface.

![Model select](../images/node_config.png)

Select your desired node and model config and click continue.

> **Note:**
When running in remote mode, Parallax will use a public relay server to help establish connections between the scheduler and nodes. The public relay server will receive the IP information of both the scheduler and the nodes in order to facilitate this connection.

#### Step 3: Connect your nodes

Copy the generated join command line to your node and run. For remote connection, you can find your scheduler-address in the scheduler logs.

```sh
# local area network env
parallax join
# public network env
parallax join -s {scheduler-address}
# example
parallax join -s 12D3KooWLX7MWuzi1Txa5LyZS4eTQ2tPaJijheH8faHggB9SxnBu
```

![Node join](../images/node_join.png)

You should see your nodes start to show up with their status. Wait until all nodes are successfully connected, and you will automatically be directed to the chat interface.

When running `parallax join` for the first time or after an update, the code version info might be sent to help improve the project. To disable this, use the `-u` flag:
```sh
parallax join -u
```

#### Step 4: Chat

Done! You have your own AI cluster now.

![Chat](../images/chat_interface.png)

#### Accessing the chat interface from another non-scheduler computer

You can access the chat interface from any non-scheduler computer, not just those running a node server. Simply start the chat server with:

```sh
# local area network env
parallax chat
# public network env
parallax chat -s {scheduler-address}
# example
parallax chat -s 12D3KooWLX7MWuzi1Txa5LyZS4eTQ2tPaJijheH8faHggB9SxnBu
```

After launching, visit [http://localhost:3002](http://localhost:3002) in your browser to use the chat interface.

To allow the API to be accessible from other machines, add the argument `--host 0.0.0.0` when launching chat interface.
```sh
parallax chat --host 0.0.0.0
```

### Without frontend
#### Step 1: Launch scheduler
First launch our scheduler on the main node.
```sh
parallax run -m {model-name} -n {number-of-worker-nodes}
```
For example:
```sh
parallax run -m Qwen/Qwen3-0.6B -n 2
```
Please notice and record the scheduler ip4 address generated in the terminal.

#### Step 2: Connect your nodes
For each distributed nodes including the main node, open a terminal and join the server with the scheduler address.
```sh
# local area network env
parallax join
# public network env
parallax join -s {scheduler-address}
```
For example:
```sh
# first node
parallax join -s 12D3KooWLX7MWuzi1Txa5LyZS4eTQ2tPaJijheH8faHggB9SxnBu
# second node
parallax join -s 12D3KooWLX7MWuzi1Txa5LyZS4eTQ2tPaJijheH8faHggB9SxnBu
```

#### Step 3: Call chat api with Scheduler
```sh
curl --location 'http://localhost:3001/v1/chat/completions' --header 'Content-Type: application/json' --data '{
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": "hello"
      }
    ],
    "stream": true
}'
```

> **Note:**
For models such as Qwen3 and gpt-oss, the "reasoning" (or "thinking") feature is enabled by default. To disable it, add `"chat_template_kwargs": {"enable_thinking": false}` to your request payload.

### Skipping Scheduler
Developers can start Parallax backend engine without a scheduler. Pipeline parallel start/end layers should be set manually.
An example of serving Qwen3-0.6B with 2-nodes:
- First node:
```sh
python3 ./parallax/src/parallax/launch.py \
--model-path Qwen/Qwen3-0.6B \
--port 3000 \
--max-batch-size 8 \
--start-layer 0 \
--end-layer 14
```
- Second node:
```sh
python3 ./parallax/src/parallax/launch.py \
--model-path Qwen/Qwen3-0.6B \
--port 3000 \
--max-batch-size 8 \
--start-layer 14 \
--end-layer 28
```

Call chat API on one of the nodes:
```sh
curl --location 'http://localhost:3000/v1/chat/completions' --header 'Content-Type: application/json' --data '{
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": "hello"
      }
    ],
    "stream": true
}'

```

### FAQ
> Q: When deploying on cloud servers, I encounter an error like "lattica RPC call failed". What does this mean and how can I resolve it?

A: This error typically occurs when the necessary network ports for communication between the scheduler and nodes are blockedâ€”most often due to firewall or security group settings on your cloud platform.

**How to fix:**
- Ensure that the relevant TCP/UDP ports for both the scheduler and nodes are open and accessible between all machines in your cluster.
    - By default, the scheduler uses HTTP port `3001`, and nodes use HTTP port `3000`. You can change these with the `--port` argument (e.g., `parallax run --port <your_port>` or `parallax join --port <your_port>`).
    - For Lattica (node-to-node) communication, random ports are used by default. It is best to explicitly specify which TCP and UDP ports to use (e.g., `--tcp-port <your_tcp_port> --udp-port <your_udp_port>`), and then open those ports for inbound and outbound traffic in your cloud provider's security settings.
- Check your cloud provider's firewall or network security group configurations:
    1. Open inbound rules for the ports mentioned above on all scheduler and node machines.
    2. Make sure that ports are open to the desired sources (e.g., to all cluster instances, or to your public IPs if required).

After updating the firewall/security group settings to allow these ports, restart your scheduler and nodes.

> Q: When running on macOS, I encounter the error: `error sending packet on iface address No route to host (os error 65) address=192.168.xxx.xxx`. What does this mean and how can I fix it?

A: On macOS, you need to allow your terminal or IDE (such as Terminal, iTerm2, VS Code, Cursor, etc.) access to the local network in order for Parallax to work correctly. If the application prompts you for network access the first time you run Parallax, click "Allow." If you have already denied access, follow these steps to enable it:

1. Open System Settings from the Apple menu.
2. Click on Privacy & Security in the sidebar.
3. Click on Local Network.
4. For each app listed, turn the ability to access your local network on or off using the toggle switch.

This will ensure Parallax has the proper network permissions for local communication.

> Q: When running the scheduler on Windows, nodes on other PCs cannot detect the scheduler ID over the local network. Why can't other machines join the cluster?

A: If you are running Parallax in WSL (Windows Subsystem for Linux), make sure you are using the "Mirrored" networking mode. By default, WSL uses "NAT" (Network Address Translation) mode, which isolates your WSL environment behind a virtual network. As a result, services running inside WSL (such as Parallax scheduler) are not directly accessible from other devices on the LAN.

To ensure that other machines on your network can connect to your WSL instance, change the WSL networking mode to "Mirrored" (supported on Windows 11 version 22H2 or later). In "Mirrored" mode, your WSL environment will share the same network as your host, allowing local network discovery and seamless joining of nodes to your Parallax cluster.


================================================================================
File: pyproject.toml
Size: 1.57 kB
================================================================================

[build-system]
requires = ["setuptools>=68", "wheel", "poetry-core"]
build-backend = "poetry.core.masonry.api"


[project]
name = "parallax"
version = "0.1.2"
description = "Decentralised pipeline-parallel LLM serving with Sglang + MLX-LM + Lattica"
readme = "README.md"
requires-python = ">=3.11,<3.14"
packages = [
  { include = "parallax", from = "src" },
  { include = "scheduling", from = "src" },
  { include = "parallax_utils", from = "src" },
]

dependencies = [
  "msgpack>=1.0.7",
  "safetensors>=0.5.1",
  "huggingface-hub",
  "transformers==4.57.1",
  "jinja2>=3.1.0",
  "numpy>=1.26",
  "pyzmq>=25.0",
  "psutil>=5.9.5",
  "httpx[socks]>=0.26.0",
  "aiohttp",
  "uvicorn",
  "uvloop",
  "fastapi",
  "pydantic",
  "protobuf==6.31.1",
  "dijkstar==2.6.0",
  "lattica==1.0.14",
  "orjson",
]

[project.scripts]
parallax = "parallax.cli:main"

[project.optional-dependencies]

mac = [
  "torch==2.8.0",
  "mlx-lm==0.28.0",
  "mlx==0.29.1",
]

gpu = [
  "sglang[all]==0.5.5",
  "mlx-lm==0.28.0",
  "mlx[cpu]==0.29.1",
]

vllm = [
  "vllm==0.11.0",
  "mlx-lm==0.28.0",
  "mlx[cpu]==0.29.1",
]

benchmark = [
  "transformers",
  "tqdm",
  "datasets",
  "pillow",
  "modelscope",
]

dev = [
  "black>=24.3",
  "ruff>=0.4",
  "pytest>=8.2",
  "pytest-mock>=3.14",
  "pytest-cov>=5.0",
  "pre-commit>=3.2",
]

[tool.setuptools.packages.find]
where = ["src"]                # put your code under ./src/â€¦

[tool.black]
line-length = 100

[tool.isort]
profile = "black"

[tool.ruff]
ignore = ["E501"]              # line length handled by Black
line-length = 100


================================================================================
File: src/backend/__init__.py
Size: 0 B
================================================================================



================================================================================
File: src/backend/benchmark/backend_request_func.py
Size: 12.34 kB
================================================================================

"""
Adapted from vLLM: https://github.com/vllm-project/vllm/blob/v0.7.2/benchmarks/backend_request_func.py
"""

import json
import os
import sys
import time
import traceback
from dataclasses import dataclass, field
from typing import List, Optional, Union

import aiohttp
import huggingface_hub.constants
from modelscope import snapshot_download
from tqdm.asyncio import tqdm
from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast

AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)


@dataclass
class RequestFuncInput:
    prompt: str
    api_url: str
    prompt_len: int
    output_len: int
    model: str
    model_name: Optional[str] = None
    best_of: int = 1
    logprobs: Optional[int] = None
    extra_body: Optional[dict] = None
    multi_modal_content: Optional[dict] = None
    ignore_eos: bool = False


@dataclass
class RequestFuncOutput:
    generated_text: str = ""
    success: bool = False
    latency: float = 0.0
    output_tokens: int = 0
    ttft: float = 0.0  # Time to first token
    itl: List[float] = field(default_factory=list)  # List of inter-token latencies
    tpot: float = 0.0  # avg next-token latencies
    prompt_len: int = 0
    error: str = ""


async def async_request_openai_completions(
    request_func_input: RequestFuncInput,
    pbar: Optional[tqdm] = None,
) -> RequestFuncOutput:
    api_url = request_func_input.api_url
    assert api_url.endswith(
        ("completions", "profile")
    ), "OpenAI Completions API URL must end with 'completions' or 'profile'."

    async with aiohttp.ClientSession(trust_env=True, timeout=AIOHTTP_TIMEOUT) as session:
        payload = {
            "model": (
                request_func_input.model_name
                if request_func_input.model_name
                else request_func_input.model
            ),
            "prompt": request_func_input.prompt,
            "temperature": 0.0,
            "best_of": request_func_input.best_of,
            "max_tokens": request_func_input.output_len,
            "logprobs": request_func_input.logprobs,
            "stream": True,
            "stream_options": {
                "include_usage": True,
            },
        }
        if request_func_input.ignore_eos:
            payload["ignore_eos"] = request_func_input.ignore_eos
        if request_func_input.extra_body:
            payload.update(request_func_input.extra_body)
        headers = {"Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}"}

        output = RequestFuncOutput()
        output.prompt_len = request_func_input.prompt_len

        generated_text = ""
        st = time.perf_counter()
        most_recent_timestamp = st
        try:
            async with session.post(url=api_url, json=payload, headers=headers) as response:
                if response.status == 200:
                    first_chunk_received = False
                    async for chunk_bytes in response.content:
                        chunk_bytes = chunk_bytes.strip()
                        if not chunk_bytes:
                            continue

                        chunk = chunk_bytes.decode("utf-8").removeprefix("data: ")
                        if chunk != "[DONE]":
                            data = json.loads(chunk)

                            # NOTE: Some completion API might have a last
                            # usage summary response without a token so we
                            # want to check a token was generated
                            if choices := data.get("choices"):
                                # Note that text could be empty here
                                # e.g. for special tokens
                                text = choices[0].get("text")
                                timestamp = time.perf_counter()
                                # First token
                                if not first_chunk_received:
                                    first_chunk_received = True
                                    ttft = time.perf_counter() - st
                                    output.ttft = ttft

                                # Decoding phase
                                else:
                                    output.itl.append(timestamp - most_recent_timestamp)

                                most_recent_timestamp = timestamp
                                generated_text += text or ""
                            elif usage := data.get("usage"):
                                output.output_tokens = usage.get("completion_tokens")
                    if first_chunk_received:
                        output.success = True
                    else:
                        output.success = False
                        output.error = (
                            "Never received a valid chunk to calculate TTFT."
                            "This response will be marked as failed!"
                        )
                    output.generated_text = generated_text
                    output.latency = most_recent_timestamp - st
                else:
                    output.error = response.reason or ""
                    output.success = False
        except Exception:
            output.success = False
            exc_info = sys.exc_info()
            output.error = "".join(traceback.format_exception(*exc_info))

    if pbar:
        pbar.update(1)
    return output


async def async_request_openai_chat_completions(
    request_func_input: RequestFuncInput,
    pbar: Optional[tqdm] = None,
) -> RequestFuncOutput:
    """Send a streaming request to an OpenAI-compatible Chat Completions API.

    This implementation measures client-side latencies consistently:
    - TTFT (time-to-first-token) is recorded at the arrival time of the first
      non-empty content delta.
    - ITL (inter-token latencies) are measured between subsequent non-empty
      content deltas.
    - Overall latency is measured up to the last non-empty content delta, not
      the trailing usage summary event. This aligns TPOT and ITL.

    Args:
        request_func_input: Request parameters and payload settings.
        pbar: Optional progress bar to update upon completion.

    Returns:
        RequestFuncOutput populated with generated text, token counts, and
        timing metrics captured from the streaming response.
    """
    api_url = request_func_input.api_url
    assert api_url.endswith(
        "chat/completions"
    ), "OpenAI Chat Completions API URL must end with 'chat/completions'."

    async with aiohttp.ClientSession(trust_env=True, timeout=AIOHTTP_TIMEOUT) as session:
        content = [{"type": "text", "text": request_func_input.prompt}]
        if request_func_input.multi_modal_content:
            content.append(request_func_input.multi_modal_content)
        payload = {
            "model": (
                request_func_input.model_name
                if request_func_input.model_name
                else request_func_input.model
            ),
            "messages": [
                {"role": "user", "content": content},
            ],
            "temperature": 0.0,
            "max_tokens": request_func_input.output_len,
            "stream": True,
            "stream_options": {
                "include_usage": True,
            },
        }
        if request_func_input.ignore_eos:
            payload["ignore_eos"] = request_func_input.ignore_eos
        if request_func_input.extra_body:
            payload.update(request_func_input.extra_body)
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
        }

        output = RequestFuncOutput()
        output.prompt_len = request_func_input.prompt_len

        generated_text = ""
        st = time.perf_counter()
        # Timestamp of last non-empty content token we observed
        last_token_timestamp = st
        # Timestamp of last received event (used as a fallback if no tokens arrive)
        first_content_received = False
        try:
            async with session.post(url=api_url, json=payload, headers=headers) as response:
                if response.status == 200:
                    async for chunk_bytes in response.content:
                        chunk_bytes = chunk_bytes.strip()
                        if not chunk_bytes:
                            continue

                        chunk = chunk_bytes.decode("utf-8").removeprefix("data: ")
                        if chunk != "[DONE]":
                            timestamp = time.perf_counter()
                            data = json.loads(chunk)

                            if choices := data.get("choices"):
                                delta = choices[0].get("delta", {})
                                content = delta.get("content")
                                content_str = content.strip() if isinstance(content, str) else ""

                                # Only act on non-empty content tokens
                                if content_str:
                                    if not first_content_received:
                                        first_content_received = True
                                        output.ttft = timestamp - st
                                        last_token_timestamp = timestamp
                                    else:
                                        output.itl.append(timestamp - last_token_timestamp)
                                        last_token_timestamp = timestamp

                                    generated_text += content
                            elif usage := data.get("usage"):
                                # Capture token count from trailing usage event
                                output.output_tokens = usage.get("completion_tokens")

                            # Always record last event timestamp for fallback latency

                    output.generated_text = generated_text
                    if first_content_received:
                        output.success = True
                        # Latency is measured to the last non-empty content token
                        output.latency = last_token_timestamp - st
                    else:
                        output.success = False
                        output.error = (
                            "Never received a non-empty content delta to calculate TTFT. "
                            "This response will be marked as failed!"
                        )
                else:
                    output.error = response.reason or ""
                    output.success = False
        except Exception:
            output.success = False
            exc_info = sys.exc_info()
            output.error = "".join(traceback.format_exception(*exc_info))

    if pbar:
        pbar.update(1)
    return output


def get_model(pretrained_model_name_or_path: str) -> str:

    model_path = snapshot_download(
        model_id=pretrained_model_name_or_path,
        local_files_only=huggingface_hub.constants.HF_HUB_OFFLINE,
        ignore_file_pattern=[".*.pt", ".*.safetensors", ".*.bin"],
    )
    return model_path


def get_tokenizer(
    pretrained_model_name_or_path: str,
    tokenizer_mode: str = "auto",
    trust_remote_code: bool = False,
    **kwargs,
) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:
    if pretrained_model_name_or_path is not None and not os.path.exists(
        pretrained_model_name_or_path
    ):
        pretrained_model_name_or_path = get_model(pretrained_model_name_or_path)
    if tokenizer_mode == "slow":
        if kwargs.get("use_fast", False):
            raise ValueError("Cannot use the fast tokenizer in slow tokenizer mode.")
        kwargs["use_fast"] = False
    else:
        return AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path,
            trust_remote_code=trust_remote_code,
            **kwargs,
        )


ASYNC_REQUEST_FUNCS = {
    "vllm": async_request_openai_completions,
    "lmdeploy": async_request_openai_completions,
    "openai": async_request_openai_completions,
    "openai-chat": async_request_openai_chat_completions,
    "sglang": async_request_openai_completions,
    "parallax": async_request_openai_chat_completions,
    "llama.cpp": async_request_openai_completions,
}


================================================================================
File: src/backend/benchmark/benchmark_serving.py
Size: 44.31 kB
================================================================================

r"""Benchmark online serving throughput.
Adapted from vLLM: https://github.com/vllm-project/vllm/blob/v0.7.2/benchmarks/benchmark_serving.py

On the server side (parallax scheduler with oAI API server), run
    python src/backend/main.py --port 31328 --init-nodes-num 1

On the worker side (parallax worker nodes),
    1. Get `scheduler-addr` get from scheduler launching output
    2. Run
    python src/parallax/launch.py \
          --model-path Qwen/Qwen3-0.6B \
          --max-num-tokens-per-batch 16384 \
          --kv-block-size 1024 \
          --max-batch-size 128 \
          --port 3000 \
          --scheduler-addr /ip4/127.0.0.1/tcp/40145/p2p/12D3KooWGCoaHpKfK99BFPzFmuJZ5UQ3UDkJMKFTFzRhawnrXXzD

On the client side, run:
    python benchmark/benchmark_serving.py \
        --backend parallax \
        --model Qwen/Qwen3-0.6B \
        --port 31328 \
        --dataset-name random \
        --request-rate 16 \
        --num-prompts 1000
"""

import argparse
import asyncio
import base64
import gc
import io
import json
import os
import random
import time
import warnings
from argparse import ArgumentParser as FlexibleArgumentParser
from dataclasses import dataclass
from datetime import datetime
from json import JSONDecodeError
from typing import Any, AsyncGenerator, Collection, Dict, List, Optional, Tuple

import numpy as np
import requests
from backend_request_func import (
    ASYNC_REQUEST_FUNCS,
    RequestFuncInput,
    RequestFuncOutput,
    get_tokenizer,
)
from datasets import load_dataset
from PIL.Image import Image
from tqdm.asyncio import tqdm
from transformers import PreTrainedTokenizerBase

MILLISECONDS_TO_SECONDS_CONVERSION = 1000

SHAREGPT_URL = "https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json"


def is_file_valid_json(path):
    if not os.path.isfile(path):
        return False

    # TODO can fuse into the real file open later
    try:
        with open(path) as f:
            json.load(f)
        return True
    except JSONDecodeError as e:
        print(f"{path} exists but json loading fails ({e=}), thus treat as invalid file")
        return False


def download_and_cache_file(url: str, filename: Optional[str] = None):
    """Read and cache a file from a url."""
    if filename is None:
        filename = os.path.join("/tmp", url.split("/")[-1])

    # Check if the cache file already exists
    if is_file_valid_json(filename):
        return filename

    print(f"Downloading from {url} to {filename}")

    # Stream the response to show the progress bar
    response = requests.get(url, stream=True)
    response.raise_for_status()  # Check for request errors

    # Total size of the file in bytes
    total_size = int(response.headers.get("content-length", 0))
    chunk_size = 1024  # Download in chunks of 1KB

    # Use tqdm to display the progress bar
    with (
        open(filename, "wb") as f,
        tqdm(
            desc=filename,
            total=total_size,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
        ) as bar,
    ):
        for chunk in response.iter_content(chunk_size=chunk_size):
            f.write(chunk)
            bar.update(len(chunk))

    return filename


@dataclass
class BenchmarkMetrics:
    completed: int
    total_input: int
    total_output: int
    request_throughput: float
    request_goodput: float
    output_throughput: float
    total_token_throughput: float
    mean_ttft_ms: float
    median_ttft_ms: float
    std_ttft_ms: float
    percentiles_ttft_ms: List[Tuple[float, float]]
    mean_tpot_ms: float
    median_tpot_ms: float
    std_tpot_ms: float
    percentiles_tpot_ms: List[Tuple[float, float]]
    mean_itl_ms: float
    median_itl_ms: float
    std_itl_ms: float
    percentiles_itl_ms: List[Tuple[float, float]]
    # E2EL stands for end-to-end latency per request.
    # It is the time taken on the client side from sending
    # a request to receiving a complete response.
    mean_e2el_ms: float
    median_e2el_ms: float
    std_e2el_ms: float
    percentiles_e2el_ms: List[Tuple[float, float]]


def sample_sharegpt_requests(
    dataset_path: str,
    num_requests: int,
    tokenizer: PreTrainedTokenizerBase,
    fixed_output_len: Optional[int] = None,
) -> List[Tuple[str, int, int, None]]:
    # Load the dataset.
    with open(dataset_path, encoding="utf-8") as f:
        dataset = json.load(f)
    # Filter out the conversations with less than 2 turns.
    dataset = [data for data in dataset if len(data["conversations"]) >= 2]
    # Only keep the first two turns of each conversation.
    dataset = [
        (data["conversations"][0]["value"], data["conversations"][1]["value"]) for data in dataset
    ]

    # Shuffle the dataset.
    random.shuffle(dataset)

    # Filter out sequences that are too long or too short
    filtered_dataset: List[Tuple[str, int, int]] = []
    for i in range(len(dataset)):
        if len(filtered_dataset) == num_requests:
            break

        # Tokenize the prompts and completions.
        prompt = dataset[i][0]
        prompt_token_ids = tokenizer(prompt).input_ids
        completion = dataset[i][1]
        completion_token_ids = tokenizer(completion).input_ids
        prompt_len = len(prompt_token_ids)
        output_len = len(completion_token_ids) if fixed_output_len is None else fixed_output_len
        if prompt_len < 4 or (fixed_output_len is None and output_len < 4):
            # Prune too short sequences.
            continue
        if prompt_len > 1024 or prompt_len + output_len > 2048:
            # Prune too long sequences.
            continue
        filtered_dataset.append((prompt, prompt_len, output_len, None))

    return filtered_dataset


def sample_wildchat_requests(
    dataset_path: str,
    num_requests: int,
    tokenizer: PreTrainedTokenizerBase,
    random_seed: int,
    fixed_output_len: Optional[int] = None,
) -> List[Tuple[str, int, int, None]]:
    dataset = load_dataset(dataset_path, split="train")
    filter_func = lambda x: len(x["conversation"]) >= 2
    filtered_dataset = dataset.shuffle(seed=random_seed).filter(filter_func)
    sampled_requests: List[Tuple[str, int, int, Dict[str, Collection[str]]]] = []
    for data in filtered_dataset:
        if len(sampled_requests) == num_requests:
            break

        # Tokenize the prompts and completions.
        prompt = data["conversation"][0]["content"]
        prompt_token_ids = tokenizer(prompt).input_ids
        completion = data["conversation"][1]["content"]
        completion_token_ids = tokenizer(completion).input_ids
        prompt_len = len(prompt_token_ids)
        output_len = len(completion_token_ids) if fixed_output_len is None else fixed_output_len
        if fixed_output_len is None and (prompt_len < 4 or output_len < 4):
            # Prune too short sequences.
            continue
        if fixed_output_len is None and (prompt_len > 1024 or prompt_len + output_len > 2048):
            # Prune too long sequences.
            continue

        mm_content = None

        sampled_requests.append((prompt, prompt_len, output_len, mm_content))

    return sampled_requests


def sample_hf_requests(
    dataset_path: str,
    dataset_subset: Optional[str],
    dataset_split: str,
    num_requests: int,
    tokenizer: PreTrainedTokenizerBase,
    random_seed: int,
    fixed_output_len: Optional[int] = None,
) -> List[Tuple[str, str, int, Optional[Dict[str, Collection[str]]]]]:

    dataset = load_dataset(dataset_path, name=dataset_subset, split=dataset_split, streaming=True)
    assert "conversations" in dataset.features, "HF Dataset must have 'conversations' column."
    filter_func = lambda x: len(x["conversations"]) >= 2
    filtered_dataset = dataset.shuffle(seed=random_seed).filter(filter_func)
    sampled_requests: List[Tuple[str, int, int, Dict[str, Collection[str]]]] = []
    for data in filtered_dataset:
        if len(sampled_requests) == num_requests:
            break

        # Tokenize the prompts and completions.
        prompt = data["conversations"][0]["value"]
        prompt_token_ids = tokenizer(prompt).input_ids
        completion = data["conversations"][1]["value"]
        completion_token_ids = tokenizer(completion).input_ids
        prompt_len = len(prompt_token_ids)
        output_len = len(completion_token_ids) if fixed_output_len is None else fixed_output_len
        if fixed_output_len is None and (prompt_len < 4 or output_len < 4):
            # Prune too short sequences.
            continue
        if fixed_output_len is None and (prompt_len > 1024 or prompt_len + output_len > 2048):
            # Prune too long sequences.
            continue

        if "image" in data and isinstance(data["image"], Image):
            image: Image = data["image"]
            image = image.convert("RGB")
            image_data = io.BytesIO()
            image.save(image_data, format="JPEG")
            image_base64 = base64.b64encode(image_data.getvalue()).decode("utf-8")
            mm_content = {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"},
            }
        elif "image" in data and isinstance(data["image"], str):
            if data["image"].startswith("http://") or data["image"].startswith("file://"):
                image_url = data["image"]
            else:
                image_url = f"file://{data['image']}"

            mm_content = {
                "type": "image_url",
                "image_url": {"url": image_url},
            }
        else:
            mm_content = None

        sampled_requests.append((prompt, prompt_len, output_len, mm_content))

    return sampled_requests


def sample_random_requests(
    prefix_len: int,
    input_len: int,
    output_len: int,
    num_prompts: int,
    range_ratio: float,
    tokenizer: PreTrainedTokenizerBase,
) -> List[Tuple[str, int, int]]:
    prefix_token_ids = np.random.randint(0, tokenizer.vocab_size, size=prefix_len).tolist()

    input_lens = np.random.randint(
        int(input_len * range_ratio),
        input_len + 1,
        size=num_prompts,
    )
    output_lens = np.random.randint(
        int(output_len * range_ratio),
        output_len + 1,
        size=num_prompts,
    )
    offsets = np.random.randint(0, tokenizer.vocab_size, size=num_prompts)
    input_requests = []
    for i in range(num_prompts):
        prompt = tokenizer.decode(
            prefix_token_ids
            + [(offsets[i] + i + j) % tokenizer.vocab_size for j in range(input_lens[i])]
        )

        input_requests.append((prompt, int(prefix_len + input_lens[i]), int(output_lens[i]), None))

    return input_requests


async def get_request(
    input_requests: List[Tuple[str, int, int]],
    request_rate: float,
    burstiness: float = 1.0,
) -> AsyncGenerator[Tuple[str, int, int], None]:
    """
    Asynchronously generates requests at a specified rate
    with OPTIONAL burstiness.

    Args:
        input_requests:
            A list of input requests, each represented as a tuple.
        request_rate:
            The rate at which requests are generated (requests/s).
        burstiness (optional):
            The burstiness factor of the request generation.
            Only takes effect when request_rate is not inf.
            Default value is 1, which follows a Poisson process.
            Otherwise, the request intervals follow a gamma distribution.
            A lower burstiness value (0 < burstiness < 1) results
            in more bursty requests, while a higher burstiness value
            (burstiness > 1) results in a more uniform arrival of requests.
    """
    input_requests = iter(input_requests)

    # Calculate scale parameter theta to maintain the desired request_rate.
    assert burstiness > 0, f"A positive burstiness factor is expected, but given {burstiness}."
    theta = 1.0 / (request_rate * burstiness)

    for request in input_requests:
        yield request

        if request_rate == float("inf"):
            # If the request rate is infinity, then we don't need to wait.
            continue

        # Sample the request interval from the gamma distribution.
        # If burstiness is 1, it follows exponential distribution.
        interval = np.random.gamma(shape=burstiness, scale=theta)
        # The next request will be sent after the interval.
        await asyncio.sleep(interval)


def calculate_metrics(
    input_requests: List[Tuple[str, int, int]],
    outputs: List[RequestFuncOutput],
    dur_s: float,
    tokenizer: PreTrainedTokenizerBase,
    selected_percentile_metrics: List[str],
    selected_percentiles: List[float],
    goodput_config_dict: Dict[str, float],
) -> Tuple[BenchmarkMetrics, List[int]]:
    actual_output_lens: List[int] = []
    total_input = 0
    completed = 0
    good_completed = 0
    itls: List[float] = []
    tpots: List[float] = []
    all_tpots: List[float] = []
    ttfts: List[float] = []
    e2els: List[float] = []
    for i in range(len(outputs)):
        if outputs[i].success:
            output_len = outputs[i].output_tokens

            if output_len == 0:
                # We use the tokenizer to count the number of output tokens
                # for some serving backends instead of looking at
                # len(outputs[i].itl) since multiple output tokens may be
                # bundled together
                # Note : this may inflate the output token count slightly
                output_len = len(
                    tokenizer(outputs[i].generated_text, add_special_tokens=False).input_ids
                )
            actual_output_lens.append(output_len)
            total_input += input_requests[i][1]
            tpot = 0
            if output_len > 1:
                latency_minus_ttft = outputs[i].latency - outputs[i].ttft
                tpot = latency_minus_ttft / (output_len - 1)
                tpots.append(tpot)
            # Note: if output_len <= 1, we regard tpot as 0 for goodput
            all_tpots.append(tpot)
            itls += outputs[i].itl
            ttfts.append(outputs[i].ttft)
            e2els.append(outputs[i].latency)
            completed += 1
        else:
            actual_output_lens.append(0)

    if goodput_config_dict:
        valid_metrics = []
        slo_values = []

        if "ttft" in goodput_config_dict:
            valid_metrics.append(ttfts)
            slo_values.append(goodput_config_dict["ttft"] / MILLISECONDS_TO_SECONDS_CONVERSION)
        if "tpot" in goodput_config_dict:
            valid_metrics.append(all_tpots)
            slo_values.append(goodput_config_dict["tpot"] / MILLISECONDS_TO_SECONDS_CONVERSION)
        if "e2el" in goodput_config_dict:
            valid_metrics.append(e2els)
            slo_values.append(goodput_config_dict["e2el"] / MILLISECONDS_TO_SECONDS_CONVERSION)

        for req_metric in zip(*valid_metrics):
            is_good_req = all([s >= r for s, r in zip(slo_values, req_metric)])
            if is_good_req:
                good_completed += 1

    if completed == 0:
        warnings.warn(
            "All requests failed. This is likely due to a misconfiguration "
            "on the benchmark arguments.",
            stacklevel=2,
        )
    metrics = BenchmarkMetrics(
        completed=completed,
        total_input=total_input,
        total_output=sum(actual_output_lens),
        request_throughput=completed / dur_s,
        request_goodput=good_completed / dur_s,
        output_throughput=sum(actual_output_lens) / dur_s,
        total_token_throughput=(total_input + sum(actual_output_lens)) / dur_s,
        mean_ttft_ms=np.mean(ttfts or 0)
        * 1000,  # ttfts is empty if streaming is not supported by backend
        std_ttft_ms=np.std(ttfts or 0) * 1000,
        median_ttft_ms=np.median(ttfts or 0) * 1000,
        percentiles_ttft_ms=[
            (p, np.percentile(ttfts or 0, p) * 1000) for p in selected_percentiles
        ],
        mean_tpot_ms=np.mean(tpots or 0) * 1000,
        std_tpot_ms=np.std(tpots or 0) * 1000,
        median_tpot_ms=np.median(tpots or 0) * 1000,
        percentiles_tpot_ms=[
            (p, np.percentile(tpots or 0, p) * 1000) for p in selected_percentiles
        ],
        mean_itl_ms=np.mean(itls or 0) * 1000,
        std_itl_ms=np.std(itls or 0) * 1000,
        median_itl_ms=np.median(itls or 0) * 1000,
        percentiles_itl_ms=[(p, np.percentile(itls or 0, p) * 1000) for p in selected_percentiles],
        mean_e2el_ms=np.mean(e2els or 0) * 1000,
        std_e2el_ms=np.std(e2els or 0) * 1000,
        median_e2el_ms=np.median(e2els or 0) * 1000,
        percentiles_e2el_ms=[
            (p, np.percentile(e2els or 0, p) * 1000) for p in selected_percentiles
        ],
    )

    return metrics, actual_output_lens


async def benchmark(
    backend: str,
    api_url: str,
    base_url: str,
    model_id: str,
    model_name: str,
    tokenizer: PreTrainedTokenizerBase,
    input_requests: List[Tuple[str, int, int]],
    logprobs: Optional[int],
    best_of: int,
    request_rate: float,
    burstiness: float,
    disable_tqdm: bool,
    profile: bool,
    selected_percentile_metrics: List[str],
    selected_percentiles: List[str],
    ignore_eos: bool,
    goodput_config_dict: Dict[str, float],
    max_concurrency: Optional[int],
):
    if backend in ASYNC_REQUEST_FUNCS:
        request_func = ASYNC_REQUEST_FUNCS[backend]
    else:
        raise ValueError(f"Unknown backend: {backend}")

    print("Starting initial single prompt test run...")
    test_prompt, test_prompt_len, test_output_len, test_mm_content = input_requests[0]
    if backend != "openai-chat" and test_mm_content is not None:
        # multi-modal benchmark is only available on OpenAI Chat backend.
        raise ValueError("Multi-modal content is only supported on 'openai-chat' backend.")
    test_input = RequestFuncInput(
        model=model_id,
        model_name=model_name,
        prompt=test_prompt,
        api_url=api_url,
        prompt_len=test_prompt_len,
        output_len=test_output_len,
        logprobs=logprobs,
        best_of=best_of,
        multi_modal_content=test_mm_content,
        ignore_eos=ignore_eos,
    )
    test_output = await request_func(request_func_input=test_input)
    if not test_output.success:
        raise ValueError(
            "Initial test run failed - Please make sure benchmark arguments "
            f"are correctly specified. Error: {test_output.error}"
        )
    else:
        print("Initial test run completed. Starting main benchmark run...")

    if profile:
        print("Starting profiler...")
        profile_input = RequestFuncInput(
            model=model_id,
            model_name=model_name,
            prompt=test_prompt,
            api_url=base_url + "/start_profile",
            prompt_len=test_prompt_len,
            output_len=test_output_len,
            logprobs=logprobs,
            best_of=best_of,
            multi_modal_content=test_mm_content,
            ignore_eos=ignore_eos,
        )
        profile_output = await request_func(request_func_input=profile_input)
        if profile_output.success:
            print("Profiler started")

    if burstiness == 1.0:
        distribution = "Poisson process"
    else:
        distribution = "Gamma distribution"

    print(f"Traffic request rate: {request_rate}")
    print(f"Burstiness factor: {burstiness} ({distribution})")
    print(f"Maximum request concurrency: {max_concurrency}")

    pbar = None if disable_tqdm else tqdm(total=len(input_requests))

    # This can be used once the minimum Python version is 3.10 or higher,
    # and it will simplify the code in limited_request_func.
    #    semaphore = (asyncio.Semaphore(max_concurrency)
    #                 if max_concurrency else contextlib.nullcontext())
    semaphore = asyncio.Semaphore(max_concurrency) if max_concurrency else None

    async def limited_request_func(request_func_input, pbar):
        if semaphore is None:
            return await request_func(request_func_input=request_func_input, pbar=pbar)
        async with semaphore:
            return await request_func(request_func_input=request_func_input, pbar=pbar)

    benchmark_start_time = time.perf_counter()
    tasks: List[asyncio.Task] = []
    async for request in get_request(input_requests, request_rate, burstiness):
        prompt, prompt_len, output_len, mm_content = request
        request_func_input = RequestFuncInput(
            model=model_id,
            model_name=model_name,
            prompt=prompt,
            api_url=api_url,
            prompt_len=prompt_len,
            output_len=output_len,
            logprobs=logprobs,
            best_of=best_of,
            multi_modal_content=mm_content,
            ignore_eos=ignore_eos,
        )
        tasks.append(
            asyncio.create_task(
                limited_request_func(request_func_input=request_func_input, pbar=pbar)
            )
        )
    outputs: List[RequestFuncOutput] = await asyncio.gather(*tasks)

    if profile:
        print("Stopping profiler...")
        profile_input = RequestFuncInput(
            model=model_id,
            prompt=test_prompt,
            api_url=base_url + "/stop_profile",
            prompt_len=test_prompt_len,
            output_len=test_output_len,
            logprobs=logprobs,
            best_of=best_of,
        )
        profile_output = await request_func(request_func_input=profile_input)
        if profile_output.success:
            print("Profiler stopped")

    if pbar is not None:
        pbar.close()

    benchmark_duration = time.perf_counter() - benchmark_start_time

    metrics, actual_output_lens = calculate_metrics(
        input_requests=input_requests,
        outputs=outputs,
        dur_s=benchmark_duration,
        tokenizer=tokenizer,
        selected_percentile_metrics=selected_percentile_metrics,
        selected_percentiles=selected_percentiles,
        goodput_config_dict=goodput_config_dict,
    )

    print("{s:{c}^{n}}".format(s=" Serving Benchmark Result ", n=50, c="="))
    print("{:<40} {:<10}".format("Successful requests:", metrics.completed))
    print("{:<40} {:<10.2f}".format("Benchmark duration (s):", benchmark_duration))
    print("{:<40} {:<10}".format("Total input tokens:", metrics.total_input))
    print("{:<40} {:<10}".format("Total generated tokens:", metrics.total_output))
    print("{:<40} {:<10.2f}".format("Request throughput (req/s):", metrics.request_throughput))
    if goodput_config_dict:
        print("{:<40} {:<10.2f}".format("Request goodput (req/s):", metrics.request_goodput))
    print("{:<40} {:<10.2f}".format("Output token throughput (tok/s):", metrics.output_throughput))
    print(
        "{:<40} {:<10.2f}".format("Total Token throughput (tok/s):", metrics.total_token_throughput)
    )

    result = {
        "duration": benchmark_duration,
        "completed": metrics.completed,
        "total_input_tokens": metrics.total_input,
        "total_output_tokens": metrics.total_output,
        "request_throughput": metrics.request_throughput,
        "request_goodput:": metrics.request_goodput if goodput_config_dict else None,
        "output_throughput": metrics.output_throughput,
        "total_token_throughput": metrics.total_token_throughput,
        "input_lens": [output.prompt_len for output in outputs],
        "output_lens": actual_output_lens,
        "ttfts": [output.ttft for output in outputs],
        "itls": [output.itl for output in outputs],
        "generated_texts": [output.generated_text for output in outputs],
        "errors": [output.error for output in outputs],
    }

    def process_one_metric(
        # E.g., "ttft"
        metric_attribute_name: str,
        # E.g., "TTFT"
        metric_name: str,
        # E.g., "Time to First Token"
        metric_header: str,
    ):
        # This function prints and adds statistics of the specified
        # metric.
        if metric_attribute_name not in selected_percentile_metrics:
            return
        print("{s:{c}^{n}}".format(s=metric_header, n=50, c="-"))
        print(
            "{:<40} {:<10.2f}".format(
                f"Mean {metric_name} (ms):", getattr(metrics, f"mean_{metric_attribute_name}_ms")
            )
        )
        print(
            "{:<40} {:<10.2f}".format(
                f"Median {metric_name} (ms):",
                getattr(metrics, f"median_{metric_attribute_name}_ms"),
            )
        )
        result[f"mean_{metric_attribute_name}_ms"] = getattr(
            metrics, f"mean_{metric_attribute_name}_ms"
        )
        result[f"median_{metric_attribute_name}_ms"] = getattr(
            metrics, f"median_{metric_attribute_name}_ms"
        )
        result[f"std_{metric_attribute_name}_ms"] = getattr(
            metrics, f"std_{metric_attribute_name}_ms"
        )
        for p, value in getattr(metrics, f"percentiles_{metric_attribute_name}_ms"):
            p_word = str(int(p)) if int(p) == p else str(p)
            print("{:<40} {:<10.2f}".format(f"P{p_word} {metric_name} (ms):", value))
            result[f"p{p_word}_{metric_attribute_name}_ms"] = value

    process_one_metric("ttft", "TTFT", "Time to First Token")
    process_one_metric("tpot", "TPOT", "Time per Output Token (excl. 1st token)")
    process_one_metric("itl", "ITL", "Inter-token Latency")
    process_one_metric("e2el", "E2EL", "End-to-end Latency")

    print("=" * 50)

    return result


def check_goodput_args(args):
    # Check and parse goodput arguments
    goodput_config_dict = {}
    VALID_NAMES = ["ttft", "tpot", "e2el"]
    if args.goodput:
        goodput_config_dict = parse_goodput(args.goodput)
        for slo_name, slo_val in goodput_config_dict.items():
            if slo_name not in VALID_NAMES:
                raise ValueError(
                    f"Invalid metric name found, {slo_name}: {slo_val}. "
                    "The service level objective name should be one of "
                    f"{str(VALID_NAMES)}. "
                )
            if slo_val < 0:
                raise ValueError(
                    f"Invalid value found, {slo_name}: {slo_val}. "
                    "The service level objective value should be "
                    "non-negative."
                )
    return goodput_config_dict


def parse_goodput(slo_pairs):
    goodput_config_dict = {}
    try:
        for slo_pair in slo_pairs:
            slo_name, slo_val = slo_pair.split(":")
            goodput_config_dict[slo_name] = float(slo_val)
    except ValueError as err:
        raise argparse.ArgumentTypeError(
            "Invalid format found for service level objectives. "
            'Specify service level objectives for goodput as "KEY:VALUE" '
            "pairs, where the key is a metric name, and the value is a "
            "number in milliseconds."
        ) from err
    return goodput_config_dict


def main(args: argparse.Namespace):
    print(args)
    random.seed(args.seed)
    np.random.seed(args.seed)

    backend = args.backend
    model_id = args.model
    model_name = args.served_model_name
    tokenizer_id = args.tokenizer if args.tokenizer is not None else args.model
    tokenizer_mode = args.tokenizer_mode

    if args.base_url is not None:
        api_url = f"{args.base_url}{args.endpoint}"
        base_url = f"{args.base_url}"
    else:
        api_url = f"http://{args.host}:{args.port}{args.endpoint}"
        base_url = f"http://{args.host}:{args.port}"

    tokenizer = get_tokenizer(
        tokenizer_id, tokenizer_mode=tokenizer_mode, trust_remote_code=args.trust_remote_code
    )

    if args.dataset is not None:
        warnings.warn(
            "The '--dataset' argument will be deprecated in the next "
            "release. Please use '--dataset-name' and "
            "'--dataset-path' in the future runs.",
            stacklevel=2,
        )
        input_requests = sample_sharegpt_requests(
            dataset_path=args.dataset,
            num_requests=args.num_prompts,
            tokenizer=tokenizer,
            fixed_output_len=args.sharegpt_output_len,
        )

    elif args.dataset_name == "sharegpt":
        if args.dataset_path is not None:
            sharegpt_path = args.dataset_path
        else:
            sharegpt_path = download_and_cache_file(SHAREGPT_URL)
        input_requests = sample_sharegpt_requests(
            dataset_path=sharegpt_path,
            num_requests=args.num_prompts,
            tokenizer=tokenizer,
            fixed_output_len=args.sharegpt_output_len,
        )

    elif args.dataset_name == "wildchat":
        input_requests = sample_wildchat_requests(
            dataset_path="allenai/WildChat",
            num_requests=args.num_prompts,
            tokenizer=tokenizer,
            random_seed=args.seed,
            fixed_output_len=args.sharegpt_output_len,
        )

    elif args.dataset_name == "sonnet":
        # Do not format the prompt, pass to message directly
        if args.backend == "openai-chat":
            input_requests = sample_sonnet_requests(
                dataset_path=args.dataset_path,
                num_requests=args.num_prompts,
                input_len=args.sonnet_input_len,
                output_len=args.sonnet_output_len,
                prefix_len=args.sonnet_prefix_len,
                tokenizer=tokenizer,
            )
            input_requests = [
                (prompt, prompt_len, output_len, None)
                for prompt, prompt_formatted, prompt_len, output_len, _ in input_requests
            ]
        else:
            assert (
                tokenizer.chat_template or tokenizer.default_chat_template
            ), "Tokenizer/model must have chat template for sonnet dataset."
            input_requests = sample_sonnet_requests(
                dataset_path=args.dataset_path,
                num_requests=args.num_prompts,
                input_len=args.sonnet_input_len,
                output_len=args.sonnet_output_len,
                prefix_len=args.sonnet_prefix_len,
                tokenizer=tokenizer,
            )
            input_requests = [
                (prompt_formatted, prompt_len, output_len, None)
                for prompt, prompt_formatted, prompt_len, output_len, _ in input_requests
            ]

    elif args.dataset_name == "hf":
        input_requests = sample_hf_requests(
            dataset_path=args.dataset_path,
            dataset_subset=args.hf_subset,
            dataset_split=args.hf_split,
            num_requests=args.num_prompts,
            tokenizer=tokenizer,
            random_seed=args.seed,
            fixed_output_len=args.hf_output_len,
        )

    elif args.dataset_name == "random":
        input_requests = sample_random_requests(
            prefix_len=args.random_prefix_len,
            input_len=args.random_input_len,
            output_len=args.random_output_len,
            num_prompts=args.num_prompts,
            range_ratio=args.random_range_ratio,
            tokenizer=tokenizer,
        )

    else:
        raise ValueError(f"Unknown dataset: {args.dataset_name}")

    goodput_config_dict = check_goodput_args(args)

    # Avoid GC processing "static" data - reduce pause times.
    gc.collect()
    gc.freeze()

    benchmark_result = asyncio.run(
        benchmark(
            backend=backend,
            api_url=api_url,
            base_url=base_url,
            model_id=model_id,
            model_name=model_name,
            tokenizer=tokenizer,
            input_requests=input_requests,
            logprobs=args.logprobs,
            best_of=args.best_of,
            request_rate=args.request_rate,
            burstiness=args.burstiness,
            disable_tqdm=args.disable_tqdm,
            profile=args.profile,
            selected_percentile_metrics=args.percentile_metrics.split(","),
            selected_percentiles=[float(p) for p in args.metric_percentiles.split(",")],
            ignore_eos=args.ignore_eos,
            goodput_config_dict=goodput_config_dict,
            max_concurrency=args.max_concurrency,
        )
    )

    # Save config and results to json
    if args.save_result:
        result_json: Dict[str, Any] = {}

        # Setup
        current_dt = datetime.now().strftime("%Y%m%d-%H%M%S")
        result_json["date"] = current_dt
        result_json["backend"] = backend
        result_json["model_id"] = model_id
        result_json["tokenizer_id"] = tokenizer_id
        result_json["best_of"] = args.best_of
        result_json["num_prompts"] = args.num_prompts

        # Metadata
        if args.metadata:
            for item in args.metadata:
                if "=" in item:
                    kvstring = item.split("=")
                    result_json[kvstring[0].strip()] = kvstring[1].strip()
                else:
                    raise ValueError("Invalid metadata format. Please use KEY=VALUE format.")

        # Traffic
        result_json["request_rate"] = (
            args.request_rate if args.request_rate < float("inf") else "inf"
        )
        result_json["burstiness"] = args.burstiness
        result_json["max_concurrency"] = args.max_concurrency

        # Merge with benchmark result
        result_json = {**result_json, **benchmark_result}

        # Save to file
        base_model_id = model_id.split("/")[-1]
        max_concurrency_str = (
            f"-concurrency{args.max_concurrency}" if args.max_concurrency is not None else ""
        )
        file_name = f"{backend}-{args.request_rate}qps{max_concurrency_str}-{base_model_id}-{current_dt}.json"  # noqa
        if args.result_filename:
            file_name = args.result_filename
        if args.result_dir:
            file_name = os.path.join(args.result_dir, file_name)
        with open(file_name, "w", encoding="utf-8") as outfile:
            json.dump(result_json, outfile)


if __name__ == "__main__":
    parser = FlexibleArgumentParser(description="Benchmark the online serving throughput.")
    parser.add_argument(
        "--backend",
        type=str,
        default="vllm",
        choices=list(ASYNC_REQUEST_FUNCS.keys()),
    )
    parser.add_argument(
        "--base-url",
        type=str,
        default=None,
        help="Server or API base url if not using http host and port.",
    )
    parser.add_argument("--host", type=str, default="localhost")
    parser.add_argument("--port", type=int, default=8000)
    parser.add_argument(
        "--endpoint",
        type=str,
        default="/v1/chat/completions",
        help="API endpoint.",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default=None,
        help="Path to the ShareGPT dataset, will be deprecated in the " "next release.",
    )
    parser.add_argument(
        "--dataset-name",
        type=str,
        default="sharegpt",
        choices=["sharegpt", "wildchat", "sonnet", "random", "hf"],
        help="Name of the dataset to benchmark on.",
    )
    parser.add_argument(
        "--dataset-path",
        type=str,
        default=None,
        help="Path to the sharegpt/sonnet dataset. "
        "Or the huggingface dataset ID if using HF dataset.",
    )
    parser.add_argument(
        "--max-concurrency",
        type=int,
        default=None,
        help="Maximum number of concurrent requests. This can be used "
        "to help simulate an environment where a higher level component "
        "is enforcing a maximum number of concurrent requests. While the "
        "--request-rate argument controls the rate at which requests are "
        "initiated, this argument will control how many are actually allowed "
        "to execute at a time. This means that when used in combination, the "
        "actual request rate may be lower than specified with --request-rate, "
        "if the server is not processing requests fast enough to keep up.",
    )

    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="Name of the model.",
    )
    parser.add_argument(
        "--tokenizer",
        type=str,
        help="Name or path of the tokenizer, if not using the default tokenizer.",  # noqa: E501
    )
    parser.add_argument(
        "--best-of",
        type=int,
        default=1,
        help="Generates `best_of` sequences per prompt and " "returns the best one.",
    )
    parser.add_argument("--use-beam-search", action="store_true")
    parser.add_argument(
        "--num-prompts",
        type=int,
        default=1000,
        help="Number of prompts to process.",
    )
    parser.add_argument(
        "--logprobs",
        type=int,
        default=None,
        help=(
            "Number of logprobs-per-token to compute & return as part of "
            "the request. If unspecified, then either (1) if beam search "
            "is disabled, no logprobs are computed & a single dummy "
            "logprob is returned for each token; or (2) if beam search "
            "is enabled 1 logprob per token is computed"
        ),
    )
    parser.add_argument(
        "--request-rate",
        type=float,
        default=float("inf"),
        help="Number of requests per second. If this is inf, "
        "then all the requests are sent at time 0. "
        "Otherwise, we use Poisson process or gamma distribution "
        "to synthesize the request arrival times.",
    )
    parser.add_argument(
        "--burstiness",
        type=float,
        default=1.0,
        help="Burstiness factor of the request generation. "
        "Only take effect when request_rate is not inf. "
        "Default value is 1, which follows Poisson process. "
        "Otherwise, the request intervals follow a gamma distribution. "
        "A lower burstiness value (0 < burstiness < 1) results in more "
        "bursty requests. A higher burstiness value (burstiness > 1) "
        "results in a more uniform arrival of requests.",
    )
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument(
        "--trust-remote-code",
        action="store_true",
        help="Trust remote code from huggingface",
    )
    parser.add_argument(
        "--disable-tqdm",
        action="store_true",
        help="Specify to disable tqdm progress bar.",
    )
    parser.add_argument(
        "--profile",
        action="store_true",
        help="Use Torch Profiler. The endpoint must be launched with "
        "VLLM_TORCH_PROFILER_DIR to enable profiler.",
    )
    parser.add_argument(
        "--save-result",
        action="store_true",
        help="Specify to save benchmark results to a json file",
    )
    parser.add_argument(
        "--metadata",
        metavar="KEY=VALUE",
        nargs="*",
        help="Key-value pairs (e.g, --metadata version=0.3.3 tp=1) "
        "for metadata of this run to be saved in the result JSON file "
        "for record keeping purposes.",
    )
    parser.add_argument(
        "--result-dir",
        type=str,
        default=None,
        help="Specify directory to save benchmark json results."
        "If not specified, results are saved in the current directory.",
    )
    parser.add_argument(
        "--result-filename",
        type=str,
        default=None,
        help="Specify the filename to save benchmark json results."
        "If not specified, results will be saved in "
        "{backend}-{args.request_rate}qps-{base_model_id}-{current_dt}.json"
        " format.",
    )
    parser.add_argument(
        "--ignore-eos",
        action="store_true",
        help="Set ignore_eos flag when sending the benchmark request."
        "Warning: ignore_eos is not supported in deepspeed_mii and tgi.",
    )
    parser.add_argument(
        "--percentile-metrics",
        type=str,
        default="ttft,tpot,itl,e2el",
        help="Comma-seperated list of selected metrics to report percentils. "
        "This argument specifies the metrics to report percentiles. "
        'Allowed metric names are "ttft", "tpot", "itl", "e2el". '
        'Default value is "ttft,tpot,itl".',
    )
    parser.add_argument(
        "--metric-percentiles",
        type=str,
        default="95,96,97,98,99,100",
        help="Comma-seperated list of percentiles for selected metrics. "
        'To report 25-th, 50-th, and 75-th percentiles, use "25,50,75". '
        'Default value is "99". '
        'Use "--percentile-metrics" to select metrics.',
    )
    parser.add_argument(
        "--goodput",
        nargs="+",
        required=False,
        help='Specify service level objectives for goodput as "KEY:VALUE" '
        "pairs, where the key is a metric name, and the value is in "
        'milliseconds. Multiple "KEY:VALUE" pairs can be provided, '
        "separated by spaces. Allowed request level metric names are "
        '"ttft", "tpot", "e2el". For more context on the definition of '
        "goodput, refer to DistServe paper: https://arxiv.org/pdf/2401.09670 "
        "and the blog: https://hao-ai-lab.github.io/blogs/distserve",
    )

    # group for dataset specific arguments
    sonnet_group = parser.add_argument_group("sonnet dataset options")
    sonnet_group.add_argument(
        "--sonnet-input-len",
        type=int,
        default=550,
        help="Number of input tokens per request, used only for sonnet dataset.",
    )
    sonnet_group.add_argument(
        "--sonnet-output-len",
        type=int,
        default=150,
        help="Number of output tokens per request, used only for sonnet dataset.",
    )
    sonnet_group.add_argument(
        "--sonnet-prefix-len",
        type=int,
        default=200,
        help="Number of prefix tokens per request, used only for sonnet dataset.",
    )

    sharegpt_group = parser.add_argument_group("sharegpt dataset options")
    sharegpt_group.add_argument(
        "--sharegpt-output-len",
        type=int,
        default=None,
        help="Output length for each request. Overrides the output length "
        "from the ShareGPT dataset.",
    )

    random_group = parser.add_argument_group("random dataset options")
    random_group.add_argument(
        "--random-input-len",
        type=int,
        default=1024,
        help="Number of input tokens per request, used only for random sampling.",
    )
    random_group.add_argument(
        "--random-output-len",
        type=int,
        default=128,
        help="Number of output tokens per request, used only for random sampling.",
    )
    random_group.add_argument(
        "--random-range-ratio",
        type=float,
        default=1.0,
        help="Range of sampled ratio of input/output length, " "used only for random sampling.",
    )
    random_group.add_argument(
        "--random-prefix-len",
        type=int,
        default=0,
        help="Number of fixed prefix tokens before random "
        " context. The length range of context in a random "
        " request is [random-prefix-len, "
        " random-prefix-len + random-prefix-len * random-range-ratio).",
    )

    hf_group = parser.add_argument_group("hf dataset options")
    hf_group.add_argument("--hf-subset", type=str, default=None, help="Subset of the HF dataset.")
    hf_group.add_argument("--hf-split", type=str, default=None, help="Split of the HF dataset.")
    hf_group.add_argument(
        "--hf-output-len",
        type=int,
        default=None,
        help="Output length for each request. Overrides the output lengths "
        "from the sampled HF dataset.",
    )

    parser.add_argument(
        "--tokenizer-mode",
        type=str,
        default="auto",
        choices=["auto", "slow", "mistral"],
        help='The tokenizer mode.\n\n* "auto" will use the '
        'fast tokenizer if available.\n* "slow" will '
        "always use the slow tokenizer. \n* "
        '"mistral" will always use the `mistral_common` tokenizer.',
    )

    parser.add_argument(
        "--served-model-name",
        type=str,
        default=None,
        help="The model name used in the API. "
        "If not specified, the model name will be the "
        "same as the ``--model`` argument. ",
    )

    args = parser.parse_args()
    main(args)


================================================================================
File: src/backend/main.py
Size: 5.97 kB
================================================================================

import asyncio
import json
import time
import uuid

import uvicorn
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles

from backend.server.request_handler import RequestHandler
from backend.server.scheduler_manage import SchedulerManage
from backend.server.server_args import parse_args
from backend.server.static_config import (
    get_model_list,
    get_node_join_command,
    init_model_info_dict_cache,
)
from parallax_utils.ascii_anime import display_parallax_run
from parallax_utils.file_util import get_project_root
from parallax_utils.logging_config import get_logger, set_log_level
from parallax_utils.version_check import check_latest_release

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logger = get_logger(__name__)

scheduler_manage = None
request_handler = RequestHandler()


@app.get("/model/list")
async def model_list():
    return JSONResponse(
        content={
            "type": "model_list",
            "data": get_model_list(),
        },
        status_code=200,
    )


@app.post("/scheduler/init")
async def scheduler_init(raw_request: Request):
    request_data = await raw_request.json()
    model_name = request_data.get("model_name")
    init_nodes_num = request_data.get("init_nodes_num")
    is_local_network = request_data.get("is_local_network")

    # Validate required parameters
    if model_name is None:
        return JSONResponse(
            content={
                "type": "scheduler_init",
                "error": "model_name is required",
            },
            status_code=400,
        )
    if init_nodes_num is None:
        return JSONResponse(
            content={
                "type": "scheduler_init",
                "error": "init_nodes_num is required",
            },
            status_code=400,
        )

    try:
        # If scheduler is already running, stop it first
        if scheduler_manage.is_running():
            logger.info(f"Stopping existing scheduler to switch to model: {model_name}")
            scheduler_manage.stop()

        # Start scheduler with new model
        logger.info(
            f"Initializing scheduler with model: {model_name}, init_nodes_num: {init_nodes_num}"
        )
        scheduler_manage.run(model_name, init_nodes_num, is_local_network)

        return JSONResponse(
            content={
                "type": "scheduler_init",
                "data": {
                    "model_name": model_name,
                    "init_nodes_num": init_nodes_num,
                    "is_local_network": is_local_network,
                },
            },
            status_code=200,
        )
    except Exception as e:
        logger.exception(f"Error initializing scheduler: {e}")
        return JSONResponse(
            content={
                "type": "scheduler_init",
                "error": str(e),
            },
            status_code=500,
        )


@app.get("/node/join/command")
async def node_join_command():
    peer_id = scheduler_manage.get_peer_id()
    is_local_network = scheduler_manage.get_is_local_network()

    return JSONResponse(
        content={
            "type": "node_join_command",
            "data": get_node_join_command(peer_id, is_local_network),
        },
        status_code=200,
    )


@app.get("/cluster/status")
async def cluster_status():
    async def stream_cluster_status():
        while True:
            yield json.dumps(scheduler_manage.get_cluster_status(), ensure_ascii=False) + "\n"
            await asyncio.sleep(1)

    return StreamingResponse(
        stream_cluster_status(),
        media_type="application/x-ndjson",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


@app.post("/v1/chat/completions")
async def openai_v1_chat_completions(raw_request: Request):
    request_data = await raw_request.json()
    request_id = uuid.uuid4()
    received_ts = time.time()
    return await request_handler.v1_chat_completions(request_data, request_id, received_ts)


# Disable caching for index.html
@app.get("/")
async def serve_index():
    response = FileResponse(str(get_project_root()) + "/src/frontend/dist/index.html")
    # Disable cache
    response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


# mount the frontend
app.mount(
    "/",
    StaticFiles(directory=str(get_project_root() / "src" / "frontend" / "dist"), html=True),
    name="static",
)

if __name__ == "__main__":
    args = parse_args()
    set_log_level(args.log_level)
    logger.info(f"args: {args}")

    if args.model_name is None:
        init_model_info_dict_cache(args.use_hfcache)

    if args.log_level != "DEBUG":
        display_parallax_run()

    check_latest_release()

    scheduler_manage = SchedulerManage(
        initial_peers=args.initial_peers,
        relay_servers=args.relay_servers,
        dht_prefix=args.dht_prefix,
        host_maddrs=[
            f"/ip4/0.0.0.0/tcp/{args.tcp_port}",
            f"/ip4/0.0.0.0/udp/{args.udp_port}/quic-v1",
        ],
        announce_maddrs=args.announce_maddrs,
        http_port=args.port,
        use_hfcache=args.use_hfcache,
    )

    request_handler.set_scheduler_manage(scheduler_manage)

    model_name = args.model_name
    init_nodes_num = args.init_nodes_num
    is_local_network = args.is_local_network
    if model_name is not None and init_nodes_num is not None:
        scheduler_manage.run(model_name, init_nodes_num, is_local_network)

    host = args.host
    port = args.port

    uvicorn.run(app, host=host, port=port, log_level="info", loop="uvloop")


================================================================================
File: src/backend/server/constants.py
Size: 300 B
================================================================================

# Cluster status constants
CLUSTER_STATUS_WAITING = "waiting"
CLUSTER_STATUS_AVAILABLE = "available"
CLUSTER_STATUS_REBALANCING = "rebalancing"
CLUSTER_STATUS_FAILED = "failed"

# Node status constants
NODE_STATUS_WAITING = "waiting"
NODE_STATUS_AVAILABLE = "available"
NODE_STATUS_FAILED = "failed"


================================================================================
File: src/backend/server/request_handler.py
Size: 6.8 kB
================================================================================

import json
import time
from typing import Dict

import aiohttp
from fastapi.responses import JSONResponse, StreamingResponse
from starlette.concurrency import iterate_in_threadpool

from backend.server.constants import NODE_STATUS_AVAILABLE
from parallax_utils.logging_config import get_logger
from parallax_utils.request_metrics import get_request_metrics

logger = get_logger(__name__)

AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=20 * 60 * 60)


class RequestHandler:
    """HTTP request forwarder with scheduler-aware routing and retry logic.

    Behavior for routing resolution:
    - routing_table is None: scheduler has not decided yet -> treat as error for this attempt
    - routing_table is []: all pipelines are full now -> retry up to max attempts
    - routing_table is non-empty: forward to first hop
    """

    MAX_ROUTING_RETRY = 20
    RETRY_DELAY_SEC = 5

    def __init__(self):
        self.scheduler_manage = None
        self.stubs = {}

    def set_scheduler_manage(self, scheduler_manage):
        self.scheduler_manage = scheduler_manage

    def get_stub(self, node_id):
        if node_id not in self.stubs:
            self.stubs[node_id] = self.scheduler_manage.completion_handler.get_stub(node_id)
        return self.stubs[node_id]

    async def _forward_request(self, request_data: Dict, request_id: str, received_ts: int):
        start_time = time.time()
        logger.debug(f"Forwarding request {request_id}; stream={request_data.get('stream', False)}")
        if (
            self.scheduler_manage is None
            or not self.scheduler_manage.get_schedule_status() == NODE_STATUS_AVAILABLE
        ):
            return JSONResponse(
                content={"error": "Server is not ready"},
                status_code=500,
            )

        # Try to resolve routing; retry if table is an empty list (capacity full)
        attempts = 0
        routing_table = None
        while attempts < self.MAX_ROUTING_RETRY:
            try:
                routing_table = self.scheduler_manage.get_routing_table(request_id, received_ts)
                logger.debug(
                    f"get_routing_table for request {request_id} return: {routing_table} (attempt {attempts+1})"
                )
            except Exception as e:
                logger.exception(f"get_routing_table error: {e}")
                return JSONResponse(
                    content={"error": "Get routing table error"},
                    status_code=500,
                )

            # None -> scheduler has not set yet; treat as hard error (no waiting here)
            if routing_table is None:
                return JSONResponse(
                    content={"error": "Routing pipelines not ready"},
                    status_code=503,
                )

            # Non-empty -> proceed
            if len(routing_table) > 0:
                break

            # Empty list -> capacity full now, retry after short delay
            attempts += 1
            if attempts < self.MAX_ROUTING_RETRY:
                # small async delay before re-forwarding
                import asyncio

                await asyncio.sleep(self.RETRY_DELAY_SEC)

        # If still empty after retries, return 429 Too Many Requests
        if routing_table is not None and len(routing_table) == 0:
            return JSONResponse(
                content={"error": "All pipelines are busy or not ready. Please retry later."},
                status_code=429,
            )

        # Add request_id and routing_table to request_data
        request_data["rid"] = str(request_id)
        request_data["routing_table"] = routing_table
        stub = self.get_stub(routing_table[0])
        is_stream = request_data.get("stream", False)
        try:
            if is_stream:

                async def stream_generator():
                    response = stub.chat_completion(request_data)
                    first_token_time = None
                    last_chunk = None
                    last_token_time = None
                    try:
                        iterator = iterate_in_threadpool(response)
                        async for chunk in iterator:
                            last_token_time = time.time()
                            if first_token_time is None:
                                first_token_time = last_token_time
                            if chunk is not None and not chunk.decode("utf-8").startswith(
                                "data: [DONE]"
                            ):
                                last_chunk = chunk
                            yield chunk
                    finally:
                        if last_chunk is not None:
                            tps, ttft, input_tokens, output_tokens = get_request_metrics(
                                last_chunk, start_time, first_token_time, last_token_time
                            )
                            if (
                                tps is not None
                                and ttft is not None
                                and input_tokens is not None
                                and output_tokens is not None
                            ):
                                logger.info(
                                    f"Request ID: {request_id} | TPS: {tps:.2f} |  TTFT: {ttft} ms | Output tokens: {output_tokens} | Input tokens: {input_tokens}"
                                )
                        logger.debug(f"client disconnected for {request_id}")
                        response.cancel()

                resp = StreamingResponse(
                    stream_generator(),
                    media_type="text/event-stream",
                    headers={
                        "X-Content-Type-Options": "nosniff",
                        "Cache-Control": "no-cache",
                    },
                )
                logger.debug(f"Streaming response initiated for {request_id}")
                return resp
            else:
                response = stub.chat_completion(request_data)
                content = (await anext(iterate_in_threadpool(response))).decode()
                logger.debug(f"Non-stream response completed for {request_id}")
                # response is a JSON string; parse to Python object before returning
                return JSONResponse(content=json.loads(content))
        except Exception as e:
            logger.exception(f"Error in _forward_request: {e}")
            return JSONResponse(
                content={"error": "Internal server error"},
                status_code=500,
            )

    async def v1_chat_completions(self, request_data: Dict, request_id: str, received_ts: int):
        return await self._forward_request(request_data, request_id, received_ts)


================================================================================
File: src/backend/server/rpc_connection_handler.py
Size: 8.47 kB
================================================================================

import time

from lattica import ConnectionHandler, Lattica, rpc_method, rpc_stream, rpc_stream_iter

from parallax_utils.logging_config import get_logger
from scheduling.node import Node, NodeHardwareInfo
from scheduling.scheduler import Scheduler

logger = get_logger(__name__)

import json

import httpx


class RPCConnectionHandler(ConnectionHandler):
    """
    Handles RPC requests from clients, forwarding them to the appropriate TransformerBackend.
    Inherits from hivemind's ConnectionHandler.
    """

    def __init__(
        self,
        lattica: Lattica,
        scheduler: Scheduler,
        http_port: int,
    ):
        # Initialize the base class
        super().__init__(lattica)
        self.scheduler = scheduler
        self.http_port = http_port

    @rpc_stream
    def node_join(self, message):
        # node = {
        #     "node_id": "lattica peer id",
        #     "hardware": {
        #         "node_id": "lattica peer id",
        #         "tflops_fp16": 100,
        #         "memory_gb": 100,
        #         "memory_bandwidth_gbps": 100,
        #     },
        #     "kvcache_mem_ratio": 0.3,
        #     "param_mem_ratio": 0.5,
        #     "max_concurrent_requests": 16,
        #     "max_sequence_length": 1024,
        # }
        logger.info(f"receive node_join request: {message}")
        try:
            node = self.build_node(message)
            self.scheduler.enqueue_join(node)

            response = self.wait_layer_allocation(node.node_id, wait_seconds=300)
            logger.debug(f"node_join response: {response}")
            return response
        except Exception as e:
            logger.exception(f"node_join error: {e}")
            return {}

    @rpc_method
    def node_leave(self, message):
        logger.debug(f"receive node_leave request: {message}")
        try:
            node = self.build_node(message)
            self.scheduler.enqueue_leave(node.node_id)
            return {}
        except Exception as e:
            logger.exception(f"node_leave error: {e}")
            return {}

    @rpc_method
    def node_update(self, message):
        logger.debug(f"receive node_update request: {message}")
        try:
            node = self.build_node(message)
            # Check if node exists in scheduler
            if node.node_id not in self.scheduler.node_id_to_node:
                # Node not found, automatically join it (e.g., after model switch)
                logger.info(
                    f"Node {node.node_id} not found in scheduler, auto-joining via node_update"
                )
                self.scheduler.enqueue_join(node)
                # Wait a bit for join to be processed
                time.sleep(0.1)
                # Return layer allocation after join
                layer_allocation = self.wait_layer_allocation(node.node_id, wait_seconds=5)
                return layer_allocation

            # Node exists, update its info
            self.scheduler.enqueue_node_update(
                node.node_id,
                current_requests=node.current_requests,
                layer_latency_ms=node.layer_latency_ms,
                new_rtt_to_nodes=node.rtt_to_nodes,
                is_active=node.is_active,
            )
            # Return current layer allocation to node
            layer_allocation = self.get_layer_allocation(node.node_id)
            return layer_allocation
        except Exception as e:
            logger.exception(f"node_update error: {e}")
            return {}

    @rpc_stream_iter
    def chat_completion(
        self,
        request,
    ):
        """Handle chat completion request"""
        logger.debug(f"Chat completion request: {request}, type: {type(request)}")
        try:
            with httpx.Client(timeout=10 * 60, proxy=None, trust_env=False) as client:
                if request.get("stream", False):
                    with client.stream(
                        "POST",
                        f"http://localhost:{self.http_port}/v1/chat/completions",
                        json=request,
                    ) as response:
                        for chunk in response.iter_bytes():
                            if chunk:
                                yield chunk
                else:
                    response = client.post(
                        f"http://localhost:{self.http_port}/v1/chat/completions", json=request
                    ).json()
                    yield json.dumps(response).encode()
        except Exception as e:
            logger.exception(f"Error in chat completion: {e}")
            yield b"internal server error"

    @rpc_stream_iter
    def cluster_status(self):
        try:
            with httpx.Client(timeout=10 * 60, proxy=None, trust_env=False) as client:
                with client.stream(
                    "GET", f"http://localhost:{self.http_port}/cluster/status"
                ) as response:
                    for chunk in response.iter_bytes():
                        if chunk:
                            yield chunk
        except Exception as e:
            logger.exception(f"Error in cluster status: {e}")
            yield json.dumps({"error": "internal server error"}).encode()

    def wait_layer_allocation(self, current_node_id, wait_seconds):
        start_time = time.time()
        while True:
            layer_allocation = self.get_layer_allocation(current_node_id)
            if layer_allocation:
                return layer_allocation
            if time.time() - start_time > wait_seconds:
                return {}
            time.sleep(0.5)

    def get_layer_allocation(self, current_node_id):
        list_node_allocations = self.scheduler.list_node_allocations()
        for node_id, start_layer, end_layer in list_node_allocations:
            if current_node_id == node_id:
                node = self.scheduler.node_id_to_node.get(node_id)
                if node:
                    return {
                        "node_id": node_id,
                        "model_name": (
                            node.model_info.model_name
                            if node.hardware.device != "mlx"
                            else node.model_info.mlx_model_name
                        ),
                        "start_layer": start_layer,
                        "end_layer": end_layer,
                        "tp_size": node.hardware.num_gpus,
                    }
        return {}

    def build_node(self, node_json: dict):
        node = Node(
            node_id=node_json.get("node_id"),
            hardware=self.build_hardware(node_json.get("hardware")),
            model_info=self.scheduler.model_info,
            kvcache_mem_ratio=node_json.get("kvcache_mem_ratio"),
            param_mem_ratio=node_json.get("param_mem_ratio"),
            max_concurrent_requests=node_json.get("max_concurrent_requests"),
            max_sequence_length=node_json.get("max_sequence_length"),
            is_active=node_json.get("is_active", True),
            manual_layer_assignment=node_json.get("manual_layer_assignment", False),
        )
        if node_json.get("start_layer", None) is not None:
            node.start_layer = node_json.get("start_layer")
        if node_json.get("end_layer", None) is not None:
            node.end_layer = node_json.get("end_layer")
        if node_json.get("current_requests", None) is not None:
            node.current_requests = node_json.get("current_requests")
        if node_json.get("layer_latency_ms", None) is not None:
            node.avg_layer_latency_ms = node_json.get("layer_latency_ms")
        if node_json.get("rtt_to_nodes", None) is not None:
            node.rtt_to_nodes = node_json.get("rtt_to_nodes")
        return node

    def build_hardware(self, hardware_json):
        node_id = hardware_json.get("node_id")
        num_gpus = hardware_json.get("num_gpus")
        tflops_fp16 = hardware_json.get("tflops_fp16")
        gpu_name = hardware_json.get("gpu_name")
        memory_gb = hardware_json.get("memory_gb")
        memory_bandwidth_gbps = hardware_json.get("memory_bandwidth_gbps")
        device = hardware_json.get("device")
        return NodeHardwareInfo(
            node_id=node_id,
            num_gpus=num_gpus,
            tflops_fp16=tflops_fp16,
            gpu_name=gpu_name,
            memory_gb=memory_gb,
            memory_bandwidth_gbps=memory_bandwidth_gbps,
            device=device,
        )


================================================================================
File: src/backend/server/scheduler_manage.py
Size: 12.19 kB
================================================================================

import threading
import time
from typing import List

from lattica import Lattica

from backend.server.constants import NODE_STATUS_AVAILABLE, NODE_STATUS_WAITING
from backend.server.rpc_connection_handler import RPCConnectionHandler
from backend.server.static_config import get_model_info, get_node_join_command
from parallax.cli import PUBLIC_INITIAL_PEERS, PUBLIC_RELAY_SERVERS
from parallax.p2p.server import TransformerConnectionHandler
from parallax_utils.logging_config import get_logger
from scheduling.node import RequestSignal
from scheduling.scheduler import Scheduler

logger = get_logger(__name__)


class SchedulerManage:
    """
    Coordinates the in-process scheduler and the P2P RPC layer.

    This manager owns the `Scheduler` instance and the Lattica P2P node,
    wiring RPC calls from workers to scheduler events.
    """

    def __init__(
        self,
        initial_peers: List[str] = [],
        relay_servers: List[str] = [],
        dht_prefix: str = "gradient",
        host_maddrs: List[str] = [],
        announce_maddrs: List[str] = [],
        http_port: int = 3001,
        use_hfcache: bool = False,
    ):
        """Initialize the manager with networking bootstrap parameters."""
        self.initial_peers = initial_peers
        self.relay_servers = relay_servers
        self.dht_prefix = dht_prefix
        self.host_maddrs = host_maddrs
        self.announce_maddrs = announce_maddrs
        self.http_port = http_port
        self.use_hfcache = use_hfcache
        self.model_name = None
        self.init_nodes_num = None
        self.scheduler = None
        self.node_id = f"{dht_prefix}_announce"
        self.lattica = None
        self.stubs = {}
        self.is_local_network = False

    def run(self, model_name, init_nodes_num, is_local_network=True):
        """
        Start the scheduler and the P2P service for RPC handling.
        If Lattica is already running, it will be reused.
        Nodes will automatically rejoin via their heartbeat (node_update) mechanism.
        """
        logger.debug(
            f"SchedulerManage starting: model_name={model_name}, init_nodes_num={init_nodes_num}"
        )
        self.is_local_network = is_local_network
        if not is_local_network and not self.initial_peers and not self.relay_servers:
            logger.debug("Using public relay servers")
            self.initial_peers = PUBLIC_INITIAL_PEERS
            self.relay_servers = PUBLIC_RELAY_SERVERS

        self._start_scheduler(model_name, init_nodes_num)
        self._start_lattica()
        self.completion_handler = TransformerConnectionHandler(
            lattica=self.lattica,
            recv_from_peer_addr="",
            send_to_peer_addr="",
            block_start_index=0,
            block_end_index=1,
        )

    def is_running(self):
        """
        Returns True if the scheduler is running, False otherwise.
        """
        return self.scheduler is not None

    def stop(self):
        """
        Stop the scheduler only. Lattica will remain running.
        """
        logger.info("Stopping scheduler...")

        # Stop scheduler if running
        if self.scheduler is not None:
            logger.debug("Stopping scheduler...")
            self.scheduler._stop_event.set()
            # Wait a bit for threads to finish
            time.sleep(0.1)
            self.scheduler = None
            logger.debug("Scheduler stopped")

        # Note: We don't close Lattica here to allow model switching without restarting P2P

        logger.info("Scheduler stopped")

    def get_model_name(self):
        return self.model_name

    def get_init_nodes_num(self):
        return self.init_nodes_num

    def get_is_local_network(self):
        return self.is_local_network

    def get_peer_id(self):
        if self.lattica is None:
            return None
        return self.lattica.peer_id()

    def need_more_nodes(self):
        return self.scheduler.need_more_nodes() if self.scheduler else False

    def get_cluster_status(self):
        return {
            "type": "cluster_status",
            "data": {
                "status": self.get_schedule_status(),
                "model_name": self.model_name,
                "init_nodes_num": self.init_nodes_num,
                "node_join_command": get_node_join_command(
                    self.get_peer_id(), self.is_local_network
                ),
                "node_list": self.get_node_list(),
                "need_more_nodes": self.need_more_nodes(),
            },
        }

    def get_node_list(self):
        if self.scheduler is None:
            return []

        return [self.build_node_info(node) for node in self.scheduler.nodes]

    def build_node_info(self, node):
        return {
            "node_id": node.node_id,
            "status": NODE_STATUS_AVAILABLE if node.is_active else NODE_STATUS_WAITING,
            "gpu_num": node.hardware.num_gpus,
            "gpu_name": node.hardware.gpu_name,
            "gpu_memory": node.hardware.memory_gb,
        }

    def _start_scheduler(self, model_name, init_nodes_num):
        """
        Create the scheduler and start its background run loop.
        If scheduler already exists, it will be stopped and recreated.
        Nodes will automatically rejoin via their heartbeat (node_update) mechanism.
        """
        # Stop existing scheduler if running
        if self.scheduler is not None:
            logger.info("Scheduler already running, stopping it first for re-initialization")
            self.stop()

        self.model_name = model_name
        self.init_nodes_num = init_nodes_num

        model_info = get_model_info(model_name, self.use_hfcache)
        self.scheduler = Scheduler(model_info, [], min_nodes_bootstrapping=init_nodes_num)

        # Run the scheduler's event/dispatch loops in background so the process
        # can continue to serve RPCs and HTTP traffic.
        threading.Thread(
            target=self.scheduler.run,
            kwargs={"poll_interval": 0.05},
            name="SchedulerMain",
            daemon=True,
        ).start()
        logger.debug("Scheduler background thread started (poll_interval=0.05)")
        logger.info("Nodes will automatically rejoin via heartbeat (node_update) mechanism")

    def _start_lattica(self):
        """
        Initialize and start the Lattica P2P node used for RPCs.
        If Lattica already exists, it will be reused (no restart), but connection_handler will be updated.
        """
        # Reuse existing Lattica if running
        if self.lattica is not None:
            logger.debug("Lattica already running, reusing existing instance")
            # Update connection handler with new scheduler if it exists
            if hasattr(self, "connection_handler") and self.connection_handler is not None:
                self.connection_handler.scheduler = self.scheduler
                logger.debug("Updated connection handler with new scheduler")
            else:
                # Create connection handler if it doesn't exist
                self.connection_handler = RPCConnectionHandler(
                    lattica=self.lattica,
                    scheduler=self.scheduler,
                    http_port=self.http_port,
                )
                logger.debug("Created connection handler with existing Lattica")
            return

        logger.debug(
            f"Starting Lattica with host_maddrs={self.host_maddrs}, mdns=False, dht_prefix={self.dht_prefix}"
        )
        self.lattica = Lattica.builder().with_listen_addrs(self.host_maddrs).with_key_path(".")

        if len(self.relay_servers) > 0:
            logger.info(f"Using relay servers: {self.relay_servers}")
            self.lattica.with_relay_servers(self.relay_servers).with_dcutr(True).with_protocol("")

        if len(self.announce_maddrs) > 0:
            logger.info(f"Using announce maddrs: {self.announce_maddrs}")
            self.lattica.with_external_addrs(self.announce_maddrs)

        if len(self.initial_peers) > 0:
            logger.info(f"Using initial peers: {self.initial_peers}")
            self.lattica.with_bootstraps(self.initial_peers)

        self.lattica.build()
        logger.debug("Lattica node built")

        if len(self.relay_servers) > 0:
            try:
                is_symmetric_nat = self.lattica.is_symmetric_nat()
                if is_symmetric_nat is None:
                    logger.warning("Failed to get is symmetric NAT, skip")
                elif is_symmetric_nat:
                    logger.error(
                        "Your network NAT type is symmetric, relay does not work on this type of NAT, see https://en.wikipedia.org/wiki/Network_address_translation"
                    )
                    exit(1)
            except Exception as e:
                logger.exception(f"Error in is symmetric NAT: {e}")

        store_success = False
        for _ in range(10):
            try:
                if self.lattica.store(
                    "scheduler_peer_id",
                    self.lattica.peer_id(),
                    expiration_time=time.time() + 365 * 24 * 60 * 60,
                ):
                    logger.info(f"Stored scheduler peer id: {self.lattica.peer_id()}")
                    store_success = True
                    break
                logger.warning("Failed to store scheduler peer id, waiting for 10 seconds")
                time.sleep(10)
            except Exception as e:
                logger.error(f"Failed to store scheduler peer id: {e}, waiting for 10 seconds")
                time.sleep(10)

        if not store_success:
            logger.error("Failed to store scheduler peer id, after 10 times")
            exit(1)

        self.connection_handler = RPCConnectionHandler(
            lattica=self.lattica,
            scheduler=self.scheduler,
            http_port=self.http_port,
        )
        logger.debug("RPCConnectionHandler initialized")

    def get_routing_table(self, request_id, received_ts):
        """Block briefly until the scheduler assigns a routing path for the request.

        Distinguish three states via `RequestSignal.routing_table`:
        - None: not yet decided, keep waiting up to timeout
        - []: decided but no capacity (pipelines full), return immediately
        - [..]: valid routing path, return immediately
        """
        logger.debug(f"Routing table requested for request_id={request_id}")
        request = RequestSignal(request_id, received_ts)
        self.scheduler.receive_request(request)

        # Wait up to 5 seconds, but return immediately if the routing table is set (including an empty list)
        start_time = time.time()
        while request.routing_table is None and (time.time() - start_time) < 5.0:
            time.sleep(0.05)

        # Return the routing_table
        if request.routing_table is None:
            logger.debug(
                f"Routing table not ready after {(time.time() - start_time):.2f}s for request_id={request_id}"
            )
        else:
            logger.debug(
                f"Routing table resolved for request_id={request_id}: {request.routing_table}"
            )
        return request.routing_table

    def get_schedule_status(self):
        """
        Return whether a full pipeline has been allocated across joined nodes.
        """
        if self.scheduler is None:
            logger.debug("SchedulerManage status queried: waiting (scheduler not initialized)")
            return NODE_STATUS_WAITING

        # todo rebalance status
        status = (
            NODE_STATUS_AVAILABLE
            if self.scheduler.layer_allocator.has_full_pipeline(active_only=True)
            else NODE_STATUS_WAITING
        )
        logger.debug(f"SchedulerManage status queried: {status}")
        return status

    def get_call_url_by_node_id(self, node_id):
        """
        Lookup the HTTP endpoint for a given node id managed by the RPC layer.
        """
        url = self.connection_handler.get_call_url_by_node_id(node_id)
        logger.debug(f"Lookup call_url for node_id={node_id} -> {url}")
        return url


================================================================================
File: src/backend/server/server_args.py
Size: 1.81 kB
================================================================================

import argparse

from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Lattica configuration
    parser.add_argument("--initial-peers", nargs="+", default=[], help="List of initial DHT peers")
    parser.add_argument("--relay-servers", nargs="+", default=[], help="List of relay DHT peers")
    parser.add_argument(
        "--announce-maddrs", nargs="+", default=[], help="List of multiaddresses to announce"
    )
    parser.add_argument("--tcp-port", type=int, default=0, help="Port for Lattica TCP listening")
    parser.add_argument("--udp-port", type=int, default=0, help="Port for Lattica UDP listening")
    parser.add_argument("--dht-prefix", type=str, default="gradient", help="Prefix for DHT keys")

    # Scheduler configuration
    parser.add_argument("--host", type=str, default="localhost", help="Host to listen on")
    parser.add_argument("--port", type=int, default=3001, help="Port to listen on")
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Log level",
    )
    parser.add_argument("--model-name", type=str, default=None, help="Model name")
    parser.add_argument("--init-nodes-num", type=int, default=None, help="Number of initial nodes")
    parser.add_argument(
        "--is-local-network", type=bool, default=True, help="Whether to use local network"
    )
    parser.add_argument(
        "--use-hfcache",
        action="store_true",
        default=False,
        help="Use local Hugging Face cache only (no network download)",
    )

    args = parser.parse_args()

    return args


================================================================================
File: src/backend/server/static_config.py
Size: 10.22 kB
================================================================================

import concurrent.futures
import json
import math
from pathlib import Path

from parallax_utils.logging_config import get_logger
from scheduling.model_info import ModelInfo

logger = get_logger(__name__)

# Supported model list - key: model name, value: MLX model name (same as key if no MLX variant)
MODELS = {
    # =============================== for quickly test ===================================#
    "Qwen/Qwen3-0.6B": "Qwen/Qwen3-0.6B",
    # ======================================= End ========================================#
    #
    #
    #
    # ===============================newly added models===================================#
    # Moonshot Kimi Models
    "moonshotai/Kimi-K2-Instruct": "mlx-community/Kimi-K2-Instruct-4bit",
    "moonshotai/Kimi-K2-Instruct-0905": "mlx-community/Kimi-K2-Instruct-0905-mlx-DQ3_K_M",
    "moonshotai/Kimi-K2-Thinking": "mlx-community/Kimi-K2-Thinking",
    # OpenAI GPT-OSS Models
    "openai/gpt-oss-20b": "mlx-community/gpt-oss-20b-MXFP4-Q8",
    "openai/gpt-oss-120b": "mlx-community/gpt-oss-120b-4bit",
    "openai/gpt-oss-safeguard-20b": "lmstudio-community/gpt-oss-safeguard-20b-MLX-MXFP4",
    "openai/gpt-oss-safeguard-120b": "lmstudio-community/gpt-oss-safeguard-120b-MLX-MXFP4",
    # zai-org GLM4 Models
    "zai-org/GLM-4.6": "mlx-community/GLM-4.6-4bit",
    "zai-org/GLM-4.6-FP8": "mlx-community/GLM-4.6-4bit",
    "zai-org/GLM-4.5-Air": "lmstudio-community/GLM-4.5-Air-MLX-8bit",
    # Other Models
    "MiniMaxAI/MiniMax-M2": "mlx-community/MiniMax-M2-4bit",
    # ======================================= End ========================================#
    #
    #
    #
    # =============================== Major Models =====================================#
    # DeepSeek Models
    "deepseek-ai/DeepSeek-V3.1": "mlx-community/DeepSeek-V3.1-4bit",
    "deepseek-ai/DeepSeek-V3": "mlx-community/DeepSeek-V3-4bit",
    "deepseek-ai/DeepSeek-V2.5-1210": "mlx-community/DeepSeek-V2.5-1210-4bit",
    "deepseek-ai/DeepSeek-R1": "mlx-community/DeepSeek-R1-4bit",
    # Qwen 2.5 Series
    "Qwen/Qwen2.5-0.5B-Instruct": "Qwen/Qwen2.5-0.5B-Instruct",
    "Qwen/Qwen2.5-1.5B-Instruct": "Qwen/Qwen2.5-1.5B-Instruct",
    "Qwen/Qwen2.5-3B-Instruct": "Qwen/Qwen2.5-3B-Instruct",
    "Qwen/Qwen2.5-7B-Instruct": "Qwen/Qwen2.5-7B-Instruct",
    "Qwen/Qwen2.5-14B-Instruct": "Qwen/Qwen2.5-14B-Instruct",
    "Qwen/Qwen2.5-32B-Instruct": "Qwen/Qwen2.5-32B-Instruct",
    "Qwen/Qwen2.5-72B-Instruct": "Qwen/Qwen2.5-72B-Instruct",
    # Qwen 3 Series (small models)
    "Qwen/Qwen3-0.6B-FP8": "Qwen/Qwen3-0.6B-MLX-8bit",
    "Qwen/Qwen3-1.7B": "Qwen/Qwen3-1.7B",
    "Qwen/Qwen3-1.7B-FP8": "Qwen/Qwen3-1.7B-MLX-8bit",
    "Qwen/Qwen3-4B": "Qwen/Qwen3-4B",
    "Qwen/Qwen3-4B-FP8": "Qwen/Qwen3-4B-MLX-8bit",
    "Qwen/Qwen3-4B-Instruct-2507": "Qwen/Qwen3-4B-Instruct-2507",
    "Qwen/Qwen3-4B-Instruct-2507-FP8": "lmstudio-community/Qwen3-4B-Instruct-2507-MLX-8bit",
    "Qwen/Qwen3-4B-Thinking-2507": "Qwen/Qwen3-4B-Thinking-2507",
    "Qwen/Qwen3-4B-Thinking-2507-FP8": "lmstudio-community/Qwen3-4B-Thinking-2507-MLX-8bit",
    "Qwen/Qwen3-8B": "Qwen/Qwen3-8B",
    "Qwen/Qwen3-8B-FP8": "Qwen/Qwen3-8B-MLX-8bit",
    "Qwen/Qwen3-14B": "Qwen/Qwen3-14B",
    "Qwen/Qwen3-14B-FP8": "Qwen/Qwen3-14B-MLX-8bit",
    "Qwen/Qwen3-32B": "Qwen/Qwen3-32B",
    "Qwen/Qwen3-32B-FP8": "Qwen/Qwen3-32B-MLX-8bit",
    # Qwen 3 MoE Models
    "Qwen/Qwen3-30B-A3B": "Qwen/Qwen3-30B-A3B",
    "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8": "lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-8bit",
    "Qwen/Qwen3-30B-A3B-Thinking-2507-FP8": "lmstudio-community/Qwen3-30B-A3B-Thinking-2507-MLX-8bit",
    # Qwen 3 Next Series
    "Qwen/Qwen3-Next-80B-A3B-Instruct": "mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit",
    "Qwen/Qwen3-Next-80B-A3B-Instruct-FP8": "mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit",
    "Qwen/Qwen3-Next-80B-A3B-Thinking": "mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit",
    "Qwen/Qwen3-Next-80B-A3B-Thinking-FP8": "mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit",
    # Qwen 3 Large MoE Models
    "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8": "mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit",
    "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8": "mlx-community/Qwen3-235B-A22B-Thinking-2507-8bit",
    "Qwen/Qwen3-235B-A22B-GPTQ-Int4": "mlx-community/Qwen3-235B-A22B-4bit",
    # Llama Models
    "nvidia/Llama-3.1-8B-Instruct-FP8": "mlx-community/Meta-Llama-3.1-8B-Instruct-8bit",
    "nvidia/Llama-3.1-70B-Instruct-FP8": "mlx-community/Meta-Llama-3.1-70B-Instruct-8bit",
    "nvidia/Llama-3.3-70B-Instruct-FP8": "mlx-community/Llama-3.3-70B-Instruct-8bit",
    # ======================================= End ========================================#
}

NODE_JOIN_COMMAND_LOCAL_NETWORK = """parallax join"""

NODE_JOIN_COMMAND_PUBLIC_NETWORK = """parallax join -s {scheduler_addr} """


def get_model_info(model_name, use_hfcache: bool = False):
    def _load_config_only(name: str) -> dict:
        local_path = Path(name)
        if local_path.exists():
            config_path = local_path / "config.json"
            with open(config_path, "r") as f:
                return json.load(f)

        # Hugging Face only â€“ download just config.json
        from huggingface_hub import hf_hub_download  # type: ignore

        config_file = hf_hub_download(
            repo_id=name, filename="config.json", local_files_only=use_hfcache
        )
        with open(config_file, "r") as f:
            return json.load(f)

    config = _load_config_only(model_name)

    quant_method = config.get("quant_method", None)
    quantization_config = config.get("quantization_config", None)
    if quant_method is None and quantization_config is not None:
        quant_method = quantization_config.get("quant_method", None)

    if quant_method is None:
        param_bytes_per_element = 2
    elif quant_method == "fp8":
        param_bytes_per_element = 1
    elif quant_method in ("mxfp4", "int4", "awq", "gptq", "compressed-tensors"):
        param_bytes_per_element = 0.5
    else:
        param_bytes_per_element = 1
        logger.warning(
            f"model_name:{model_name} quant_method {quant_method} not supported in get_model_info method"
        )

    mlx_param_bytes_per_element = param_bytes_per_element
    mlx_model_name = MODELS.get(model_name, model_name)

    if mlx_model_name != model_name:
        mlx_config = _load_config_only(mlx_model_name)
        mlx_quant_dict = mlx_config.get("quantization_config", None)
        if mlx_quant_dict and "bits" in mlx_quant_dict:
            mlx_param_bytes_per_element = mlx_quant_dict["bits"] / 8

    # get local experts
    num_local_experts = config.get("num_local_experts", None)
    if num_local_experts is None:
        num_local_experts = config.get("num_experts", None)
    if num_local_experts is None:
        num_local_experts = config.get("n_routed_experts", None)

    model_info = ModelInfo(
        model_name=model_name,
        mlx_model_name=mlx_model_name,
        head_size=config.get("head_dim", 128),
        qk_nope_head_dim=config.get("qk_nope_head_dim", None),
        qk_rope_head_dim=config.get("qk_rope_head_dim", None),
        hidden_dim=config.get("hidden_size", 0),
        intermediate_dim=config.get("intermediate_size", 0),
        num_attention_heads=config.get("num_attention_heads", 0),
        num_kv_heads=config.get("num_key_value_heads", 0),
        vocab_size=config.get("vocab_size", 0),
        num_layers=config.get("num_hidden_layers", 0),
        ffn_num_projections=3,
        param_bytes_per_element=param_bytes_per_element,
        mlx_param_bytes_per_element=mlx_param_bytes_per_element,
        cache_bytes_per_element=2,
        embedding_bytes_per_element=2,
        num_local_experts=num_local_experts,
        num_experts_per_tok=config.get("num_experts_per_tok", None),
        moe_intermediate_dim=config.get("moe_intermediate_size", None),
    )
    return model_info


def get_model_info_with_try_catch(model_name, use_hfcache: bool = False):
    try:
        return get_model_info(model_name, use_hfcache)
    except Exception as e:
        logger.debug(f"Error loading config.json for {model_name}: {e}")
        return None


def get_model_info_dict(use_hfcache: bool = False):
    model_name_list = list(MODELS.keys())
    with concurrent.futures.ThreadPoolExecutor() as executor:
        model_info_dict = dict(
            executor.map(
                lambda name: (name, get_model_info_with_try_catch(name, use_hfcache)),
                model_name_list,
            )
        )
    return model_info_dict


model_info_dict_cache = None


def init_model_info_dict_cache(use_hfcache: bool = False):
    global model_info_dict_cache
    if model_info_dict_cache is not None:
        return
    model_info_dict_cache = get_model_info_dict(use_hfcache)


def get_model_info_dict_cache():
    if model_info_dict_cache is None:
        return {}
    return model_info_dict_cache


def get_model_list():
    model_name_list = list(MODELS.keys())
    model_info_dict = get_model_info_dict_cache()

    def build_single_model(model_name, model_info):
        return {
            "name": model_name,
            "vram_gb": math.ceil(estimate_vram_gb_required(model_info)),
        }

    results = [
        build_single_model(model_name, model_info_dict.get(model_name, None))
        for model_name in model_name_list
    ]
    return results


def estimate_vram_gb_required(model_info):
    if model_info is None:
        return 0

    param_mem_ratio = 0.65
    return (
        (
            model_info.embedding_io_bytes
            + model_info.num_layers * model_info.decoder_layer_io_bytes(roofline=False)
        )
        * 1.0
        / 1024
        / 1024
        / 1024
        / param_mem_ratio
    )


def get_node_join_command(scheduler_addr, is_local_network):
    if scheduler_addr:
        if is_local_network:
            return {
                "command": NODE_JOIN_COMMAND_LOCAL_NETWORK.format(scheduler_addr=scheduler_addr),
            }
        else:
            return {
                "command": NODE_JOIN_COMMAND_PUBLIC_NETWORK.format(scheduler_addr=scheduler_addr),
            }
    else:
        return None


================================================================================
File: src/frontend/.gitignore
Size: 278 B
================================================================================

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
!dist
!dist-ssr
*.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
!.vscode/settings.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?


================================================================================
File: src/frontend/.vscode/extensions.json
Size: 52 B
================================================================================

{
  "recommendations": ["esbenp.prettier-vscode"]
}


================================================================================
File: src/frontend/.vscode/settings.json
Size: 46 B
================================================================================

{
  "prettier.configPath": "./package.json"
}


================================================================================
File: src/frontend/README.md
Size: 1.01 kB
================================================================================

# Parallax Web UI

This is the front-end source code for Parallax, based on React and build by Vite.

## Build

Run the command to build this project (you need to prepare the front-end environment):

```bash
pnpm run build
```

The output directory is `./dist`.

## Local Debugging and Development

Prepare front-end environment (MacOS or Linux):

- `nvm` to install and manage versions of Node.js.
- `pnpm` as package manager to install dependencies.

```bash
# Download and install nvm:
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash

# in lieu of restarting the shell
\. "$HOME/.nvm/nvm.sh"

# Download and install Node.js:
nvm install 22

# Verify the Node.js version:
node -v # Should print "v22.19.0".

# Download and install pnpm:
corepack enable pnpm

# Verify pnpm version:
pnpm -v
```

Install dependencies:

```bash
pnpm install
```

Run the hot-reload preview service:

```bash
pnpm run dev
```

Open the url `http://localhost:5173`, edit code and preview in browser.


================================================================================
File: src/frontend/chat.html
Size: 395 B
================================================================================

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="./src/assets/gradient-icon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CHAT Parallax by Gradient</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/chat.tsx"></script>
  </body>
</html>


================================================================================
File: src/frontend/dist/assets/App-J8fDKCu3.css
Size: 292 B
================================================================================

*,*:before,*:after{box-sizing:border-box}:root{font-size:16px}html,body,#root{position:relative;width:100vw;height:100vh;margin:0;padding:0;overflow:hidden}input::-webkit-outer-spin-button,input::-webkit-inner-spin-button{-webkit-appearance:none}input[type=number]{-moz-appearance:textfield}


================================================================================
File: src/frontend/dist/assets/chat-CLr2c_Zd.js
Size: 119 B
================================================================================

import{c as t,j as e,C as o}from"./App-BlsOxMPe.js";t.createRoot(document.getElementById("root")).render(e.jsx(o,{}));


================================================================================
File: src/frontend/dist/assets/chat-OiBmCmmc.js
Size: 13.39 kB
================================================================================

import{a as H,g as N,r as b,u as W,j as n,s as T,b as U,d as z,e as E,m as G,i as ze,v as Re,w as Me,x as I,k as te,o as Le,y as je,n as $,S as le,q as ie}from"./App-BlsOxMPe.js";import{i as ee,b as qe,c as de,F as $e,u as oe,f as re,O as Ae,d as He,e as Ne,S as We,D as Ue,g as Ee,C as Be}from"./main-layout-BciTM2Vs.js";function De(e){return H("MuiFormControl",e)}N("MuiFormControl",["root","marginNone","marginNormal","marginDense","fullWidth","disabled"]);const Oe=e=>{const{classes:t,margin:o,fullWidth:r}=e,s={root:["root",o!=="none"&&`margin${z(o)}`,r&&"fullWidth"]};return E(s,De,t)},_e=T("div",{name:"MuiFormControl",slot:"Root",overridesResolver:(e,t)=>{const{ownerState:o}=e;return[t.root,t[`margin${z(o.margin)}`],o.fullWidth&&t.fullWidth]}})({display:"inline-flex",flexDirection:"column",position:"relative",minWidth:0,padding:0,margin:0,border:0,verticalAlign:"top",variants:[{props:{margin:"normal"},style:{marginTop:16,marginBottom:8}},{props:{margin:"dense"},style:{marginTop:8,marginBottom:4}},{props:{fullWidth:!0},style:{width:"100%"}}]}),Ke=b.forwardRef(function(t,o){const r=W({props:t,name:"MuiFormControl"}),{children:s,className:p,color:i="primary",component:d="div",disabled:l=!1,error:m=!1,focused:u,fullWidth:f=!1,hiddenLabel:h=!1,margin:a="none",required:v=!1,size:x="medium",variant:c="outlined",...C}=r,P={...r,color:i,component:d,disabled:l,error:m,fullWidth:f,hiddenLabel:h,margin:a,required:v,size:x,variant:c},J=Oe(P),[F,Q]=b.useState(()=>{let y=!1;return s&&b.Children.forEach(s,g=>{if(!ee(g,["Input","Select"]))return;const j=ee(g,["Select"])?g.props.input:g;j&&qe(j.props)&&(y=!0)}),y}),[B,R]=b.useState(()=>{let y=!1;return s&&b.Children.forEach(s,g=>{ee(g,["Input","Select"])&&(de(g.props,!0)||de(g.props.inputProps,!0))&&(y=!0)}),y}),[D,M]=b.useState(!1);l&&D&&M(!1);const O=u!==void 0&&!l?u:D;let _;b.useRef(!1);const K=b.useCallback(()=>{R(!0)},[]),L=b.useCallback(()=>{R(!1)},[]),X=b.useMemo(()=>({adornedStart:F,setAdornedStart:Q,color:i,disabled:l,error:m,filled:B,focused:O,fullWidth:f,hiddenLabel:h,size:x,onBlur:()=>{M(!1)},onFocus:()=>{M(!0)},onEmpty:L,onFilled:K,registerEffect:_,required:v,variant:c}),[F,i,l,m,B,O,f,h,_,L,K,v,x,c]);return n.jsx($e.Provider,{value:X,children:n.jsx(_e,{as:d,ownerState:P,className:U(J.root,p),ref:o,...C,children:s})})});function Ve(e){return H("MuiFormHelperText",e)}const ce=N("MuiFormHelperText",["root","error","disabled","sizeSmall","sizeMedium","contained","focused","filled","required"]);var pe;const Ge=e=>{const{classes:t,contained:o,size:r,disabled:s,error:p,filled:i,focused:d,required:l}=e,m={root:["root",s&&"disabled",p&&"error",r&&`size${z(r)}`,o&&"contained",d&&"focused",i&&"filled",l&&"required"]};return E(m,Ve,t)},Je=T("p",{name:"MuiFormHelperText",slot:"Root",overridesResolver:(e,t)=>{const{ownerState:o}=e;return[t.root,o.size&&t[`size${z(o.size)}`],o.contained&&t.contained,o.filled&&t.filled]}})(G(({theme:e})=>({color:(e.vars||e).palette.text.secondary,...e.typography.caption,textAlign:"left",marginTop:3,marginRight:0,marginBottom:0,marginLeft:0,[`&.${ce.disabled}`]:{color:(e.vars||e).palette.text.disabled},[`&.${ce.error}`]:{color:(e.vars||e).palette.error.main},variants:[{props:{size:"small"},style:{marginTop:4}},{props:({ownerState:t})=>t.contained,style:{marginLeft:14,marginRight:14}}]}))),Qe=b.forwardRef(function(t,o){const r=W({props:t,name:"MuiFormHelperText"}),{children:s,className:p,component:i="p",disabled:d,error:l,filled:m,focused:u,margin:f,required:h,variant:a,...v}=r,x=oe(),c=re({props:r,muiFormControl:x,states:["variant","size","disabled","error","filled","focused","required"]}),C={...r,component:i,contained:c.variant==="filled"||c.variant==="outlined",variant:c.variant,size:c.size,disabled:c.disabled,error:c.error,filled:c.filled,focused:c.focused,required:c.required};delete C.ownerState;const P=Ge(C);return n.jsx(Je,{as:i,className:U(P.root,p),ref:o,...v,ownerState:C,children:s===" "?pe||(pe=n.jsx("span",{className:"notranslate","aria-hidden":!0,children:"â€‹"})):s})});function Xe(e){return H("MuiFormLabel",e)}const A=N("MuiFormLabel",["root","colorSecondary","focused","disabled","error","filled","required","asterisk"]),Ye=e=>{const{classes:t,color:o,focused:r,disabled:s,error:p,filled:i,required:d}=e,l={root:["root",`color${z(o)}`,s&&"disabled",p&&"error",i&&"filled",r&&"focused",d&&"required"],asterisk:["asterisk",p&&"error"]};return E(l,Xe,t)},Ze=T("label",{name:"MuiFormLabel",slot:"Root",overridesResolver:(e,t)=>{const{ownerState:o}=e;return[t.root,o.color==="secondary"&&t.colorSecondary,o.filled&&t.filled]}})(G(({theme:e})=>({color:(e.vars||e).palette.text.secondary,...e.typography.body1,lineHeight:"1.4375em",padding:0,position:"relative",variants:[...Object.entries(e.palette).filter(ze()).map(([t])=>({props:{color:t},style:{[`&.${A.focused}`]:{color:(e.vars||e).palette[t].main}}})),{props:{},style:{[`&.${A.disabled}`]:{color:(e.vars||e).palette.text.disabled},[`&.${A.error}`]:{color:(e.vars||e).palette.error.main}}}]}))),et=T("span",{name:"MuiFormLabel",slot:"Asterisk"})(G(({theme:e})=>({[`&.${A.error}`]:{color:(e.vars||e).palette.error.main}}))),tt=b.forwardRef(function(t,o){const r=W({props:t,name:"MuiFormLabel"}),{children:s,className:p,color:i,component:d="label",disabled:l,error:m,filled:u,focused:f,required:h,...a}=r,v=oe(),x=re({props:r,muiFormControl:v,states:["color","required","focused","disabled","error","filled"]}),c={...r,color:x.color||"primary",component:d,disabled:x.disabled,error:x.error,filled:x.filled,focused:x.focused,required:x.required},C=Ye(c);return n.jsxs(Ze,{as:d,ownerState:c,className:U(C.root,p),ref:o,...a,children:[s,x.required&&n.jsxs(et,{ownerState:c,"aria-hidden":!0,className:C.asterisk,children:["â€‰","*"]})]})});function ot(e){return H("MuiInputLabel",e)}N("MuiInputLabel",["root","focused","disabled","error","required","asterisk","formControl","sizeSmall","shrink","animated","standard","filled","outlined"]);const rt=e=>{const{classes:t,formControl:o,size:r,shrink:s,disableAnimation:p,variant:i,required:d}=e,l={root:["root",o&&"formControl",!p&&"animated",s&&"shrink",r&&r!=="medium"&&`size${z(r)}`,i],asterisk:[d&&"asterisk"]},m=E(l,ot,t);return{...t,...m}},st=T(tt,{shouldForwardProp:e=>Re(e)||e==="classes",name:"MuiInputLabel",slot:"Root",overridesResolver:(e,t)=>{const{ownerState:o}=e;return[{[`& .${A.asterisk}`]:t.asterisk},t.root,o.formControl&&t.formControl,o.size==="small"&&t.sizeSmall,o.shrink&&t.shrink,!o.disableAnimation&&t.animated,o.focused&&t.focused,t[o.variant]]}})(G(({theme:e})=>({display:"block",transformOrigin:"top left",whiteSpace:"nowrap",overflow:"hidden",textOverflow:"ellipsis",maxWidth:"100%",variants:[{props:({ownerState:t})=>t.formControl,style:{position:"absolute",left:0,top:0,transform:"translate(0, 20px) scale(1)"}},{props:{size:"small"},style:{transform:"translate(0, 17px) scale(1)"}},{props:({ownerState:t})=>t.shrink,style:{transform:"translate(0, -1.5px) scale(0.75)",transformOrigin:"top left",maxWidth:"133%"}},{props:({ownerState:t})=>!t.disableAnimation,style:{transition:e.transitions.create(["color","transform","max-width"],{duration:e.transitions.duration.shorter,easing:e.transitions.easing.easeOut})}},{props:{variant:"filled"},style:{zIndex:1,pointerEvents:"none",transform:"translate(12px, 16px) scale(1)",maxWidth:"calc(100% - 24px)"}},{props:{variant:"filled",size:"small"},style:{transform:"translate(12px, 13px) scale(1)"}},{props:({variant:t,ownerState:o})=>t==="filled"&&o.shrink,style:{userSelect:"none",pointerEvents:"auto",transform:"translate(12px, 7px) scale(0.75)",maxWidth:"calc(133% - 24px)"}},{props:({variant:t,ownerState:o,size:r})=>t==="filled"&&o.shrink&&r==="small",style:{transform:"translate(12px, 4px) scale(0.75)"}},{props:{variant:"outlined"},style:{zIndex:1,pointerEvents:"none",transform:"translate(14px, 16px) scale(1)",maxWidth:"calc(100% - 24px)"}},{props:{variant:"outlined",size:"small"},style:{transform:"translate(14px, 9px) scale(1)"}},{props:({variant:t,ownerState:o})=>t==="outlined"&&o.shrink,style:{userSelect:"none",pointerEvents:"auto",maxWidth:"calc(133% - 32px)",transform:"translate(14px, -9px) scale(0.75)"}}]}))),nt=b.forwardRef(function(t,o){const r=W({name:"MuiInputLabel",props:t}),{disableAnimation:s=!1,margin:p,shrink:i,variant:d,className:l,...m}=r,u=oe();let f=i;typeof f>"u"&&u&&(f=u.filled||u.focused||u.adornedStart);const h=re({props:r,muiFormControl:u,states:["size","variant","required","focused"]}),a={...r,disableAnimation:s,formControl:u,shrink:f,size:h.size,variant:h.variant,required:h.required,focused:h.focused},v=rt(a);return n.jsx(st,{"data-shrink":f,ref:o,className:U(v.root,l),...m,ownerState:a,classes:v})});function at(e){return H("MuiTextField",e)}N("MuiTextField",["root"]);const lt={standard:Ne,filled:He,outlined:Ae},it=e=>{const{classes:t}=e;return E({root:["root"]},at,t)},dt=T(Ke,{name:"MuiTextField",slot:"Root"})({}),ct=b.forwardRef(function(t,o){const r=W({props:t,name:"MuiTextField"}),{autoComplete:s,autoFocus:p=!1,children:i,className:d,color:l="primary",defaultValue:m,disabled:u=!1,error:f=!1,FormHelperTextProps:h,fullWidth:a=!1,helperText:v,id:x,InputLabelProps:c,inputProps:C,InputProps:P,inputRef:J,label:F,maxRows:Q,minRows:B,multiline:R=!1,name:D,onBlur:M,onChange:O,onFocus:_,placeholder:K,required:L=!1,rows:X,select:y=!1,SelectProps:g,slots:j={},slotProps:ue={},type:me,value:se,variant:V="outlined",...fe}=r,S={...r,autoFocus:p,color:l,disabled:u,error:f,fullWidth:a,multiline:R,required:L,select:y,variant:V},xe=it(S),k=Me(x),Y=v&&k?`${k}-helper-text`:void 0,ne=F&&k?`${k}-label`:void 0,be=lt[V],w={slots:j,slotProps:{input:P,inputLabel:c,htmlInput:C,formHelperText:h,select:g,...ue}},q={},Z=w.slotProps.inputLabel;V==="outlined"&&(Z&&typeof Z.shrink<"u"&&(q.notched=Z.shrink),q.label=F),y&&((!g||!g.native)&&(q.id=void 0),q["aria-describedby"]=void 0);const[he,ve]=I("root",{elementType:dt,shouldForwardComponentProp:!0,externalForwardedProps:{...w,...fe},ownerState:S,className:U(xe.root,d),ref:o,additionalProps:{disabled:u,error:f,fullWidth:a,required:L,color:l,variant:V}}),[ge,Ce]=I("input",{elementType:be,externalForwardedProps:w,additionalProps:q,ownerState:S}),[ye,Fe]=I("inputLabel",{elementType:nt,externalForwardedProps:w,ownerState:S}),[Se,ke]=I("htmlInput",{elementType:"input",externalForwardedProps:w,ownerState:S}),[we,Pe]=I("formHelperText",{elementType:Qe,externalForwardedProps:w,ownerState:S}),[Ie,Te]=I("select",{elementType:We,externalForwardedProps:w,ownerState:S}),ae=n.jsx(ge,{"aria-describedby":Y,autoComplete:s,autoFocus:p,defaultValue:m,fullWidth:a,multiline:R,name:D,rows:X,maxRows:Q,minRows:B,type:me,value:se,id:k,inputRef:J,onBlur:M,onChange:O,onFocus:_,placeholder:K,inputProps:ke,slots:{input:j.htmlInput?Se:void 0},...Ce});return n.jsxs(he,{...ve,children:[F!=null&&F!==""&&n.jsx(ye,{htmlFor:k,id:ne,...Fe,children:F}),y?n.jsx(Ie,{"aria-describedby":Y,id:k,labelId:ne,value:se,input:ae,...Te,children:i}):ae,v&&n.jsx(we,{id:Y,...Pe,children:v})]})});/**
 * @license @tabler/icons-react v3.35.0 - MIT
 *
 * This source code is licensed under the MIT license.
 * See the LICENSE file in the root directory of this source tree.
 */const pt=[["path",{d:"M9 14l-4 -4l4 -4",key:"svg-0"}],["path",{d:"M5 10h11a4 4 0 1 1 0 8h-1",key:"svg-1"}]],ut=te("outline","arrow-back-up","ArrowBackUp",pt);/**
 * @license @tabler/icons-react v3.35.0 - MIT
 *
 * This source code is licensed under the MIT license.
 * See the LICENSE file in the root directory of this source tree.
 */const mt=[["path",{d:"M12 5l0 14",key:"svg-0"}],["path",{d:"M18 11l-6 -6",key:"svg-1"}],["path",{d:"M6 11l6 -6",key:"svg-2"}]],ft=te("outline","arrow-up","ArrowUp",mt);/**
 * @license @tabler/icons-react v3.35.0 - MIT
 *
 * This source code is licensed under the MIT license.
 * See the LICENSE file in the root directory of this source tree.
 */const xt=[["path",{d:"M19 2h-14a3 3 0 0 0 -3 3v14a3 3 0 0 0 3 3h14a3 3 0 0 0 3 -3v-14a3 3 0 0 0 -3 -3z",key:"svg-0"}]],bt=te("filled","square-filled","SquareFilled",xt),ht=()=>{const[{clusterInfo:{status:e}}]=Le(),[{input:t,status:o},{setInput:r,generate:s,stop:p,clear:i}]=je(),d=b.useRef(!1),l=$(a=>{d.current=!0}),m=$(a=>{d.current=!1}),u=$(a=>{a.key==="Enter"&&!a.shiftKey&&!d.current&&(a.preventDefault(),s())}),f=$(a=>{o==="opened"||o==="generating"?p():(o==="closed"||o==="error")&&s()}),h=$(a=>{i()});return n.jsx(le,{"data-status":o,children:n.jsx(ct,{value:t,onChange:a=>r(a.target.value),multiline:!0,maxRows:4,placeholder:"Ask anything",fullWidth:!0,onCompositionStart:l,onCompositionEnd:m,onKeyDown:u,slotProps:{input:{sx:{border:"1px solid",borderColor:"grey.300",borderRadius:2,fontSize:"0.95rem",boxShadow:"2px 2px 4px rgba(0,0,0,0.05)",flexDirection:"column","& textarea":{fontSize:"0.875rem",scrollbarWidth:"none",msOverflowStyle:"none","&::-webkit-scrollbar":{display:"none"}}},endAdornment:n.jsxs(le,{direction:"row",sx:{alignSelf:"flex-end",alignItems:"center",gap:2},children:[n.jsx(ie,{variant:"text",sx:{color:"text.secondary"},startIcon:n.jsx(ut,{}),disabled:o==="opened"||o==="generating",onClick:h,children:"Clear"}),n.jsx(ie,{size:"small",color:"primary",disabled:e!=="available"||o==="opened",onClick:f,children:o==="opened"?n.jsx(Ue,{size:"medium"}):o==="generating"?n.jsx(bt,{size:"1.25rem"}):n.jsx(ft,{size:"1.25rem"})})]})}}})})};function Ct(){return n.jsxs(Ee,{children:[n.jsx(Be,{}),n.jsx(ht,{})]})}export{Ct as default};


================================================================================
File: src/frontend/dist/assets/gradient-icon-CRwZKfVU.svg
Size: 221 B
================================================================================

<svg width="26" height="28" viewBox="0 0 26 28" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M7 19H4V22H7V19Z" fill="black"/>
<path d="M22 6V8.85011L12.9475 22H10V19.1499L19.0521 6H22Z" fill="black"/>
</svg>



================================================================================
File: src/frontend/dist/assets/join-DcQSNXBs.js
Size: 2.27 kB
================================================================================

import{k as u,o as h,r as m,j as e,T as o,s as p,A as i,q as x,L as y,S as f}from"./App-BlsOxMPe.js";import{M as v,J as j,N as g}from"./main-layout-BciTM2Vs.js";/**
 * @license @tabler/icons-react v3.35.0 - MIT
 *
 * This source code is licensed under the MIT license.
 * See the LICENSE file in the root directory of this source tree.
 */const w=[["path",{d:"M5 12l14 0",key:"svg-0"}],["path",{d:"M5 12l6 6",key:"svg-1"}],["path",{d:"M5 12l6 -6",key:"svg-2"}]],k=u("outline","arrow-left","ArrowLeft",w),r=p(f)(({theme:t})=>{const{spacing:s}=t;return{overflowY:"auto"}});function L(){const[{config:{modelInfo:t},clusterInfo:{status:s,initNodesNumber:n,needMoreNodes:d},nodeInfoList:a}]=h(),l=m.useMemo(()=>!!(n>0&&a.length>=n&&a.every(c=>c.status==="available")&&s==="waiting"),[s,n,a]);return e.jsxs(v,{contentStart:e.jsx(x,{component:y,to:"/setup",size:"medium",color:"secondary",startIcon:e.jsx(k,{}),children:"Back"}),children:[e.jsx(o,{variant:"h1",children:"Get Your Nodes Running"}),e.jsxs(r,{gap:6,sx:{overflow:"hidden"},children:[e.jsxs(r,{gap:2,children:[e.jsx(r,{gap:1,children:e.jsx(o,{variant:"body1",children:"Step 1 - Run join command on all nodes"})}),e.jsx(j,{})]}),e.jsxs(r,{gap:2,flex:1,children:[e.jsxs(r,{gap:1,children:[e.jsx(o,{variant:"body1",children:"Step 2 - Check your node status"}),e.jsx(o,{variant:"body2",color:"text.secondary",fontWeight:"regular",children:"After you successfully start your nodes, you should see them start to show up below with their status. Once all nodes are connected, you will automatically be directed to the chat interface."})]}),l&&e.jsx(i,{severity:"error",variant:"standard",children:"Your selected model requires more nodes. Please go back to the previous step to add more nodes, or choose a smaller model."},"error")||e.jsx(i,{severity:"info",variant:"standard",children:"If your nodes cannot connect properly, retry the above join command to restart the server."},"info"),!!t&&t.vram>0&&d&&e.jsx(i,{severity:"warning",variant:"standard",children:e.jsx(o,{variant:"inherit",children:["Your selected model requires more nodes.","Youâ€™ll need a ",e.jsx("strong",{children:`minimum of ${t.vram} GB of total VRAM`})," to host this model."]})},"vram-warning"),e.jsx(g,{},"node-list")]})]})]})}export{L as default};


================================================================================
File: src/frontend/dist/assets/main-C3viVoAI.js
Size: 119 B
================================================================================

import{c as t,j as e,M as o}from"./App-BlsOxMPe.js";t.createRoot(document.getElementById("root")).render(e.jsx(o,{}));


================================================================================
File: src/frontend/dist/assets/main-layout-DVneG3Rq.css
Size: 28.81 kB
================================================================================

@font-face{font-family:KaTeX_AMS;font-style:normal;font-weight:400;src:url(/assets/KaTeX_AMS-Regular-BQhdFMY1.woff2) format("woff2"),url(/assets/KaTeX_AMS-Regular-DMm9YOAa.woff) format("woff"),url(/assets/KaTeX_AMS-Regular-DRggAlZN.ttf) format("truetype")}@font-face{font-family:KaTeX_Caligraphic;font-style:normal;font-weight:700;src:url(/assets/KaTeX_Caligraphic-Bold-Dq_IR9rO.woff2) format("woff2"),url(/assets/KaTeX_Caligraphic-Bold-BEiXGLvX.woff) format("woff"),url(/assets/KaTeX_Caligraphic-Bold-ATXxdsX0.ttf) format("truetype")}@font-face{font-family:KaTeX_Caligraphic;font-style:normal;font-weight:400;src:url(/assets/KaTeX_Caligraphic-Regular-Di6jR-x-.woff2) format("woff2"),url(/assets/KaTeX_Caligraphic-Regular-CTRA-rTL.woff) format("woff"),url(/assets/KaTeX_Caligraphic-Regular-wX97UBjC.ttf) format("truetype")}@font-face{font-family:KaTeX_Fraktur;font-style:normal;font-weight:700;src:url(/assets/KaTeX_Fraktur-Bold-CL6g_b3V.woff2) format("woff2"),url(/assets/KaTeX_Fraktur-Bold-BsDP51OF.woff) format("woff"),url(/assets/KaTeX_Fraktur-Bold-BdnERNNW.ttf) format("truetype")}@font-face{font-family:KaTeX_Fraktur;font-style:normal;font-weight:400;src:url(/assets/KaTeX_Fraktur-Regular-CTYiF6lA.woff2) format("woff2"),url(/assets/KaTeX_Fraktur-Regular-Dxdc4cR9.woff) format("woff"),url(/assets/KaTeX_Fraktur-Regular-CB_wures.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:normal;font-weight:700;src:url(/assets/KaTeX_Main-Bold-Cx986IdX.woff2) format("woff2"),url(/assets/KaTeX_Main-Bold-Jm3AIy58.woff) format("woff"),url(/assets/KaTeX_Main-Bold-waoOVXN0.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:italic;font-weight:700;src:url(/assets/KaTeX_Main-BoldItalic-DxDJ3AOS.woff2) format("woff2"),url(/assets/KaTeX_Main-BoldItalic-SpSLRI95.woff) format("woff"),url(/assets/KaTeX_Main-BoldItalic-DzxPMmG6.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:italic;font-weight:400;src:url(/assets/KaTeX_Main-Italic-NWA7e6Wa.woff2) format("woff2"),url(/assets/KaTeX_Main-Italic-BMLOBm91.woff) format("woff"),url(/assets/KaTeX_Main-Italic-3WenGoN9.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:normal;font-weight:400;src:url(/assets/KaTeX_Main-Regular-B22Nviop.woff2) format("woff2"),url(/assets/KaTeX_Main-Regular-Dr94JaBh.woff) format("woff"),url(/assets/KaTeX_Main-Regular-ypZvNtVU.ttf) format("truetype")}@font-face{font-family:KaTeX_Math;font-style:italic;font-weight:700;src:url(/assets/KaTeX_Math-BoldItalic-CZnvNsCZ.woff2) format("woff2"),url(/assets/KaTeX_Math-BoldItalic-iY-2wyZ7.woff) format("woff"),url(/assets/KaTeX_Math-BoldItalic-B3XSjfu4.ttf) format("truetype")}@font-face{font-family:KaTeX_Math;font-style:italic;font-weight:400;src:url(/assets/KaTeX_Math-Italic-t53AETM-.woff2) format("woff2"),url(/assets/KaTeX_Math-Italic-DA0__PXp.woff) format("woff"),url(/assets/KaTeX_Math-Italic-flOr_0UB.ttf) format("truetype")}@font-face{font-family:KaTeX_SansSerif;font-style:normal;font-weight:700;src:url(/assets/KaTeX_SansSerif-Bold-D1sUS0GD.woff2) format("woff2"),url(/assets/KaTeX_SansSerif-Bold-DbIhKOiC.woff) format("woff"),url(/assets/KaTeX_SansSerif-Bold-CFMepnvq.ttf) format("truetype")}@font-face{font-family:KaTeX_SansSerif;font-style:italic;font-weight:400;src:url(/assets/KaTeX_SansSerif-Italic-C3H0VqGB.woff2) format("woff2"),url(/assets/KaTeX_SansSerif-Italic-DN2j7dab.woff) format("woff"),url(/assets/KaTeX_SansSerif-Italic-YYjJ1zSn.ttf) format("truetype")}@font-face{font-family:KaTeX_SansSerif;font-style:normal;font-weight:400;src:url(/assets/KaTeX_SansSerif-Regular-DDBCnlJ7.woff2) format("woff2"),url(/assets/KaTeX_SansSerif-Regular-CS6fqUqJ.woff) format("woff"),url(/assets/KaTeX_SansSerif-Regular-BNo7hRIc.ttf) format("truetype")}@font-face{font-family:KaTeX_Script;font-style:normal;font-weight:400;src:url(/assets/KaTeX_Script-Regular-D3wIWfF6.woff2) format("woff2"),url(/assets/KaTeX_Script-Regular-D5yQViql.woff) format("woff"),url(/assets/KaTeX_Script-Regular-C5JkGWo-.ttf) format("truetype")}@font-face{font-family:KaTeX_Size1;font-style:normal;font-weight:400;src:url(/assets/KaTeX_Size1-Regular-mCD8mA8B.woff2) format("woff2"),url(/assets/KaTeX_Size1-Regular-C195tn64.woff) format("woff"),url(/assets/KaTeX_Size1-Regular-Dbsnue_I.ttf) format("truetype")}@font-face{font-family:KaTeX_Size2;font-style:normal;font-weight:400;src:url(/assets/KaTeX_Size2-Regular-Dy4dx90m.woff2) format("woff2"),url(/assets/KaTeX_Size2-Regular-oD1tc_U0.woff) format("woff"),url(/assets/KaTeX_Size2-Regular-B7gKUWhC.ttf) format("truetype")}@font-face{font-family:KaTeX_Size3;font-style:normal;font-weight:400;src:url(data:font/woff2;base64,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) format("woff2"),url(/assets/KaTeX_Size3-Regular-CTq5MqoE.woff) format("woff"),url(/assets/KaTeX_Size3-Regular-DgpXs0kz.ttf) format("truetype")}@font-face{font-family:KaTeX_Size4;font-style:normal;font-weight:400;src:url(/assets/KaTeX_Size4-Regular-Dl5lxZxV.woff2) format("woff2"),url(/assets/KaTeX_Size4-Regular-BF-4gkZK.woff) format("woff"),url(/assets/KaTeX_Size4-Regular-DWFBv043.ttf) format("truetype")}@font-face{font-family:KaTeX_Typewriter;font-style:normal;font-weight:400;src:url(/assets/KaTeX_Typewriter-Regular-CO6r4hn1.woff2) format("woff2"),url(/assets/KaTeX_Typewriter-Regular-C0xS9mPB.woff) format("woff"),url(/assets/KaTeX_Typewriter-Regular-D3Ib7_Hf.ttf) format("truetype")}.katex{font: 1.21em KaTeX_Main,Times New Roman,serif;line-height:1.2;text-indent:0;text-rendering:auto}.katex *{-ms-high-contrast-adjust:none!important;border-color:currentColor}.katex .katex-version:after{content:"0.16.22"}.katex .katex-mathml{clip:rect(1px,1px,1px,1px);border:0;height:1px;overflow:hidden;padding:0;position:absolute;width:1px}.katex .katex-html>.newline{display:block}.katex .base{position:relative;white-space:nowrap;width:-webkit-min-content;width:-moz-min-content;width:min-content}.katex .base,.katex .strut{display:inline-block}.katex .textbf{font-weight:700}.katex .textit{font-style:italic}.katex .textrm{font-family:KaTeX_Main}.katex .textsf{font-family:KaTeX_SansSerif}.katex .texttt{font-family:KaTeX_Typewriter}.katex .mathnormal{font-family:KaTeX_Math;font-style:italic}.katex .mathit{font-family:KaTeX_Main;font-style:italic}.katex .mathrm{font-style:normal}.katex .mathbf{font-family:KaTeX_Main;font-weight:700}.katex .boldsymbol{font-family:KaTeX_Math;font-style:italic;font-weight:700}.katex .amsrm,.katex .mathbb,.katex .textbb{font-family:KaTeX_AMS}.katex .mathcal{font-family:KaTeX_Caligraphic}.katex .mathfrak,.katex .textfrak{font-family:KaTeX_Fraktur}.katex .mathboldfrak,.katex .textboldfrak{font-family:KaTeX_Fraktur;font-weight:700}.katex .mathtt{font-family:KaTeX_Typewriter}.katex .mathscr,.katex .textscr{font-family:KaTeX_Script}.katex .mathsf,.katex .textsf{font-family:KaTeX_SansSerif}.katex .mathboldsf,.katex .textboldsf{font-family:KaTeX_SansSerif;font-weight:700}.katex .mathitsf,.katex .mathsfit,.katex .textitsf{font-family:KaTeX_SansSerif;font-style:italic}.katex .mainrm{font-family:KaTeX_Main;font-style:normal}.katex .vlist-t{border-collapse:collapse;display:inline-table;table-layout:fixed}.katex .vlist-r{display:table-row}.katex .vlist{display:table-cell;position:relative;vertical-align:bottom}.katex .vlist>span{display:block;height:0;position:relative}.katex .vlist>span>span{display:inline-block}.katex .vlist>span>.pstrut{overflow:hidden;width:0}.katex .vlist-t2{margin-right:-2px}.katex .vlist-s{display:table-cell;font-size:1px;min-width:2px;vertical-align:bottom;width:2px}.katex .vbox{align-items:baseline;display:inline-flex;flex-direction:column}.katex .hbox{width:100%}.katex .hbox,.katex .thinbox{display:inline-flex;flex-direction:row}.katex .thinbox{max-width:0;width:0}.katex .msupsub{text-align:left}.katex .mfrac>span>span{text-align:center}.katex .mfrac .frac-line{border-bottom-style:solid;display:inline-block;width:100%}.katex .hdashline,.katex .hline,.katex .mfrac .frac-line,.katex .overline .overline-line,.katex .rule,.katex .underline .underline-line{min-height:1px}.katex .mspace{display:inline-block}.katex .clap,.katex .llap,.katex .rlap{position:relative;width:0}.katex .clap>.inner,.katex .llap>.inner,.katex .rlap>.inner{position:absolute}.katex .clap>.fix,.katex .llap>.fix,.katex .rlap>.fix{display:inline-block}.katex .llap>.inner{right:0}.katex .clap>.inner,.katex .rlap>.inner{left:0}.katex .clap>.inner>span{margin-left:-50%;margin-right:50%}.katex .rule{border:0 solid;display:inline-block;position:relative}.katex .hline,.katex .overline .overline-line,.katex .underline .underline-line{border-bottom-style:solid;display:inline-block;width:100%}.katex .hdashline{border-bottom-style:dashed;display:inline-block;width:100%}.katex .sqrt>.root{margin-left:.2777777778em;margin-right:-.5555555556em}.katex .fontsize-ensurer.reset-size1.size1,.katex .sizing.reset-size1.size1{font-size:1em}.katex .fontsize-ensurer.reset-size1.size2,.katex .sizing.reset-size1.size2{font-size:1.2em}.katex .fontsize-ensurer.reset-size1.size3,.katex .sizing.reset-size1.size3{font-size:1.4em}.katex .fontsize-ensurer.reset-size1.size4,.katex .sizing.reset-size1.size4{font-size:1.6em}.katex .fontsize-ensurer.reset-size1.size5,.katex .sizing.reset-size1.size5{font-size:1.8em}.katex .fontsize-ensurer.reset-size1.size6,.katex .sizing.reset-size1.size6{font-size:2em}.katex .fontsize-ensurer.reset-size1.size7,.katex .sizing.reset-size1.size7{font-size:2.4em}.katex .fontsize-ensurer.reset-size1.size8,.katex .sizing.reset-size1.size8{font-size:2.88em}.katex .fontsize-ensurer.reset-size1.size9,.katex .sizing.reset-size1.size9{font-size:3.456em}.katex .fontsize-ensurer.reset-size1.size10,.katex .sizing.reset-size1.size10{font-size:4.148em}.katex .fontsize-ensurer.reset-size1.size11,.katex .sizing.reset-size1.size11{font-size:4.976em}.katex .fontsize-ensurer.reset-size2.size1,.katex .sizing.reset-size2.size1{font-size:.8333333333em}.katex .fontsize-ensurer.reset-size2.size2,.katex .sizing.reset-size2.size2{font-size:1em}.katex .fontsize-ensurer.reset-size2.size3,.katex .sizing.reset-size2.size3{font-size:1.1666666667em}.katex .fontsize-ensurer.reset-size2.size4,.katex .sizing.reset-size2.size4{font-size:1.3333333333em}.katex .fontsize-ensurer.reset-size2.size5,.katex .sizing.reset-size2.size5{font-size:1.5em}.katex .fontsize-ensurer.reset-size2.size6,.katex .sizing.reset-size2.size6{font-size:1.6666666667em}.katex .fontsize-ensurer.reset-size2.size7,.katex .sizing.reset-size2.size7{font-size:2em}.katex .fontsize-ensurer.reset-size2.size8,.katex .sizing.reset-size2.size8{font-size:2.4em}.katex .fontsize-ensurer.reset-size2.size9,.katex .sizing.reset-size2.size9{font-size:2.88em}.katex .fontsize-ensurer.reset-size2.size10,.katex .sizing.reset-size2.size10{font-size:3.4566666667em}.katex .fontsize-ensurer.reset-size2.size11,.katex .sizing.reset-size2.size11{font-size:4.1466666667em}.katex .fontsize-ensurer.reset-size3.size1,.katex .sizing.reset-size3.size1{font-size:.7142857143em}.katex .fontsize-ensurer.reset-size3.size2,.katex .sizing.reset-size3.size2{font-size:.8571428571em}.katex .fontsize-ensurer.reset-size3.size3,.katex .sizing.reset-size3.size3{font-size:1em}.katex .fontsize-ensurer.reset-size3.size4,.katex .sizing.reset-size3.size4{font-size:1.1428571429em}.katex .fontsize-ensurer.reset-size3.size5,.katex .sizing.reset-size3.size5{font-size:1.2857142857em}.katex .fontsize-ensurer.reset-size3.size6,.katex .sizing.reset-size3.size6{font-size:1.4285714286em}.katex .fontsize-ensurer.reset-size3.size7,.katex .sizing.reset-size3.size7{font-size:1.7142857143em}.katex .fontsize-ensurer.reset-size3.size8,.katex .sizing.reset-size3.size8{font-size:2.0571428571em}.katex .fontsize-ensurer.reset-size3.size9,.katex .sizing.reset-size3.size9{font-size:2.4685714286em}.katex .fontsize-ensurer.reset-size3.size10,.katex .sizing.reset-size3.size10{font-size:2.9628571429em}.katex .fontsize-ensurer.reset-size3.size11,.katex .sizing.reset-size3.size11{font-size:3.5542857143em}.katex .fontsize-ensurer.reset-size4.size1,.katex .sizing.reset-size4.size1{font-size:.625em}.katex .fontsize-ensurer.reset-size4.size2,.katex .sizing.reset-size4.size2{font-size:.75em}.katex .fontsize-ensurer.reset-size4.size3,.katex .sizing.reset-size4.size3{font-size:.875em}.katex .fontsize-ensurer.reset-size4.size4,.katex .sizing.reset-size4.size4{font-size:1em}.katex .fontsize-ensurer.reset-size4.size5,.katex .sizing.reset-size4.size5{font-size:1.125em}.katex .fontsize-ensurer.reset-size4.size6,.katex .sizing.reset-size4.size6{font-size:1.25em}.katex .fontsize-ensurer.reset-size4.size7,.katex .sizing.reset-size4.size7{font-size:1.5em}.katex .fontsize-ensurer.reset-size4.size8,.katex .sizing.reset-size4.size8{font-size:1.8em}.katex .fontsize-ensurer.reset-size4.size9,.katex .sizing.reset-size4.size9{font-size:2.16em}.katex .fontsize-ensurer.reset-size4.size10,.katex .sizing.reset-size4.size10{font-size:2.5925em}.katex .fontsize-ensurer.reset-size4.size11,.katex .sizing.reset-size4.size11{font-size:3.11em}.katex .fontsize-ensurer.reset-size5.size1,.katex .sizing.reset-size5.size1{font-size:.5555555556em}.katex .fontsize-ensurer.reset-size5.size2,.katex .sizing.reset-size5.size2{font-size:.6666666667em}.katex .fontsize-ensurer.reset-size5.size3,.katex .sizing.reset-size5.size3{font-size:.7777777778em}.katex .fontsize-ensurer.reset-size5.size4,.katex .sizing.reset-size5.size4{font-size:.8888888889em}.katex .fontsize-ensurer.reset-size5.size5,.katex .sizing.reset-size5.size5{font-size:1em}.katex .fontsize-ensurer.reset-size5.size6,.katex .sizing.reset-size5.size6{font-size:1.1111111111em}.katex .fontsize-ensurer.reset-size5.size7,.katex .sizing.reset-size5.size7{font-size:1.3333333333em}.katex .fontsize-ensurer.reset-size5.size8,.katex .sizing.reset-size5.size8{font-size:1.6em}.katex .fontsize-ensurer.reset-size5.size9,.katex .sizing.reset-size5.size9{font-size:1.92em}.katex .fontsize-ensurer.reset-size5.size10,.katex .sizing.reset-size5.size10{font-size:2.3044444444em}.katex .fontsize-ensurer.reset-size5.size11,.katex .sizing.reset-size5.size11{font-size:2.7644444444em}.katex .fontsize-ensurer.reset-size6.size1,.katex .sizing.reset-size6.size1{font-size:.5em}.katex .fontsize-ensurer.reset-size6.size2,.katex .sizing.reset-size6.size2{font-size:.6em}.katex .fontsize-ensurer.reset-size6.size3,.katex .sizing.reset-size6.size3{font-size:.7em}.katex .fontsize-ensurer.reset-size6.size4,.katex .sizing.reset-size6.size4{font-size:.8em}.katex .fontsize-ensurer.reset-size6.size5,.katex .sizing.reset-size6.size5{font-size:.9em}.katex .fontsize-ensurer.reset-size6.size6,.katex .sizing.reset-size6.size6{font-size:1em}.katex .fontsize-ensurer.reset-size6.size7,.katex .sizing.reset-size6.size7{font-size:1.2em}.katex .fontsize-ensurer.reset-size6.size8,.katex .sizing.reset-size6.size8{font-size:1.44em}.katex .fontsize-ensurer.reset-size6.size9,.katex .sizing.reset-size6.size9{font-size:1.728em}.katex .fontsize-ensurer.reset-size6.size10,.katex .sizing.reset-size6.size10{font-size:2.074em}.katex .fontsize-ensurer.reset-size6.size11,.katex .sizing.reset-size6.size11{font-size:2.488em}.katex .fontsize-ensurer.reset-size7.size1,.katex .sizing.reset-size7.size1{font-size:.4166666667em}.katex .fontsize-ensurer.reset-size7.size2,.katex .sizing.reset-size7.size2{font-size:.5em}.katex .fontsize-ensurer.reset-size7.size3,.katex .sizing.reset-size7.size3{font-size:.5833333333em}.katex .fontsize-ensurer.reset-size7.size4,.katex .sizing.reset-size7.size4{font-size:.6666666667em}.katex .fontsize-ensurer.reset-size7.size5,.katex .sizing.reset-size7.size5{font-size:.75em}.katex .fontsize-ensurer.reset-size7.size6,.katex .sizing.reset-size7.size6{font-size:.8333333333em}.katex .fontsize-ensurer.reset-size7.size7,.katex .sizing.reset-size7.size7{font-size:1em}.katex .fontsize-ensurer.reset-size7.size8,.katex .sizing.reset-size7.size8{font-size:1.2em}.katex .fontsize-ensurer.reset-size7.size9,.katex .sizing.reset-size7.size9{font-size:1.44em}.katex .fontsize-ensurer.reset-size7.size10,.katex .sizing.reset-size7.size10{font-size:1.7283333333em}.katex .fontsize-ensurer.reset-size7.size11,.katex .sizing.reset-size7.size11{font-size:2.0733333333em}.katex .fontsize-ensurer.reset-size8.size1,.katex .sizing.reset-size8.size1{font-size:.3472222222em}.katex .fontsize-ensurer.reset-size8.size2,.katex .sizing.reset-size8.size2{font-size:.4166666667em}.katex .fontsize-ensurer.reset-size8.size3,.katex .sizing.reset-size8.size3{font-size:.4861111111em}.katex .fontsize-ensurer.reset-size8.size4,.katex .sizing.reset-size8.size4{font-size:.5555555556em}.katex .fontsize-ensurer.reset-size8.size5,.katex .sizing.reset-size8.size5{font-size:.625em}.katex .fontsize-ensurer.reset-size8.size6,.katex .sizing.reset-size8.size6{font-size:.6944444444em}.katex .fontsize-ensurer.reset-size8.size7,.katex .sizing.reset-size8.size7{font-size:.8333333333em}.katex .fontsize-ensurer.reset-size8.size8,.katex .sizing.reset-size8.size8{font-size:1em}.katex .fontsize-ensurer.reset-size8.size9,.katex .sizing.reset-size8.size9{font-size:1.2em}.katex .fontsize-ensurer.reset-size8.size10,.katex .sizing.reset-size8.size10{font-size:1.4402777778em}.katex .fontsize-ensurer.reset-size8.size11,.katex .sizing.reset-size8.size11{font-size:1.7277777778em}.katex .fontsize-ensurer.reset-size9.size1,.katex .sizing.reset-size9.size1{font-size:.2893518519em}.katex .fontsize-ensurer.reset-size9.size2,.katex .sizing.reset-size9.size2{font-size:.3472222222em}.katex .fontsize-ensurer.reset-size9.size3,.katex .sizing.reset-size9.size3{font-size:.4050925926em}.katex .fontsize-ensurer.reset-size9.size4,.katex .sizing.reset-size9.size4{font-size:.462962963em}.katex .fontsize-ensurer.reset-size9.size5,.katex .sizing.reset-size9.size5{font-size:.5208333333em}.katex .fontsize-ensurer.reset-size9.size6,.katex .sizing.reset-size9.size6{font-size:.5787037037em}.katex .fontsize-ensurer.reset-size9.size7,.katex .sizing.reset-size9.size7{font-size:.6944444444em}.katex .fontsize-ensurer.reset-size9.size8,.katex .sizing.reset-size9.size8{font-size:.8333333333em}.katex .fontsize-ensurer.reset-size9.size9,.katex .sizing.reset-size9.size9{font-size:1em}.katex .fontsize-ensurer.reset-size9.size10,.katex .sizing.reset-size9.size10{font-size:1.2002314815em}.katex .fontsize-ensurer.reset-size9.size11,.katex .sizing.reset-size9.size11{font-size:1.4398148148em}.katex .fontsize-ensurer.reset-size10.size1,.katex .sizing.reset-size10.size1{font-size:.2410800386em}.katex .fontsize-ensurer.reset-size10.size2,.katex .sizing.reset-size10.size2{font-size:.2892960463em}.katex .fontsize-ensurer.reset-size10.size3,.katex .sizing.reset-size10.size3{font-size:.337512054em}.katex .fontsize-ensurer.reset-size10.size4,.katex .sizing.reset-size10.size4{font-size:.3857280617em}.katex .fontsize-ensurer.reset-size10.size5,.katex .sizing.reset-size10.size5{font-size:.4339440694em}.katex .fontsize-ensurer.reset-size10.size6,.katex .sizing.reset-size10.size6{font-size:.4821600771em}.katex .fontsize-ensurer.reset-size10.size7,.katex .sizing.reset-size10.size7{font-size:.5785920926em}.katex .fontsize-ensurer.reset-size10.size8,.katex .sizing.reset-size10.size8{font-size:.6943105111em}.katex .fontsize-ensurer.reset-size10.size9,.katex .sizing.reset-size10.size9{font-size:.8331726133em}.katex .fontsize-ensurer.reset-size10.size10,.katex .sizing.reset-size10.size10{font-size:1em}.katex .fontsize-ensurer.reset-size10.size11,.katex .sizing.reset-size10.size11{font-size:1.1996142719em}.katex .fontsize-ensurer.reset-size11.size1,.katex .sizing.reset-size11.size1{font-size:.2009646302em}.katex .fontsize-ensurer.reset-size11.size2,.katex .sizing.reset-size11.size2{font-size:.2411575563em}.katex .fontsize-ensurer.reset-size11.size3,.katex .sizing.reset-size11.size3{font-size:.2813504823em}.katex .fontsize-ensurer.reset-size11.size4,.katex .sizing.reset-size11.size4{font-size:.3215434084em}.katex .fontsize-ensurer.reset-size11.size5,.katex .sizing.reset-size11.size5{font-size:.3617363344em}.katex .fontsize-ensurer.reset-size11.size6,.katex .sizing.reset-size11.size6{font-size:.4019292605em}.katex .fontsize-ensurer.reset-size11.size7,.katex .sizing.reset-size11.size7{font-size:.4823151125em}.katex .fontsize-ensurer.reset-size11.size8,.katex .sizing.reset-size11.size8{font-size:.578778135em}.katex .fontsize-ensurer.reset-size11.size9,.katex .sizing.reset-size11.size9{font-size:.6945337621em}.katex .fontsize-ensurer.reset-size11.size10,.katex .sizing.reset-size11.size10{font-size:.8336012862em}.katex .fontsize-ensurer.reset-size11.size11,.katex .sizing.reset-size11.size11{font-size:1em}.katex .delimsizing.size1{font-family:KaTeX_Size1}.katex .delimsizing.size2{font-family:KaTeX_Size2}.katex .delimsizing.size3{font-family:KaTeX_Size3}.katex .delimsizing.size4{font-family:KaTeX_Size4}.katex .delimsizing.mult .delim-size1>span{font-family:KaTeX_Size1}.katex .delimsizing.mult .delim-size4>span{font-family:KaTeX_Size4}.katex .nulldelimiter{display:inline-block;width:.12em}.katex .delimcenter,.katex .op-symbol{position:relative}.katex .op-symbol.small-op{font-family:KaTeX_Size1}.katex .op-symbol.large-op{font-family:KaTeX_Size2}.katex .accent>.vlist-t,.katex .op-limits>.vlist-t{text-align:center}.katex .accent .accent-body{position:relative}.katex .accent .accent-body:not(.accent-full){width:0}.katex .overlay{display:block}.katex .mtable .vertical-separator{display:inline-block;min-width:1px}.katex .mtable .arraycolsep{display:inline-block}.katex .mtable .col-align-c>.vlist-t{text-align:center}.katex .mtable .col-align-l>.vlist-t{text-align:left}.katex .mtable .col-align-r>.vlist-t{text-align:right}.katex .svg-align{text-align:left}.katex svg{fill:currentColor;stroke:currentColor;fill-rule:nonzero;fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1;display:block;height:inherit;position:absolute;width:100%}.katex svg path{stroke:none}.katex img{border-style:none;max-height:none;max-width:none;min-height:0;min-width:0}.katex .stretchy{display:block;overflow:hidden;position:relative;width:100%}.katex .stretchy:after,.katex .stretchy:before{content:""}.katex .hide-tail{overflow:hidden;position:relative;width:100%}.katex .halfarrow-left{left:0;overflow:hidden;position:absolute;width:50.2%}.katex .halfarrow-right{overflow:hidden;position:absolute;right:0;width:50.2%}.katex .brace-left{left:0;overflow:hidden;position:absolute;width:25.1%}.katex .brace-center{left:25%;overflow:hidden;position:absolute;width:50%}.katex .brace-right{overflow:hidden;position:absolute;right:0;width:25.1%}.katex .x-arrow-pad{padding:0 .5em}.katex .cd-arrow-pad{padding:0 .55556em 0 .27778em}.katex .mover,.katex .munder,.katex .x-arrow{text-align:center}.katex .boxpad{padding:0 .3em}.katex .fbox,.katex .fcolorbox{border:.04em solid;box-sizing:border-box}.katex .cancel-pad{padding:0 .2em}.katex .cancel-lap{margin-left:-.2em;margin-right:-.2em}.katex .sout{border-bottom-style:solid;border-bottom-width:.08em}.katex .angl{border-right:.049em solid;border-top:.049em solid;box-sizing:border-box;margin-right:.03889em}.katex .anglpad{padding:0 .03889em}.katex .eqn-num:before{content:"(" counter(katexEqnNo) ")";counter-increment:katexEqnNo}.katex .mml-eqn-num:before{content:"(" counter(mmlEqnNo) ")";counter-increment:mmlEqnNo}.katex .mtr-glue{width:50%}.katex .cd-vert-arrow{display:inline-block;position:relative}.katex .cd-label-left{display:inline-block;position:absolute;right:calc(50% + .3em);text-align:left}.katex .cd-label-right{display:inline-block;left:calc(50% + .3em);position:absolute;text-align:right}.katex-display{display:block;margin:1em 0;text-align:center}.katex-display>.katex{display:block;text-align:center;white-space:nowrap}.katex-display>.katex>.katex-html{display:block;position:relative}.katex-display>.katex>.katex-html>.tag{position:absolute;right:0}.katex-display.leqno>.katex>.katex-html>.tag{left:0;right:auto}.katex-display.fleqn>.katex{padding-left:2em;text-align:left}body{counter-reset:katexEqnNo mmlEqnNo}


================================================================================
File: src/frontend/dist/assets/setup-CfR0Ohts.js
Size: 11.61 kB
================================================================================

import{r as f,g as _,a as q,u as z,j as o,s as L,b as E,T as m,d as R,e as O,m as G,f as J,h as K,B as Q,t as j,i as X,k as Z,l as tt,n as k,I as W,o as et,p as ot,S as $,A as nt,q as rt}from"./App-BlsOxMPe.js";import{u as st,F as at,O as it,I as lt,M as dt,a as ut}from"./main-layout-BciTM2Vs.js";function ct(t){return f.Children.toArray(t).filter(e=>f.isValidElement(e))}function pt(t){return q("MuiInputAdornment",t)}const F=_("MuiInputAdornment",["root","filled","standard","outlined","positionStart","positionEnd","disablePointerEvents","hiddenLabel","sizeSmall"]);var U;const gt=(t,e)=>{const{ownerState:n}=t;return[e.root,e[`position${R(n.position)}`],n.disablePointerEvents===!0&&e.disablePointerEvents,e[n.variant]]},ft=t=>{const{classes:e,disablePointerEvents:n,hiddenLabel:l,position:a,size:p,variant:v}=t,d={root:["root",n&&"disablePointerEvents",a&&`position${R(a)}`,v,l&&"hiddenLabel",p&&`size${R(p)}`]};return O(d,pt,e)},vt=L("div",{name:"MuiInputAdornment",slot:"Root",overridesResolver:gt})(G(({theme:t})=>({display:"flex",maxHeight:"2em",alignItems:"center",whiteSpace:"nowrap",color:(t.vars||t).palette.action.active,variants:[{props:{variant:"filled"},style:{[`&.${F.positionStart}&:not(.${F.hiddenLabel})`]:{marginTop:16}}},{props:{position:"start"},style:{marginRight:8}},{props:{position:"end"},style:{marginLeft:8}},{props:{disablePointerEvents:!0},style:{pointerEvents:"none"}}]}))),D=f.forwardRef(function(e,n){const l=z({props:e,name:"MuiInputAdornment"}),{children:a,className:p,component:v="div",disablePointerEvents:d=!1,disableTypography:h=!1,position:x,variant:r,...u}=l,i=st()||{};let c=r;r&&i.variant,i&&!c&&(c=i.variant);const y={...l,hiddenLabel:i.hiddenLabel,size:i.size,disablePointerEvents:d,position:x,variant:c},C=ft(y);return o.jsx(at.Provider,{value:null,children:o.jsx(vt,{as:v,ownerState:y,className:E(C.root,p),ref:n,...u,children:typeof a=="string"&&!h?o.jsx(m,{color:"textSecondary",children:a}):o.jsxs(f.Fragment,{children:[x==="start"?U||(U=o.jsx("span",{className:"notranslate","aria-hidden":!0,children:"â€‹"})):null,a]})})})}),H=f.createContext({}),Y=f.createContext(void 0);function xt(t,e){return e===void 0||t===void 0?!1:Array.isArray(e)?e.includes(t):t===e}const ht=t=>{const{classes:e,fullWidth:n,selected:l,disabled:a,size:p,color:v}=t,d={root:["root",l&&"selected",a&&"disabled",n&&"fullWidth",`size${R(p)}`,v]};return O(d,K,e)},bt=L(Q,{name:"MuiToggleButton",slot:"Root",overridesResolver:(t,e)=>{const{ownerState:n}=t;return[e.root,e[`size${R(n.size)}`]]}})(G(({theme:t})=>({...t.typography.button,borderRadius:(t.vars||t).shape.borderRadius,padding:11,border:`1px solid ${(t.vars||t).palette.divider}`,color:(t.vars||t).palette.action.active,[`&.${j.disabled}`]:{color:(t.vars||t).palette.action.disabled,border:`1px solid ${(t.vars||t).palette.action.disabledBackground}`},"&:hover":{textDecoration:"none",backgroundColor:t.alpha((t.vars||t).palette.text.primary,(t.vars||t).palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:"transparent"}},variants:[{props:{color:"standard"},style:{[`&.${j.selected}`]:{color:(t.vars||t).palette.text.primary,backgroundColor:t.alpha((t.vars||t).palette.text.primary,(t.vars||t).palette.action.selectedOpacity),"&:hover":{backgroundColor:t.alpha((t.vars||t).palette.text.primary,`${(t.vars||t).palette.action.selectedOpacity} + ${(t.vars||t).palette.action.hoverOpacity}`),"@media (hover: none)":{backgroundColor:t.alpha((t.vars||t).palette.text.primary,(t.vars||t).palette.action.selectedOpacity)}}}}},...Object.entries(t.palette).filter(X()).map(([e])=>({props:{color:e},style:{[`&.${j.selected}`]:{color:(t.vars||t).palette[e].main,backgroundColor:t.alpha((t.vars||t).palette[e].main,(t.vars||t).palette.action.selectedOpacity),"&:hover":{backgroundColor:t.alpha((t.vars||t).palette[e].main,`${(t.vars||t).palette.action.selectedOpacity} + ${(t.vars||t).palette.action.hoverOpacity}`),"@media (hover: none)":{backgroundColor:t.alpha((t.vars||t).palette[e].main,(t.vars||t).palette.action.selectedOpacity)}}}}})),{props:{fullWidth:!0},style:{width:"100%"}},{props:{size:"small"},style:{padding:7,fontSize:t.typography.pxToRem(13)}},{props:{size:"large"},style:{padding:15,fontSize:t.typography.pxToRem(15)}}]}))),V=f.forwardRef(function(e,n){const{value:l,...a}=f.useContext(H),p=f.useContext(Y),v=J({...a,selected:xt(e.value,l)},e),d=z({props:v,name:"MuiToggleButton"}),{children:h,className:x,color:r="standard",disabled:u=!1,disableFocusRipple:i=!1,fullWidth:c=!1,onChange:y,onClick:C,selected:B,size:I="medium",value:T,...P}=d,w={...d,color:r,disabled:u,disableFocusRipple:i,fullWidth:c,size:I},M=ht(w),A=g=>{C&&(C(g,T),g.defaultPrevented)||y&&y(g,T)},b=p||"";return o.jsx(bt,{className:E(a.className,M.root,x,b),disabled:u,focusRipple:!i,ref:n,onClick:A,onChange:y,value:T,ownerState:w,"aria-pressed":B,...P,children:h})});function mt(t){return q("MuiToggleButtonGroup",t)}const s=_("MuiToggleButtonGroup",["root","selected","horizontal","vertical","disabled","grouped","groupedHorizontal","groupedVertical","fullWidth","firstButton","lastButton","middleButton"]),yt=t=>{const{classes:e,orientation:n,fullWidth:l,disabled:a}=t,p={root:["root",n,l&&"fullWidth"],grouped:["grouped",`grouped${R(n)}`,a&&"disabled"],firstButton:["firstButton"],lastButton:["lastButton"],middleButton:["middleButton"]};return O(p,mt,e)},Bt=L("div",{name:"MuiToggleButtonGroup",slot:"Root",overridesResolver:(t,e)=>{const{ownerState:n}=t;return[{[`& .${s.grouped}`]:e.grouped},{[`& .${s.grouped}`]:e[`grouped${R(n.orientation)}`]},{[`& .${s.firstButton}`]:e.firstButton},{[`& .${s.lastButton}`]:e.lastButton},{[`& .${s.middleButton}`]:e.middleButton},e.root,n.orientation==="vertical"&&e.vertical,n.fullWidth&&e.fullWidth]}})(G(({theme:t})=>({display:"inline-flex",borderRadius:(t.vars||t).shape.borderRadius,variants:[{props:{orientation:"vertical"},style:{flexDirection:"column",[`& .${s.grouped}`]:{[`&.${s.selected} + .${s.grouped}.${s.selected}`]:{borderTop:0,marginTop:0}},[`& .${s.firstButton},& .${s.middleButton}`]:{borderBottomLeftRadius:0,borderBottomRightRadius:0},[`& .${s.lastButton},& .${s.middleButton}`]:{marginTop:-1,borderTop:"1px solid transparent",borderTopLeftRadius:0,borderTopRightRadius:0},[`& .${s.lastButton}.${j.disabled},& .${s.middleButton}.${j.disabled}`]:{borderTop:"1px solid transparent"}}},{props:{fullWidth:!0},style:{width:"100%"}},{props:{orientation:"horizontal"},style:{[`& .${s.grouped}`]:{[`&.${s.selected} + .${s.grouped}.${s.selected}`]:{borderLeft:0,marginLeft:0}},[`& .${s.firstButton},& .${s.middleButton}`]:{borderTopRightRadius:0,borderBottomRightRadius:0},[`& .${s.lastButton},& .${s.middleButton}`]:{marginLeft:-1,borderLeft:"1px solid transparent",borderTopLeftRadius:0,borderBottomLeftRadius:0},[`& .${s.lastButton}.${j.disabled},& .${s.middleButton}.${j.disabled}`]:{borderLeft:"1px solid transparent"}}}]}))),Ct=f.forwardRef(function(e,n){const l=z({props:e,name:"MuiToggleButtonGroup"}),{children:a,className:p,color:v="standard",disabled:d=!1,exclusive:h=!1,fullWidth:x=!1,onChange:r,orientation:u="horizontal",size:i="medium",value:c,...y}=l,C={...l,disabled:d,fullWidth:x,orientation:u,size:i},B=yt(C),I=f.useCallback((b,g)=>{if(!r)return;const S=c&&c.indexOf(g);let N;c&&S>=0?(N=c.slice(),N.splice(S,1)):N=c?c.concat(g):[g],r(b,N)},[r,c]),T=f.useCallback((b,g)=>{r&&r(b,c===g?null:g)},[r,c]),P=f.useMemo(()=>({className:B.grouped,onChange:h?T:I,value:c,size:i,fullWidth:x,color:v,disabled:d}),[B.grouped,h,T,I,c,i,x,v,d]),w=ct(a),M=w.length,A=b=>{const g=b===0,S=b===M-1;return g&&S?"":g?B.firstButton:S?B.lastButton:B.middleButton};return o.jsx(Bt,{role:"group",className:E(B.root,p),ref:n,ownerState:C,...y,children:o.jsx(H.Provider,{value:P,children:w.map((b,g)=>o.jsx(Y.Provider,{value:A(g),children:b},g))})})});/**
 * @license @tabler/icons-react v3.35.0 - MIT
 *
 * This source code is licensed under the MIT license.
 * See the LICENSE file in the root directory of this source tree.
 */const $t=[["path",{d:"M5 12l14 0",key:"svg-0"}]],jt=Z("outline","minus","Minus",$t),Rt=({inputRef:t,slotProps:e,onChange:n,...l})=>{const a=f.useRef(null),p=tt(a,t),v=k(r=>{r.target.value=`${Math.max(1,parseInt(r.target.value)||1)}`,n?.(r)}),d=k(r=>{if(n){const u={target:{value:r.toString()},currentTarget:{value:r.toString()},type:"change",bubbles:!0,cancelable:!0,preventDefault:()=>{},stopPropagation:()=>{}};n(u)}}),h=k(()=>{const{current:r}=a;if(!r)return;const u=Number(r.value),i=Math.max(1,u-1);r.value=i.toString(),d(i)}),x=k(()=>{const{current:r}=a;if(!r)return;const u=Number(r.value),i=Math.max(1,u+1);r.value=i.toString(),d(i)});return o.jsx(it,{...l,type:"number",sx:{"& input":{textAlign:"center",width:"2.5rem"}},inputRef:p,onChange:v,startAdornment:o.jsx(D,{position:"start",children:o.jsx(W,{onClick:h,children:o.jsx(jt,{})})}),endAdornment:o.jsx(D,{position:"end",children:o.jsx(W,{onClick:x,children:o.jsx(lt,{})})}),slotProps:{input:{step:1,sx:{textAlign:"center",...e?.input?.sx},min:1,...e?.input},...e}})};function St(){const[{config:{networkType:t,initNodesNumber:e,modelInfo:n},clusterInfo:{status:l}},{config:{setNetworkType:a,setInitNodesNumber:p},init:v}]=et(),d=ot(),[h,x]=f.useState(!1),r=k(async()=>{if(l==="idle"||l==="failed"){x(!0),Promise.resolve().then(()=>v()).then(()=>d("/join")).catch(u=>console.error(u)).finally(()=>x(!1));return}else d("/join")});return o.jsxs(dt,{children:[o.jsx(m,{variant:"h1",children:"Build Your Own AI Cluster"}),o.jsxs($,{gap:2.5,children:[o.jsxs($,{gap:.5,children:[o.jsx(m,{variant:"body1",children:"Step 1 - Specify the initial number of nodes"}),o.jsxs(m,{variant:"body2",color:"text.secondary",fontWeight:"regular",children:["Parallax runs and hosts model distributedly on your everyday hardware. Select the number of nodes you would like to add to your cluster with their connection types."," "]})]}),o.jsxs($,{direction:"row",justifyContent:"space-between",alignItems:"center",gap:2,children:[o.jsx(m,{color:"text.secondary",children:"Node Number"}),o.jsx(Rt,{sx:{width:"10rem",boxShadow:"none",bgcolor:"transparent"},slotProps:{root:{sx:{bgcolor:"transparent","&:hover":{bgcolor:"transparent"},"&:focus-within":{bgcolor:"transparent"}}},input:{sx:{bgcolor:"transparent !important","&:focus":{outline:"none"}}}},value:e,onChange:u=>p(Number(u.target.value))})]}),o.jsxs($,{direction:"row",justifyContent:"space-between",alignItems:"center",gap:2,children:[o.jsx(m,{color:"text.secondary",children:"Are you nodes within the same local network?"}),o.jsxs(Ct,{sx:{width:"10rem",textTransform:"none"},exclusive:!0,value:t,onChange:(u,i)=>i&&a(i),children:[o.jsx(V,{value:"local",sx:{textTransform:"none"},children:"Local"}),o.jsx(V,{value:"remote",sx:{textTransform:"none"},children:"Remote"})]})]})]}),o.jsxs($,{gap:2.5,children:[o.jsxs($,{gap:.5,children:[o.jsx(m,{variant:"body1",children:"Step 2 - Select the model you would like to host"}),o.jsx(m,{variant:"body2",color:"text.secondary",fontWeight:"regular",children:"Currently we support a handful of state-of-the-art open source models. Do keep in mind that larger models require more nodes to host, so If this is your first time trying Parallax, we suggest you to start with smaller models."})]}),o.jsx(ut,{}),!!n&&n.vram>0&&o.jsx(nt,{severity:"warning",variant:"standard",children:o.jsx(m,{variant:"inherit",children:["Youâ€™ll need a ",o.jsx("strong",{children:`minimum of ${n.vram} GB of total VRAM`})," to host this model."]})},"vram-warning")]}),o.jsx($,{direction:"row",justifyContent:"flex-end",alignItems:"center",gap:2,children:o.jsx(rt,{loading:h,onClick:r,children:"Continue"})})]})}export{St as default};


================================================================================
File: src/frontend/dist/chat.html
Size: 568 B
================================================================================

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/assets/gradient-icon-CRwZKfVU.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CHAT Parallax by Gradient</title>
    <script type="module" crossorigin src="/assets/chat-CLr2c_Zd.js"></script>
    <link rel="modulepreload" crossorigin href="/assets/App-BlsOxMPe.js">
    <link rel="stylesheet" crossorigin href="/assets/App-J8fDKCu3.css">
  </head>
  <body>
    <div id="root"></div>
  </body>
</html>


================================================================================
File: src/frontend/dist/index.html
Size: 563 B
================================================================================

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/assets/gradient-icon-CRwZKfVU.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Parallax by Gradient</title>
    <script type="module" crossorigin src="/assets/main-C3viVoAI.js"></script>
    <link rel="modulepreload" crossorigin href="/assets/App-BlsOxMPe.js">
    <link rel="stylesheet" crossorigin href="/assets/App-J8fDKCu3.css">
  </head>
  <body>
    <div id="root"></div>
  </body>
</html>


================================================================================
File: src/frontend/eslint.config.js
Size: 687 B
================================================================================

import js from '@eslint/js';
import globals from 'globals';
import reactHooks from 'eslint-plugin-react-hooks';
import reactRefresh from 'eslint-plugin-react-refresh';
import tseslint from 'typescript-eslint';
import { globalIgnores } from 'eslint/config';

export default tseslint.config([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      js.configs.recommended,
      tseslint.configs.recommended,
      reactHooks.configs['recommended-latest'],
      reactRefresh.configs.vite,
    ],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
    },
    rules: {
      '@typescript-eslint/no-unused-vars': 'off',
    },
  },
]);


================================================================================
File: src/frontend/index.html
Size: 390 B
================================================================================

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="./src/assets/gradient-icon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Parallax by Gradient</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>


================================================================================
File: src/frontend/package.json
Size: 1.73 kB
================================================================================

{
  "name": "parallax",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc -b && vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "@ai-sdk/openai": "^2.0.35",
    "@ai-sdk/react": "^2.0.52",
    "@emotion/react": "^11.14.0",
    "@emotion/styled": "^11.14.1",
    "@mui/material": "^7.3.2",
    "@mui/types": "^7.4.6",
    "@mui/utils": "^7.3.2",
    "@mui/x-charts": "^8.11.3",
    "@mui/x-data-grid": "^8.11.3",
    "@mui/x-date-pickers": "^8.11.3",
    "@mui/x-tree-view": "^8.11.3",
    "@tabler/icons-react": "^3.35.0",
    "ai": "^5.0.52",
    "clsx": "^2.1.1",
    "dayjs": "^1.11.18",
    "framer-motion": "^12.23.13",
    "katex": "^0.16.22",
    "motion": "^12.23.18",
    "notistack": "^3.0.2",
    "react": "^19.1.1",
    "react-dom": "^19.1.1",
    "react-hook-form": "^7.62.0",
    "react-markdown": "^10.1.0",
    "react-router-dom": "^7.9.1",
    "rehype-katex": "^7.0.1",
    "rehype-raw": "^7.0.0",
    "rehype-sanitize": "^6.0.0",
    "remark-gfm": "^4.0.1",
    "remark-math": "^6.0.0"
  },
  "devDependencies": {
    "@eslint/js": "^9.33.0",
    "@gradient-network/prettier-config": "^0.0.1",
    "@types/node": "^22.18.12",
    "@types/react": "^19.1.10",
    "@types/react-dom": "^19.1.7",
    "@vitejs/plugin-react-swc": "^4.0.0",
    "eslint": "^9.33.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.20",
    "globals": "^16.3.0",
    "prettier": "^3.6.2",
    "typescript": "~5.8.3",
    "typescript-eslint": "^8.39.1",
    "vite": "^7.1.2"
  },
  "prettier": "@gradient-network/prettier-config",
  "engines": {
    "node": ">=22",
    "pnpm": ">=10"
  }
}


================================================================================
File: src/frontend/src/App.tsx
Size: 1.54 kB
================================================================================

import './global.css';

import type { FC, PropsWithChildren } from 'react';
import { StrictMode } from 'react';
import { HashRouter } from 'react-router-dom';
import { CssBaseline, styled } from '@mui/material';
import { ThemeProvider } from './themes';
import { MainRouter, ChatRouter } from './router';
import { ChatProvider, ClusterProvider, HostProvider, type HostProps } from './services';

const AppRoot = styled('div')(({ theme }) => {
  const { palette, typography } = theme;
  return {
    ...typography.body2,

    color: palette.text.primary,
    backgroundColor: palette.background.default,

    width: '100%',
    height: '100%',
    display: 'flex',
    flexFlow: 'column nowrap',
    justifyContent: 'center',
    alignItems: 'center',
  };
});

const Providers: FC<PropsWithChildren & { readonly hostProps: HostProps }> = ({
  children,
  hostProps,
}) => {
  return (
    <StrictMode>
      <HashRouter>
        <ThemeProvider>
          <CssBaseline />
          <AppRoot>
            <HostProvider {...hostProps}>
              <ClusterProvider>
                <ChatProvider>{children}</ChatProvider>
              </ClusterProvider>
            </HostProvider>
          </AppRoot>
        </ThemeProvider>
      </HashRouter>
    </StrictMode>
  );
};

export const Main = () => {
  return (
    <Providers hostProps={{ type: 'cluster' }}>
      <MainRouter />
    </Providers>
  );
};

export const Chat = () => {
  return (
    <Providers hostProps={{ type: 'node' }}>
      <ChatRouter />
    </Providers>
  );
};


================================================================================
File: src/frontend/src/apis/data.ts
Size: 373 B
================================================================================

export const TYPE_PATH_MAP = {
  model_list: '/api/model/list',
  scheduler_init: '/api/scheduler/init',
  node_join_command: '/api/node/join_command',
} as const;

export type TypePathMap = typeof TYPE_PATH_MAP;

export type TypeDataMap = {
  readonly model_list: [never, never];
  readonly scheduler_init: [never, never];
  readonly node_join_command: [never, never];
};


================================================================================
File: src/frontend/src/apis/http.ts
Size: 27 B
================================================================================

export const request = {};


================================================================================
File: src/frontend/src/apis/index.ts
Size: 48 B
================================================================================

export * from './data';
export * from './http';


================================================================================
File: src/frontend/src/assets/gradient-icon.svg
Size: 221 B
================================================================================

<svg width="26" height="28" viewBox="0 0 26 28" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M7 19H4V22H7V19Z" fill="black"/>
<path d="M22 6V8.85011L12.9475 22H10V19.1499L19.0521 6H22Z" fill="black"/>
</svg>



================================================================================
File: src/frontend/src/assets/models/OpenAI-black-monoblossom.svg
Size: 2.97 kB
================================================================================

<svg width="721" height="721" viewBox="0 0 721 721" fill="none" xmlns="http://www.w3.org/2000/svg">
<g clip-path="url(#clip0_1637_2934)">
<g clip-path="url(#clip1_1637_2934)">
<path d="M304.246 294.611V249.028C304.246 245.189 305.687 242.309 309.044 240.392L400.692 187.612C413.167 180.415 428.042 177.058 443.394 177.058C500.971 177.058 537.44 221.682 537.44 269.182C537.44 272.54 537.44 276.379 536.959 280.218L441.954 224.558C436.197 221.201 430.437 221.201 424.68 224.558L304.246 294.611ZM518.245 472.145V363.224C518.245 356.505 515.364 351.707 509.608 348.349L389.174 278.296L428.519 255.743C431.877 253.826 434.757 253.826 438.115 255.743L529.762 308.523C556.154 323.879 573.905 356.505 573.905 388.171C573.905 424.636 552.315 458.225 518.245 472.141V472.145ZM275.937 376.182L236.592 353.152C233.235 351.235 231.794 348.354 231.794 344.515V238.956C231.794 187.617 271.139 148.749 324.4 148.749C344.555 148.749 363.264 155.468 379.102 167.463L284.578 222.164C278.822 225.521 275.942 230.319 275.942 237.039V376.186L275.937 376.182ZM360.626 425.122L304.246 393.455V326.283L360.626 294.616L417.002 326.283V393.455L360.626 425.122ZM396.852 570.989C376.698 570.989 357.989 564.27 342.151 552.276L436.674 497.574C442.431 494.217 445.311 489.419 445.311 482.699V343.552L485.138 366.582C488.495 368.499 489.936 371.379 489.936 375.219V480.778C489.936 532.117 450.109 570.985 396.852 570.985V570.989ZM283.134 463.99L191.486 411.211C165.094 395.854 147.343 363.229 147.343 331.562C147.343 294.616 169.415 261.509 203.48 247.593V356.991C203.48 363.71 206.361 368.508 212.117 371.866L332.074 441.437L292.729 463.99C289.372 465.907 286.491 465.907 283.134 463.99ZM277.859 542.68C223.639 542.68 183.813 501.895 183.813 451.514C183.813 447.675 184.294 443.836 184.771 439.997L279.295 494.698C285.051 498.056 290.812 498.056 296.568 494.698L417.002 425.127V470.71C417.002 474.549 415.562 477.429 412.204 479.346L320.557 532.126C308.081 539.323 293.206 542.68 277.854 542.68H277.859ZM396.852 599.776C454.911 599.776 503.37 558.513 514.41 503.812C568.149 489.896 602.696 439.515 602.696 388.176C602.696 354.587 588.303 321.962 562.392 298.45C564.791 288.373 566.231 278.296 566.231 268.224C566.231 199.611 510.571 148.267 446.274 148.267C433.322 148.267 420.846 150.184 408.37 154.505C386.775 133.392 357.026 119.958 324.4 119.958C266.342 119.958 217.883 161.22 206.843 215.921C153.104 229.837 118.557 280.218 118.557 331.557C118.557 365.146 132.95 397.771 158.861 421.283C156.462 431.36 155.022 441.437 155.022 451.51C155.022 520.123 210.682 571.466 274.978 571.466C287.931 571.466 300.407 569.549 312.883 565.228C334.473 586.341 364.222 599.776 396.852 599.776Z" fill="black"/>
</g>
</g>
<defs>
<clipPath id="clip0_1637_2934">
<rect width="720" height="720" fill="white" transform="translate(0.606934 0.0999756)"/>
</clipPath>
<clipPath id="clip1_1637_2934">
<rect width="484.139" height="479.818" fill="white" transform="translate(118.557 119.958)"/>
</clipPath>
</defs>
</svg>


================================================================================
File: src/frontend/src/chat.tsx
Size: 141 B
================================================================================

import { createRoot } from 'react-dom/client';
import { Chat } from './App';

createRoot(document.getElementById('root')!).render(<Chat />);


================================================================================
File: src/frontend/src/components/brand/icon.tsx
Size: 278 B
================================================================================

export const IconBrandGradient = () => (
  <svg width='26' height='28' viewBox='0 0 26 28' fill='none' xmlns='http://www.w3.org/2000/svg'>
    <path d='M7 19H4V22H7V19Z' fill='black' />
    <path d='M22 6V8.85011L12.9475 22H10V19.1499L19.0521 6H22Z' fill='black' />
  </svg>
);


================================================================================
File: src/frontend/src/components/brand/index.ts
Size: 48 B
================================================================================

export * from './logo';
export * from './icon';


================================================================================
File: src/frontend/src/components/brand/logo.tsx
Size: 6.23 kB
================================================================================

export const LogoGradient = () => (
  <svg width='100' height='21' viewBox='0 0 100 21' fill='none' xmlns='http://www.w3.org/2000/svg'>
    <path
      d='M25.9027 21C24.2555 21 22.9819 20.6707 22.082 20.012C21.1821 19.3687 20.6482 18.4344 20.4805 17.209H23.7064C23.9352 18.1586 24.6825 18.6335 25.9485 18.6335C26.8789 18.6335 27.5576 18.4037 27.9847 17.9442C28.4118 17.5 28.6253 16.8184 28.6253 15.8993V14.0383C28.3203 14.5897 27.8779 15.0339 27.2984 15.3709C26.7188 15.6926 26.0324 15.8534 25.2393 15.8534C24.2478 15.8534 23.3556 15.6313 22.5624 15.1871C21.7693 14.7276 21.1439 14.0536 20.6864 13.1652C20.2288 12.2768 20 11.2046 20 9.94858C20 8.69256 20.2288 7.62035 20.6864 6.73195C21.1439 5.82823 21.7693 5.15427 22.5624 4.71007C23.3556 4.25055 24.2478 4.02079 25.2393 4.02079C25.9866 4.02079 26.6654 4.18928 27.2755 4.52626C27.8856 4.86324 28.3355 5.29978 28.6253 5.83589V4.36543H31.7826V15.7845C31.7826 17.3928 31.3021 18.6641 30.3412 19.5985C29.3803 20.5328 27.9008 21 25.9027 21ZM25.9256 13.4409C26.7493 13.4409 27.4127 13.1116 27.9161 12.453C28.4194 11.7943 28.6711 10.9519 28.6711 9.9256C28.6711 8.89934 28.4194 8.06455 27.9161 7.42123C27.4127 6.76258 26.7493 6.43326 25.9256 6.43326C25.041 6.43326 24.3699 6.73961 23.9123 7.3523C23.4547 7.96499 23.2259 8.82276 23.2259 9.9256C23.2259 11.0284 23.4547 11.8939 23.9123 12.5219C24.3699 13.1346 25.041 13.4409 25.9256 13.4409Z'
      fill='black'
    />
    <path
      d='M33.39 16.1751V4.36543H36.4787V6.91575C36.8905 5.81291 37.4091 5.03939 38.0344 4.59519C38.6598 4.15098 39.5368 3.95952 40.6655 4.02079V6.98468C40.3299 6.95405 40.0783 6.93873 39.9105 6.93873C39.4682 6.93873 39.0487 7 38.6522 7.12254C37.2489 7.59737 36.5473 9.01422 36.5473 11.3731V16.1751H33.39Z'
      fill='black'
    />
    <path
      d='M44.1585 16.4967C42.9382 16.4967 41.9316 16.1751 41.1384 15.5317C40.3606 14.8884 39.9716 14 39.9716 12.8665C39.9716 11.9628 40.2157 11.2659 40.7037 10.7757C41.2071 10.2702 41.7714 9.9256 42.3968 9.74179C43.0374 9.55799 43.7619 9.41247 44.5703 9.30525L45.5312 9.19037C46.3396 9.09847 46.9192 8.96061 47.27 8.7768C47.636 8.57768 47.8191 8.20241 47.8191 7.65098C47.8191 7.19147 47.6589 6.83917 47.3386 6.59409C47.0183 6.34902 46.5607 6.22648 45.9659 6.22648C45.3863 6.22648 44.9135 6.34902 44.5474 6.59409C44.1966 6.82385 43.9754 7.14551 43.8839 7.55908H40.4978C40.7266 6.44092 41.3138 5.57549 42.2595 4.9628C43.2204 4.33479 44.4788 4.02079 46.0345 4.02079C49.2986 4.02079 50.9306 5.26149 50.9306 7.74289V13.326C50.9306 14.2451 51.0602 15.1947 51.3195 16.1751H48.208C48.086 15.6083 48.0021 15.1105 47.9563 14.6816C47.6055 15.233 47.1022 15.6772 46.4463 16.0142C45.7905 16.3359 45.0278 16.4967 44.1585 16.4967ZM45.1422 14.1532C45.9354 14.1532 46.576 13.8928 47.0641 13.372C47.5674 12.8512 47.8191 12.1849 47.8191 11.3731V10.2013C47.6055 10.4617 47.3386 10.6608 47.0183 10.7987C46.7133 10.9365 46.2709 11.0514 45.6913 11.1433C44.8372 11.2965 44.2195 11.4803 43.8381 11.6947C43.4568 11.8939 43.2662 12.2538 43.2662 12.7746C43.2662 13.2035 43.4416 13.5405 43.7924 13.7856C44.1432 14.0306 44.5932 14.1532 45.1422 14.1532Z'
      fill='black'
    />
    <path
      d='M63.6153 0.0919039V16.1751H60.458V14.7965C60.1682 15.3173 59.7183 15.7385 59.1082 16.0602C58.4981 16.3665 57.7964 16.5197 57.0033 16.5197C56.0119 16.5197 55.112 16.267 54.3036 15.7615C53.5105 15.256 52.8775 14.5361 52.4047 13.6018C51.9471 12.6521 51.7183 11.5416 51.7183 10.2702C51.7183 8.99891 51.9471 7.89606 52.4047 6.96171C52.8775 6.01203 53.5105 5.28446 54.3036 4.77899C55.112 4.27352 56.0119 4.02079 57.0033 4.02079C57.7812 4.02079 58.4752 4.18928 59.0853 4.52626C59.7106 4.84792 60.1682 5.2768 60.458 5.81291V0.0919039H63.6153ZM57.6897 14.0383C58.5438 14.0383 59.2226 13.686 59.7259 12.9814C60.2445 12.2768 60.5038 11.3731 60.5038 10.2702C60.5038 9.1674 60.2445 8.26368 59.7259 7.55908C59.2226 6.85449 58.5438 6.50219 57.6897 6.50219C56.7898 6.50219 56.1034 6.83917 55.6306 7.51313C55.173 8.17177 54.9442 9.09081 54.9442 10.2702C54.9442 11.4497 55.173 12.3764 55.6306 13.0503C56.1034 13.709 56.7898 14.0383 57.6897 14.0383Z'
      fill='black'
    />
    <path
      d='M68.4939 3.10175H64.9476V0H68.4939V3.10175ZM68.2879 16.1751H65.1764V4.36543H68.2879V16.1751Z'
      fill='black'
    />
    <path
      d='M75.1212 14.0842C76.2194 14.0842 76.9515 13.7396 77.3176 13.0503H80.6121C80.2766 14.1225 79.6207 14.9726 78.6446 15.6007C77.6836 16.2133 76.4939 16.5197 75.0755 16.5197C73.7942 16.5197 72.6808 16.2593 71.7351 15.7385C70.8047 15.2177 70.0879 14.4902 69.5845 13.5558C69.0812 12.6061 68.8295 11.5186 68.8295 10.2932C68.8295 9.09847 69.1041 8.02626 69.6532 7.07659C70.2023 6.1116 70.9344 5.36105 71.8495 4.82495C72.7799 4.28884 73.7942 4.02079 74.8924 4.02079C76.0821 4.02079 77.1269 4.30416 78.0268 4.8709C78.942 5.42232 79.6589 6.24179 80.1774 7.32932C80.696 8.40153 80.9629 9.69584 80.9782 11.2123H72.0555C72.1165 12.0853 72.4291 12.7823 72.9935 13.3031C73.5731 13.8239 74.2823 14.0842 75.1212 14.0842ZM75.0068 6.45624C74.2594 6.45624 73.6112 6.70131 73.0621 7.19147C72.5283 7.68162 72.2003 8.29431 72.0783 9.02954H77.8438C77.7065 8.24836 77.3938 7.62801 76.9058 7.16849C76.4177 6.69365 75.7847 6.45624 75.0068 6.45624Z'
      fill='black'
    />
    <path
      d='M81.5786 16.1751V4.36543H84.6444V5.85886C84.9647 5.35339 85.407 4.92451 85.9713 4.57221C86.5357 4.2046 87.2449 4.02079 88.0991 4.02079C89.304 4.02079 90.2421 4.39606 90.9132 5.14661C91.5995 5.89716 91.9427 6.95405 91.9427 8.31729V16.1751H88.7854V8.54705C88.7854 7.21444 88.2135 6.54814 87.0695 6.54814C86.4289 6.54814 85.8798 6.80853 85.4222 7.32932C84.9647 7.83479 84.7359 8.593 84.7359 9.60394V16.1751H81.5786Z'
      fill='black'
    />
    <path
      d='M98.1011 16.2899C96.9876 16.2899 96.0801 16.0066 95.3785 15.4398C94.6921 14.8731 94.3489 13.9311 94.3489 12.6138V6.7779H92.3356V4.36543H94.3489V0.873086H97.4147V4.36543H99.7712V6.7779H97.4147V12.0853C97.4147 12.698 97.5291 13.1575 97.7579 13.4639C98.0019 13.7549 98.3985 13.9004 98.9476 13.9004C99.2374 13.9004 99.5882 13.8545 100 13.7626V16.0372C99.5272 16.2057 98.8942 16.2899 98.1011 16.2899Z'
      fill='black'
    />
    <path d='M3 13H0V16H3V13Z' fill='black' />
    <path d='M18 0V2.85011L8.94745 16H6V13.1499L15.0521 0H18Z' fill='black' />
  </svg>
);


================================================================================
File: src/frontend/src/components/common/drawer-layout.tsx
Size: 11.18 kB
================================================================================

import { useEffect, useState, type FC, type PropsWithChildren } from 'react';
import {
  Box,
  Button,
  Divider,
  IconButton,
  Stack,
  styled,
  Tooltip,
  Typography,
} from '@mui/material';
import { useCluster, useHost } from '../../services';
import { useAlertDialog } from '../mui';
import { IconBrandGradient } from '../brand';
import {
  IconCirclePlus,
  IconInfoCircle,
  IconLayoutSidebarLeftCollapse,
  IconLayoutSidebarLeftExpand,
  IconLayoutSidebarRightCollapse,
  IconLayoutSidebarRightExpand,
  IconPlus,
  IconTopologyStar3,
} from '@tabler/icons-react';
import { JoinCommand, ModelSelect, NodeList } from '../inputs';

const DrawerLayoutRoot = styled(Stack)(({ theme }) => {
  const { spacing } = theme;
  return {
    width: '100%',
    height: '100%',
    justifyContent: 'flex-start',
    alignItems: 'stretch',
    overflow: 'hidden',
  };
});

const DrawerLayoutSide = styled(Stack)(({ theme }) => {
  const { palette, spacing } = theme;
  return {
    height: '100%',
    paddingBlock: spacing(2),
    paddingInline: spacing(2),
    gap: spacing(3),
    overflow: 'hidden',
    transition: 'width 0.3s ease-in-out',
    backgroundColor: palette.grey[200],
  };
});

const DrawerLayoutHeader = styled(Stack)(({ theme }) => {
  const { spacing } = theme;
  return {
    width: '100%',
    height: '2.5rem',
    flex: 'none',
    marginTop: spacing(1),
    paddingBlock: spacing(1),
    paddingInline: spacing(4),
    overflow: 'hidden',
  };
});

const DrawerLayoutContainer = styled(Stack)(({ theme }) => {
  const { palette, spacing } = theme;
  return {
    flex: 1,
    alignItems: 'center',
    overflow: 'hidden',
    backgroundColor: palette.grey[100],
  };
});

const DrawerLayoutContent = styled(Stack)(({ theme }) => {
  const { spacing } = theme;
  return {
    width: '48.75rem',
    maxWidth: '100%',
    height: '100%',
    gap: spacing(2),
    paddingBlock: spacing(1),
    paddingInline: spacing(4),
    overflow: 'hidden',
  };
});

export const DrawerLayout: FC<PropsWithChildren> = ({ children }) => {
  const [{ type: hostType }] = useHost();

  const [
    {
      config: { modelInfo },
      clusterInfo: { status: clusterStatus, needMoreNodes },
    },
  ] = useCluster();

  const [dialogWaiting, { open: openWaiting }] = useAlertDialog({
    color: 'primary',
    titleIcon: <IconInfoCircle />,
    title: 'Reconnect your nodes',
    content: (
      <Stack sx={{ gap: 7 }}>
        <Stack sx={{ gap: 1 }}>
          <Typography variant='body1'>Run join command on your new Node</Typography>
          <JoinCommand />
        </Stack>
        <Stack sx={{ gap: 1 }}>
          <Typography variant='body1'>Check your live node status</Typography>
          <Typography variant='body2' color='text.disabled'>
            After you successfully start the server on the nodes, you should see them show up on the
            below dashboard.
          </Typography>
          <NodeList />
        </Stack>
      </Stack>
    ),
    confirmLabel: 'Finish',
  });
  useEffect(() => {
    if (hostType === 'cluster' && clusterStatus === 'waiting') {
      openWaiting();
    }
  }, [clusterStatus, openWaiting]);

  const [dialogRebalancing, { open: openRebalancing }] = useAlertDialog({
    color: 'primary',
    title: '',
    content: (
      <>
        <Typography variant='body1'>Cluster rebalancing</Typography>
        <Typography variant='body2' color='text.disabled'>
          We have noticed one of your nodes has been disconnected. We are now rebalancing your
          inference requests onto working nodes. Please wait a few seconds for the cluster to
          rebalance itself.
        </Typography>
        <NodeList variant='menu' />
      </>
    ),
    confirmLabel: 'Finish',
  });
  useEffect(() => {
    if (clusterStatus === 'rebalancing') {
      openRebalancing();
    }
  }, [clusterStatus, openRebalancing]);

  const [dialogNeedMoreNodes, { open: openDialogNeedMoreNodes }] = useAlertDialog({
    color: 'primary',
    title: '',
    content: (
      <>
        <Typography variant='body1'>
          Your selected model requires more nodes.
          {(!!modelInfo
            && modelInfo.vram > 0 && [
              `Youâ€™ll need a `,
              <strong>{`minimum of ${modelInfo.vram} GB of total VRAM`}</strong>,
              ` to host this model.`,
            ])
            || ''}
        </Typography>
      </>
    ),
    confirmLabel: 'Finish',
  });
  useEffect(() => {
    if (needMoreNodes) {
      openDialogNeedMoreNodes();
    }
  }, [needMoreNodes, openDialogNeedMoreNodes]);

  const [dialogFailed, { open: openFailed }] = useAlertDialog({
    color: 'primary',
    title: '',
    content: (
      <>
        <Typography variant='body1'>Scheduler restart</Typography>
        <Typography variant='body2' color='text.disabled'>
          We have noticed that your scheduler has been disconnected (this would be the computer that
          ran the <strong>parallax run</strong> command). You would need to restart the scheduler,
          reconfigure the cluster, and your chat will be back up again!
        </Typography>
      </>
    ),
    confirmLabel: 'Finish',
  });
  useEffect(() => {
    if (clusterStatus === 'failed') {
      openFailed();
      return;
    }
    if (clusterStatus === 'idle') {
      // Delay trigger, due to the cluster init status is 'idle' before connecting to the scheduler.
      const timeoutId = setTimeout(() => openFailed(), 1000);
      return () => clearTimeout(timeoutId);
    }
  }, [clusterStatus, openFailed]);

  const [sidebarExpanded, setMenuOpen] = useState(true);

  const [dialogJoinCommand, { open: openJoinCommand }] = useAlertDialog({
    color: 'primary',
    titleIcon: <IconCirclePlus />,
    title: 'Add New Nodes',
    content: (
      <Stack sx={{ gap: 5 }}>
        <Stack sx={{ gap: 1 }}>
          <Typography variant='body1'>Run join command on all nodes</Typography>
          <JoinCommand />
        </Stack>
        <Stack sx={{ gap: 1 }}>
          <Typography variant='body1'>Check your live node status</Typography>
          <Typography variant='body2' color='text.disabled'>
            After you successfully start the server on the nodes, you should see them show up on the
            below dashboard.
          </Typography>
          <NodeList />
        </Stack>
      </Stack>
    ),
    confirmLabel: 'Finish',
  });

  const IconCluster = () => (
    <svg width='1.5rem' height='1.5rem' viewBox='0 0 27 27' fill='currentColor'>
      <g
        fill='none'
        stroke='currentColor'
        stroke-linecap='round'
        stroke-linejoin='round'
        stroke-width='2'
      >
        <rect width='6' height='6' x='16' y='16' rx='1' />
        <rect width='6' height='6' x='2' y='16' rx='1' />
        <rect width='6' height='6' x='9' y='2' rx='1' />
        <path d='M5 16v-3a1 1 0 0 1 1-1h12a1 1 0 0 1 1 1v3m-7-4V8' />
      </g>
    </svg>
  );

  return (
    <DrawerLayoutRoot direction='row'>
      <DrawerLayoutSide
        sx={{
          width: sidebarExpanded ? '16.25rem' : '3.5rem',
          paddingInline: sidebarExpanded ? 2 : 2,
        }}
      >
        <Stack direction='row' sx={{ justifyContent: 'flex-end', alignItems: 'center', gap: 2 }}>
          {sidebarExpanded ?
            <>
              <IconBrandGradient />
              <Box sx={{ flex: 1 }} />
              <Tooltip
                title='Collapse Sidebar'
                placement='right'
                slotProps={{
                  tooltip: { sx: { bgcolor: 'primary.main', color: 'common.white' } },
                }}
              >
                <IconButton
                  size='em'
                  sx={{
                    fontSize: '1.5rem',
                    borderRadius: '8px',
                    color: '#808080FF',
                    '&:hover': { bgcolor: 'action.hover' },
                  }}
                  onClick={() => setMenuOpen((prev) => !prev)}
                >
                  <IconLayoutSidebarLeftCollapse />
                </IconButton>
              </Tooltip>
            </>
          : <>
              <Box
                sx={{
                  position: 'relative',
                  width: 28,
                  height: 28,
                  display: 'flex',
                  alignItems: 'center',
                  justifyContent: 'center',
                  '&:hover .logo': { opacity: 0 },
                  '&:hover .toggle': { opacity: 1, pointerEvents: 'auto', transform: 'scale(1)' },
                }}
              >
                <Box
                  className='logo'
                  sx={{
                    position: 'absolute',
                    inset: 0,
                    display: 'flex',
                    alignItems: 'center',
                    justifyContent: 'center',
                    transition: 'opacity .15s ease',
                    opacity: 1,
                  }}
                >
                  <IconBrandGradient />
                </Box>

                <Tooltip
                  title='Expand Sidebar'
                  placement='right'
                  slotProps={{
                    tooltip: { sx: { bgcolor: 'primary.main', color: 'common.white' } },
                  }}
                >
                  <IconButton
                    className='toggle'
                    size='em'
                    sx={{
                      position: 'absolute',
                      opacity: 0,
                      pointerEvents: 'none',
                      fontSize: '1.5rem',
                      transition: 'opacity .15s ease, transform .15s ease',
                      '&:hover': { bgcolor: 'action.hover' },
                    }}
                    aria-label='Expand Sidebar'
                    onClick={() => setMenuOpen((prev) => !prev)}
                  >
                    <IconLayoutSidebarLeftExpand />
                  </IconButton>
                </Tooltip>
              </Box>
            </>
          }
        </Stack>
        {sidebarExpanded && (
          <Stack>
            <Stack direction='row' sx={{ gap: 1, color: 'text.primary' }}>
              {/* <IconCluster /> */}
              <Typography variant='body1' sx={{ mt: '1.5px', color: '#A7A7A7FF', fontWeight: 600 }}>
                Cluster topology
              </Typography>
            </Stack>
            <NodeList variant='menu' sx={{ py: '2rem' }} />
            <Button
              color='info'
              startIcon={<IconPlus />}
              onClick={openJoinCommand}
              // onClick={openRebalancing}
              // onClick={openWaiting}
            >
              Add Nodes
            </Button>
          </Stack>
        )}
      </DrawerLayoutSide>
      <DrawerLayoutContainer>
        <DrawerLayoutHeader direction='row'>
          <ModelSelect variant='text' autoCommit />
        </DrawerLayoutHeader>
        <DrawerLayoutContent>{children}</DrawerLayoutContent>
      </DrawerLayoutContainer>
      {dialogJoinCommand}
      {dialogWaiting}
      {dialogRebalancing}
      {dialogFailed}
      {dialogNeedMoreNodes}
    </DrawerLayoutRoot>
  );
};


================================================================================
File: src/frontend/src/components/common/index.ts
Size: 64 B
================================================================================

export * from './drawer-layout';
export * from './main-layout';


================================================================================
File: src/frontend/src/components/common/main-layout.tsx
Size: 3.75 kB
================================================================================

import type { FC, PropsWithChildren, ReactNode } from 'react';
import { Link as RouterLink } from 'react-router-dom';
import { Box, Link, Stack, styled, Typography } from '@mui/material';
import { LogoGradient } from '../brand';
import { useCluster } from '../../services';

export interface MainLayoutProps {
  contentStart?: ReactNode;
  contentEnd?: ReactNode;
}

const MainLayoutRoot = styled(Stack)(({ theme }) => {
  const { palette, spacing } = theme;
  return {
    width: '100%',
    height: '100%',
    display: 'flex',
    alignItems: 'center',
    gap: spacing(3),
    padding: spacing(3),
    overflow: 'hidden',
    backgroundColor: palette.grey[100],
  };
});

const MainLayoutHeader = styled(Stack)(({ theme }) => {
  const { spacing } = theme;
  return {
    width: '100%',
    flex: 'none',
    justifyContent: 'flex-start',
    alignItems: 'center',
    gap: spacing(1),
  };
});

const MainLayoutContainer = styled(Box)(({ theme }) => {
  const { spacing } = theme;
  return {
    position: 'relative',
    flex: '1',
    width: '100%',
    display: 'flex',
    flexFlow: 'column nowrap',
    justifyContent: 'center',
    alignItems: 'center',
    overflowY: 'hidden',
  };
});

const MainLayoutContent = styled(Stack)(({ theme }) => {
  const { spacing } = theme;
  return {
    width: '31rem',
    height: '100%',
    gap: spacing(7),
    paddingInline: spacing(1),
    overflowY: 'auto',
  };
});

const MainLayoutStart = styled(Stack)(({ theme }) => {
  const { spacing } = theme;
  return {
    width: 'calc((100% - 30rem) / 2 - 4rem)',
    height: '100%',
    overflow: 'auto',
    position: 'absolute',
    top: 0,
    left: '2rem',
    alignItems: 'flex-end',
    gap: spacing(2),
  };
});

const MainLayoutEnd = styled(Stack)(({ theme }) => {
  const { spacing } = theme;
  return {
    width: 'calc((100% - 30rem) / 2 - 4rem)',
    height: '100%',
    overflow: 'auto',
    position: 'absolute',
    top: 0,
    right: '2rem',
    alignItems: 'flex-start',
    gap: spacing(2),
  };
});

export const MainLayout: FC<PropsWithChildren<MainLayoutProps>> = ({
  children,
  contentStart,
  contentEnd = <DebugInfo />,
}) => {
  return (
    <MainLayoutRoot>
      <MainLayoutHeader direction='row'>
        <LogoGradient />
      </MainLayoutHeader>
      <MainLayoutContainer>
        <MainLayoutContent className='MainLayoutContent'>{children}</MainLayoutContent>
        {(contentStart && <MainLayoutStart>{contentStart}</MainLayoutStart>) || undefined}
        {(contentEnd && <MainLayoutEnd>{contentEnd}</MainLayoutEnd>) || undefined}
      </MainLayoutContainer>
    </MainLayoutRoot>
  );
};

const DebugInfo: FC = () => {
  const [
    {
      config: { initNodesNumber, networkType, modelName },
      clusterInfo,
    },
  ] = useCluster();

  const renderRecord = (title: string, record: Record<string, any>) => (
    <Stack gap={1}>
      <Typography variant='subtitle2'>{title}</Typography>
      {Object.entries(record).map(([key, value]) => (
        <Typography variant='body2' key={key}>
          {key}: {JSON.stringify(value)}
        </Typography>
      ))}
    </Stack>
  );

  return (
    (import.meta.env.DEV && (
      <Stack gap={4} data-debug-info='true'>
        <Typography variant='subtitle1'>Debug Info</Typography>

        {renderRecord('Init Parameters', { initNodesNumber, networkType, modelName })}

        {renderRecord('Status Info', clusterInfo)}

        <Stack>
          <Link component={RouterLink} to='/setup'>
            Setup
          </Link>
          <Link component={RouterLink} to='/join'>
            Join
          </Link>
          <Link component={RouterLink} to='/chat'>
            Chat
          </Link>
        </Stack>
      </Stack>
    ))
    || null
  );
};


================================================================================
File: src/frontend/src/components/inputs/chat-input.tsx
Size: 3.61 kB
================================================================================

import { Button, Stack, TextField } from '@mui/material';
import {
  useRef,
  type CompositionEventHandler,
  type FC,
  type KeyboardEventHandler,
  type MouseEventHandler,
} from 'react';
import { useRefCallback } from '../../hooks';
import { useChat, useCluster } from '../../services';
import { IconArrowBackUp, IconArrowUp, IconSquareFilled } from '@tabler/icons-react';
import { DotPulse } from './dot-pulse';

export const ChatInput: FC = () => {
  const [
    {
      clusterInfo: { status: clusterStatus },
    },
  ] = useCluster();
  const [{ input, status }, { setInput, generate, stop, clear }] = useChat();

  const compositionRef = useRef(false);

  const onCompositionStart = useRefCallback<CompositionEventHandler>((e) => {
    compositionRef.current = true;
  });

  const onCompositionEnd = useRefCallback<CompositionEventHandler>((e) => {
    compositionRef.current = false;
  });

  const onKeyDown = useRefCallback<KeyboardEventHandler>((e) => {
    if (e.key === 'Enter' && !e.shiftKey && !compositionRef.current) {
      e.preventDefault();
      generate();
    }
  });

  const onClickMainButton = useRefCallback<MouseEventHandler>((e) => {
    if (status === 'opened' || status === 'generating') {
      stop();
    } else if (status === 'closed' || status === 'error') {
      generate();
    }
  });

  const onClickClearButton = useRefCallback<MouseEventHandler>((e) => {
    clear();
  });

  return (
    <Stack data-status={status}>
      {/* <Stack direction='row' sx={{ gap: 1, p: 1 }}>
        {modelName}
      </Stack> */}
      <TextField
        value={input}
        onChange={(event) => setInput(event.target.value)}
        multiline
        maxRows={4}
        placeholder='Ask anything'
        fullWidth
        onCompositionStart={onCompositionStart}
        onCompositionEnd={onCompositionEnd}
        onKeyDown={onKeyDown}
        slotProps={{
          input: {
            sx: {
              border: '1px solid',
              borderColor: 'grey.300',
              borderRadius: 2,
              fontSize: '0.95rem',
              boxShadow: '2px 2px 4px rgba(0,0,0,0.05)',
              flexDirection: 'column',
              '& textarea': {
                fontSize: '0.875rem',
                scrollbarWidth: 'none', // Firefox
                msOverflowStyle: 'none', // IE, Edge
                '&::-webkit-scrollbar': {
                  display: 'none', // Chrome, Safari
                },
              },
            },
            endAdornment: (
              <Stack direction='row' sx={{ alignSelf: 'flex-end', alignItems: 'center', gap: 2 }}>
                <Button
                  variant='text'
                  sx={{ color: 'text.secondary' }}
                  startIcon={<IconArrowBackUp />}
                  disabled={status === 'opened' || status === 'generating'}
                  onClick={onClickClearButton}
                >
                  Clear
                </Button>
                <Button
                  size='small'
                  color='primary'
                  disabled={clusterStatus !== 'available' || status === 'opened'}
                  // loading={status === 'opened'}
                  onClick={onClickMainButton}
                >
                  {status === 'opened' ?
                    <DotPulse size='medium' />
                  : status === 'generating' ?
                    <IconSquareFilled size='1.25rem' />
                  : <IconArrowUp size='1.25rem' />}
                </Button>
              </Stack>
            ),
          },
        }}
      />
    </Stack>
  );
};


================================================================================
File: src/frontend/src/components/inputs/chat-markdown.tsx
Size: 3.25 kB
================================================================================

/* eslint-disable @typescript-eslint/no-explicit-any */
import { memo } from 'react';
import { styled } from '@mui/material/styles';
import ReactMarkdown, { type Components } from 'react-markdown';
import remarkGfm from 'remark-gfm';
import remarkMath from 'remark-math';
import rehypeKatex from 'rehype-katex';
import rehypeRaw from 'rehype-raw';
import rehypeSanitize, { defaultSchema } from 'rehype-sanitize';
import 'katex/dist/katex.min.css';
import {
  Divider,
  Table,
  TableBody,
  TableCell,
  TableContainer,
  TableHead,
  TableRow,
  Typography,
} from '@mui/material';

const ChatMarkdownRoot = styled('article', {
  name: 'MuiMarkdownRenderer',
  slot: 'Root',
})<{ isThinking?: boolean }>(({ theme, isThinking }) => {
  const { palette, spacing, typography } = theme;
  const preStyles = {
    '& pre': {
      overflowX: 'auto',
    },
  };

  return {
    ...typography.body1,
    overflowWrap: 'break-word',
    maxWidth: '100%',

    display: 'flex',
    flexFlow: 'column nowrap',
    justifyContent: 'flex-start',
    alignItems: 'stretch',
    gap: spacing(1),

    color: (isThinking && palette.text.disabled) || palette.text.primary,

    ...preStyles,

    '& think': {
      color: palette.text.disabled,
    },
    '& ul, & ol': {
      margin: 0,
    },
  };
});

interface Props {
  isThinking?: boolean;
  content: string;
}

const preprocessThink = (input: string) => {
  return input.replace(/<think>/, '<think>\n\n').replace(/<\/think>/, '\n\n</think>\n\n');
};

/**
 * Convert MathJax-style \(...\) and \[...\] into KaTeX-compatible $...$ and $$...$$
 */
const preprocessMath = (input: string) => {
  return input
    .replace(/\\\[/g, '$$')
    .replace(/\\\]/g, '$$')
    .replace(/\\\(/g, '$')
    .replace(/\\\)/g, '$');
};

const components: Components = {
  h1: (props) => <Typography {...props} variant='h1' />,
  h2: (props) => <Typography {...props} variant='h2' />,
  h3: (props) => <Typography {...props} variant='h3' />,
  h4: (props) => <Typography {...props} variant='h4' />,
  h5: (props) => <Typography {...props} variant='h5' />,
  h6: (props) => <Typography {...props} variant='h6' />,
  p: (props) => <Typography {...props} variant='body1' />,
  caption: (props) => <Typography {...props} variant='caption' />,
  hr: (props) => <Divider {...props} />,
  table: (props) => (
    <TableContainer>
      <Table {...props} />
    </TableContainer>
  ),
  thead: TableHead,
  tbody: TableBody,
  tr: TableRow,
  th: TableCell as any,
  td: TableCell as any,
};

const blockList = ['script', 'iframe', 'style', 'form', 'input', 'textarea'];
const schema = {
  ...defaultSchema,
  tagNames: (defaultSchema.tagNames || []).filter((tag) => !blockList.includes(tag)),
};

const ChatMarkdown = memo<Props>(({ isThinking, content }) => {
  content = preprocessThink(content);
  content = preprocessMath(content);

  return (
    <ChatMarkdownRoot className='ChatMarkdown' isThinking={isThinking}>
      <ReactMarkdown
        remarkPlugins={[remarkGfm, remarkMath]}
        rehypePlugins={[rehypeKatex, rehypeRaw, [remarkGfm, remarkMath, rehypeSanitize, schema]]}
        components={components}
      >
        {content}
      </ReactMarkdown>
    </ChatMarkdownRoot>
  );
});

export default ChatMarkdown;


================================================================================
File: src/frontend/src/components/inputs/chat-messages.tsx
Size: 7.98 kB
================================================================================

import { memo, useEffect, useRef, useState, type FC, type UIEventHandler } from 'react';
import { useChat, type ChatMessage } from '../../services';
import { Box, Button, IconButton, Paper, Stack, Tooltip, Typography } from '@mui/material';
import { IconArrowDown, IconCopy, IconCopyCheck, IconRefresh } from '@tabler/icons-react';
import { useRefCallback } from '../../hooks';
import ChatMarkdown from './chat-markdown';
import { DotPulse } from './dot-pulse';

export const ChatMessages: FC = () => {
  const [{ status, messages }] = useChat();

  const refContainer = useRef<HTMLDivElement>(null);
  // const refBottom = useRef<HTMLDivElement>(null);
  const [isBottom, setIsBottom] = useState(true);

  const userScrolledUpRef = useRef(false);
  const autoScrollingRef = useRef(false);
  const prevScrollTopRef = useRef(0);

  const scrollToBottom = useRefCallback(() => {
    const el = refContainer.current;
    if (!el) return;
    userScrolledUpRef.current = false;
    autoScrollingRef.current = true;
    requestAnimationFrame(() => {
      el.scrollTo({ top: el.scrollHeight, behavior: 'smooth' });
      // el.lastElementChild?.scrollIntoView({ behavior: 'smooth' });
    });
    setTimeout(() => {
      autoScrollingRef.current = false;
    }, 250);
  });

  useEffect(() => {
    if (userScrolledUpRef.current) return;
    autoScrollingRef.current = true;
    scrollToBottom();
    const t = setTimeout(() => {
      autoScrollingRef.current = false;
    }, 200);
    return () => clearTimeout(t);
  }, [messages]);

  const onScroll = useRefCallback<UIEventHandler<HTMLDivElement>>((event) => {
    event.stopPropagation();

    const container = refContainer.current;
    if (!container) return;

    const { scrollTop, scrollHeight, clientHeight } = container;
    const bottomGap = scrollHeight - scrollTop - clientHeight;

    setIsBottom(bottomGap < 10);

    if (!autoScrollingRef.current) {
      if (scrollTop < prevScrollTopRef.current - 2) {
        userScrolledUpRef.current = true;
      }
    }
    prevScrollTopRef.current = scrollTop;

    if (bottomGap < 10) {
      userScrolledUpRef.current = false;
    }
  });

  const nodeScrollToBottomButton = (
    <IconButton
      key='scroll-to-bottom'
      onClick={scrollToBottom}
      size='small'
      aria-label='Scroll to bottom'
      sx={{
        position: 'absolute',
        right: 12,
        bottom: 8,
        width: 28,
        height: 28,
        bgcolor: 'white',
        border: '1px solid',
        borderColor: 'grey.300',
        '&:hover': { bgcolor: 'grey.100' },
        opacity: isBottom ? 0 : 1,
        pointerEvents: isBottom ? 'none' : 'auto',
        transition: 'opacity .15s ease',
      }}
    >
      <IconArrowDown />
    </IconButton>
  );

  const nodeStream = (
    <Stack
      key='stream'
      ref={refContainer}
      sx={{
        width: '100%',
        height: '100%',

        overflowX: 'hidden',
        overflowY: 'scroll',
        '&::-webkit-scrollbar': { display: 'none' },
        scrollbarWidth: 'none',
        msOverflowStyle: 'none',

        display: 'flex',
        gap: 4,
      }}
      onScroll={onScroll}
      onWheel={(e) => {
        if (e.deltaY < 0) userScrolledUpRef.current = true;
      }}
      onTouchMove={() => {
        userScrolledUpRef.current = true;
      }}
    >
      {messages.map((message, idx) => (
        <ChatMessage key={message.id} message={message} isLast={idx === messages.length - 1} />
      ))}

      {status === 'opened' && <DotPulse size='large' />}

      {/* Last child for scroll to bottom */}
      <Box sx={{ width: '100%', height: 0 }} />
    </Stack>
  );

  return (
    <Box
      sx={{
        position: 'relative',
        flex: 1,
        overflow: 'hidden',
      }}
    >
      {nodeStream}
      {nodeScrollToBottomButton}
    </Box>
  );
};

const ChatMessage: FC<{ message: ChatMessage; isLast?: boolean }> = memo(({ message, isLast }) => {
  const { role, status: messageStatus, thinking, content } = message;

  const [, { generate }] = useChat();

  const [copied, setCopied] = useState(false);
  useEffect(() => {
    const timeoutId = setTimeout(() => setCopied(false), 2000);
    return () => clearTimeout(timeoutId);
  }, [copied]);

  const onCopy = useRefCallback(() => {
    navigator.clipboard.writeText(content);
    setCopied(true);
  });

  const onRegenerate = useRefCallback(() => {
    generate(message);
  });

  const justifyContent = role === 'user' ? 'flex-end' : 'flex-start';

  const nodeContent =
    role === 'user' ?
      <Typography
        key='user-message'
        variant='body1'
        sx={{
          px: 2,
          py: 1.5,
          borderRadius: '0.5rem',
          backgroundColor: 'background.default',
          fontSize: '0.875rem',
        }}
      >
        {content}
      </Typography>
    : <>
        {thinking && <ChatMarkdown key='assistant-thinking' isThinking content={thinking} />}
        {content && <ChatMarkdown key='assistant-message' content={content} />}
      </>;

  const assistantDone = messageStatus === 'done';
  const showCopy = role === 'user' || (role === 'assistant' && assistantDone);
  const showRegenerate = role === 'assistant' && assistantDone;

  const userHoverRevealSx =
    role === 'user' ?
      {
        '&:hover .actions-user': {
          opacity: 1,
          pointerEvents: 'auto',
        },
      }
    : {};

  return (
    <Stack direction='row' sx={{ width: '100%', justifyContent }}>
      <Stack
        sx={{
          maxWidth: role === 'user' ? { xs: '100%', md: '80%' } : '100%',
          alignSelf: role === 'user' ? 'flex-end' : 'flex-start',
          gap: 1,
          ...userHoverRevealSx,
        }}
      >
        {nodeContent}

        {(showCopy || showRegenerate) && (
          <Stack
            key='actions'
            direction='row'
            className={role === 'user' ? 'actions-user' : undefined}
            sx={{
              justifyContent,
              color: 'grey.600',
              gap: 0.5,
              ...(role === 'user' ?
                {
                  opacity: 0,
                  pointerEvents: 'none',
                  transition: 'opacity .15s ease',
                }
              : {}),
            }}
          >
            {showCopy && (
              <Tooltip
                key='copy'
                title={copied ? 'Copied!' : 'Copy'}
                slotProps={{
                  tooltip: { sx: { bgcolor: 'primary.main', borderRadius: 1 } },
                  popper: { modifiers: [{ name: 'offset', options: { offset: [0, -8] } }] },
                }}
              >
                <IconButton
                  onClick={onCopy}
                  size='small'
                  sx={{
                    width: 24,
                    height: 24,
                    borderRadius: '8px',
                    '&:hover': { bgcolor: 'action.hover' },
                  }}
                >
                  {copied ?
                    <IconCopyCheck />
                  : <IconCopy />}
                </IconButton>
              </Tooltip>
            )}

            {showRegenerate && (
              <Tooltip
                key='regenerate'
                title='Regenerate'
                slotProps={{
                  tooltip: { sx: { bgcolor: 'primary.main', borderRadius: 1 } },
                  popper: { modifiers: [{ name: 'offset', options: { offset: [0, -8] } }] },
                }}
              >
                <IconButton
                  onClick={onRegenerate}
                  size='small'
                  sx={{
                    width: 24,
                    height: 24,
                    borderRadius: '8px',
                    '&:hover': { bgcolor: 'action.hover' },
                  }}
                >
                  <IconRefresh />
                </IconButton>
              </Tooltip>
            )}
          </Stack>
        )}
      </Stack>
    </Stack>
  );
});


================================================================================
File: src/frontend/src/components/inputs/dot-pulse.tsx
Size: 1.67 kB
================================================================================

import { forwardRef, type FC, type HTMLAttributes } from 'react';
import type { Transition, Variants } from 'motion';
import * as motion from 'motion/react-client';
import { Box, styled } from '@mui/material';

const SIZE_MAP: Record<DotPulseSize, number> = {
  small: 1,
  medium: 1.25,
  large: 2.25,
};

const DotPulseRoot = styled(motion.div)<{ size: DotPulseSize }>(({ theme, size }) => {
  const length = `${SIZE_MAP[size]}rem`;

  return {
    position: 'relative',
    width: length,
    height: length,

    display: 'inline-flex',
    flexFlow: 'row nowrap',
    justifyContent: 'center',
    alignItems: 'center',
  };
});

const Dot = styled(motion.div)(({ theme }) => {
  return {
    flex: 1,
    aspectRatio: 1,
    borderRadius: '50%',
    backgroundColor: 'currentColor',
  };
});

const VARIANTS: Variants = {
  pulse: {
    scale: [0, 0.6, 0, 0],
    keyTimes: [0, 0.3, 0.6, 1],
    transition: {
      duration: 2,
      repeat: Infinity,
      ease: 'linear',
    },
  },
};

const TRANSITION: Transition = {
  staggerChildren: 0.25,
  staggerDirection: 1,
};

export type DotPulseSize = 'small' | 'medium' | 'large';

export interface DotPulseProps {
  /**
   * The size of the dot pulse.
   * @default 'medium'
   */
  size?: DotPulseSize;
}

export const DotPulse = forwardRef<HTMLDivElement, HTMLAttributes<HTMLDivElement> & DotPulseProps>(
  ({ size = 'medium' }, ref) => {
    return (
      <DotPulseRoot ref={ref} size={size} animate='pulse' transition={TRANSITION}>
        <Dot key={1} variants={VARIANTS} />
        <Dot key={2} variants={VARIANTS} />
        <Dot key={3} variants={VARIANTS} />
      </DotPulseRoot>
    );
  },
);


================================================================================
File: src/frontend/src/components/inputs/index.ts
Size: 190 B
================================================================================

export * from './number-input';

export * from './model-select';
export * from './join-command';
export * from './node-list';

export * from './chat-input';
export * from './chat-messages';


================================================================================
File: src/frontend/src/components/inputs/join-command.tsx
Size: 2.17 kB
================================================================================

import { useEffect, useState, type FC } from 'react';
import { IconButton, Paper, Stack, styled, Typography } from '@mui/material';
import { useCluster } from '../../services';
import { IconCopy, IconCopyCheck } from '@tabler/icons-react';
import { useRefCallback } from '../../hooks';

const LABEL_MAP: Record<string, string> = {
  'linux/mac': 'Linux/MacOS',
  windows: 'Windows',
};

const JoinCommandItem = styled('div')(({ theme }) => {
  const { palette, spacing } = theme;
  return {
    display: 'flex',
    flexFlow: 'row nowrap',
    justifyContent: 'space-between',
    alignItems: 'center',
    paddingInline: spacing(2),
    paddingBlock: spacing(1.5),
    gap: spacing(1),

    overflow: 'hidden',

    borderRadius: '0.7rem',
    backgroundColor: palette.background.area,
  };
});

export const JoinCommand: FC = () => {
  const [
    {
      clusterInfo: { nodeJoinCommand },
    },
  ] = useCluster();

  const [copiedKey, setCopiedKey] = useState<string>();

  useEffect(() => {
    if (copiedKey) {
      const timeoutId = setTimeout(() => {
        setCopiedKey(undefined);
      }, 2000);
      return () => clearTimeout(timeoutId);
    }
  }, [copiedKey]);

  const copy = useRefCallback(async (key: string) => {
    await navigator.clipboard.writeText(nodeJoinCommand[key]);
    setCopiedKey(key);
  });

  return (
    <Stack gap={1}>
      {Object.entries(nodeJoinCommand).map(([key, value], index, entries) => (
        <Stack key={key} gap={1}>
          {entries.length > 1 && (
            <Typography key='label' variant='subtitle2'>
              For {LABEL_MAP[key] || key}:
            </Typography>
          )}
          <JoinCommandItem key='command'>
            <Typography sx={{ flex: 1, lineHeight: '1.125rem', whiteSpace: 'wrap' }} variant='pre'>
              {value}
            </Typography>
            <IconButton
              sx={{ flex: 'none', fontSize: '1rem' }}
              size='em'
              onClick={() => copy(key)}
            >
              {(copiedKey === key && <IconCopyCheck />) || <IconCopy />}
            </IconButton>
          </JoinCommandItem>
        </Stack>
      ))}
    </Stack>
  );
};


================================================================================
File: src/frontend/src/components/inputs/model-select.tsx
Size: 6.03 kB
================================================================================

import { useEffect, useState, type FC, type ReactNode } from 'react';
import * as motion from 'motion/react-client';
import {
  InputBase,
  MenuItem,
  OutlinedInput,
  Select,
  selectClasses,
  Stack,
  styled,
  Typography,
} from '@mui/material';
import { useCluster, useHost, type ModelInfo } from '../../services';
import { useRefCallback } from '../../hooks';
import { useAlertDialog } from '../mui';
import { IconCheck, IconLoader, IconRestore } from '@tabler/icons-react';

const ModelSelectRoot = styled(Select)<{ ownerState: ModelSelectProps }>(({
  theme,
  ownerState,
}) => {
  const { spacing, typography, palette } = theme;
  const { variant = 'outlined' } = ownerState;

  return {
    height: variant === 'outlined' ? '4rem' : '1lh',
    paddingInline: spacing(0.5),
    borderRadius: 12,
    '&:hover': {
      backgroundColor: 'action.hover',
    },

    [`.${selectClasses.select}:hover`]: {
      backgroundColor: 'transparent',
    },

    ...(variant === 'text' && {
      ...typography.h3,
      fontWeight: typography.fontWeightMedium,
      [`.${selectClasses.select}`]: {
        fontSize: 'inherit',
        fontWeight: 'inherit',
        lineHeight: 'inherit',
        padding: 0,
      },
      '&:hover': { backgroundColor: 'transparent' },
    }),
  };
});

const ModelSelectOption = styled(MenuItem)(({ theme }) => ({
  height: '3.25rem',
  gap: theme.spacing(1),
  borderRadius: 10,
}));

const ValueRow = styled(Stack)(({ theme }) => ({
  flexDirection: 'row',
  alignItems: 'center',
  gap: theme.spacing(1),
  padding: theme.spacing(1),
  '&:hover': { backgroundColor: 'transparent' },
  pointerEvents: 'none',
}));

const ModelExtraStatus = styled(motion.div)(({ theme }) => ({
  width: '1rem',
  height: '1rem',
  '& > .tabler-icon': {
    width: '1rem',
    height: '1rem',
  },
}));

const ModelLogo = styled('img')(({ theme }) => ({
  width: '2.25rem',
  height: '2.25rem',
  borderRadius: '0.5rem',
  border: `1px solid ${theme.palette.divider}`,
  objectFit: 'cover',
}));

const ModelDisplayName = styled('span')(({ theme }) => ({
  ...theme.typography.subtitle2,
  fontSize: '0.875rem',
  lineHeight: '1.125rem',
  fontWeight: theme.typography.fontWeightLight,
  color: theme.palette.text.primary,
}));

const ModelName = styled('span')(({ theme }) => ({
  ...theme.typography.body2,
  fontSize: '0.75rem',
  lineHeight: '1rem',
  fontWeight: theme.typography.fontWeightLight,
  color: theme.palette.text.secondary,
}));

const renderOption = (
  { name, displayName, logoUrl }: ModelInfo,
  { selected, loading, disabled }: { selected?: boolean; loading?: boolean; disabled?: boolean },
): ReactNode => (
  <ModelSelectOption key={name} value={name}>
    <ModelExtraStatus
      {...(loading && {
        animate: { rotate: 360 },
        transition: {
          repeat: Infinity,
          ease: 'linear',
          duration: 2,
        },
      })}
    >
      {(loading && <IconLoader />) || (selected && <IconCheck />)}
    </ModelExtraStatus>
    <ModelLogo src={logoUrl} />
    <Stack gap={0.25}>
      <ModelDisplayName>{displayName}</ModelDisplayName>
      <ModelName>{name}</ModelName>
    </Stack>
  </ModelSelectOption>
);

export interface ModelSelectProps {
  variant?: 'outlined' | 'text';
  autoCommit?: boolean;
}

export const ModelSelect: FC<ModelSelectProps> = ({ variant = 'outlined', autoCommit = false }) => {
  const [{ type: hostType }] = useHost();
  const [
    {
      config: { modelName: configModelName, modelInfoList },
      clusterInfo: { status: clusterStatus, modelName: clusterModelName },
    },
    {
      config: { setModelName },
      init,
    },
  ] = useCluster();

  // const [nodeDialog, { open: openDialog }] = useAlertDialog({
  //   titleIcon: <IconRestore />,
  //   title: 'Switch model',
  //   content: (
  //     <Typography variant='body2' color='text.secondary'>
  //       The current version of parallax only supports hosting one model at once. Switching the model
  //       will terminate your existing chat service. You can restart the current scheduler in your
  //       terminal. We will add node rebalancing and dynamic model allocation soon.
  //     </Typography>
  //   ),
  //   confirmLabel: 'Continue',
  // });

  const onChange = useRefCallback((e) => {
    // if (clusterStatus !== 'idle') {
    //   openDialog();
    //   return;
    // }
    setModelName(String(e.target.value));
  });

  const [canAutoCommit, setCanAutoCommit] = useState(false);
  useEffect(() => {
    if (autoCommit) {
      setCanAutoCommit(autoCommit);
    }
  }, [autoCommit]);
  useEffect(() => {
    if (canAutoCommit && configModelName !== clusterModelName) {
      init();
    }
  }, [canAutoCommit, configModelName]);

  return (
    <>
      <ModelSelectRoot
        ownerState={{ variant }}
        readOnly={hostType === 'node'}
        input={variant === 'outlined' ? <OutlinedInput /> : <InputBase />}
        value={configModelName}
        onChange={onChange}
        renderValue={(value: unknown) => {
          const model = modelInfoList.find((m) => m.name === value);
          if (!model) return value as string;

          return variant === 'outlined' ?
              <ValueRow>
                <ModelLogo src={model.logoUrl} />
                <Stack gap={0.25}>
                  <ModelDisplayName>{model.displayName}</ModelDisplayName>
                  <ModelName>{model.name}</ModelName>
                </Stack>
              </ValueRow>
            : model.name;
        }}
        IconComponent={hostType === 'node' ? () => null : undefined}
      >
        {modelInfoList.map((model) => {
          const { name } = model;
          const selected = name === configModelName || name === clusterModelName;
          const loading =
            clusterStatus !== 'idle'
            && name === configModelName
            && configModelName !== clusterModelName;
          return renderOption(model, { selected, loading });
        })}
      </ModelSelectRoot>

      {/* {nodeDialog} */}
    </>
  );
};


================================================================================
File: src/frontend/src/components/inputs/node-list.tsx
Size: 7.44 kB
================================================================================

import type { FC, ForwardRefExoticComponent, RefAttributes } from 'react';
import * as motion from 'motion/react-client';
import {
  IconCheck,
  IconCircleFilled,
  IconDevices2,
  IconLoader,
  IconX,
  type Icon,
  type IconProps,
} from '@tabler/icons-react';
import {
  Alert,
  List as MuiList,
  ListItem as MuiListItem,
  ListItemIcon as MuiListItemIcon,
  ListItemText as MuiListItemText,
  MenuList,
  Paper,
  Skeleton,
  styled,
  Typography,
  useTheme,
  Stack,
  Box,
  Divider,
  type ListProps,
  type StackProps,
} from '@mui/material';
import { useChat, useCluster, type NodeInfo, type NodeStatus } from '../../services';

const NodeListRoot = styled(Stack)(({ theme }) => {
  const { spacing } = theme;
  return {
    position: 'relative',
    flex: 1,
    gap: spacing(1.5),
    overflowX: 'hidden',
    overflowY: 'auto',
  };
});

const List = styled(MuiList)<{ variant: NodeListVariant }>(({ theme, variant }) => {
  const { spacing } = theme;
  return {
    // menu no need gap, use dash line to separate nodes
    gap: spacing(variant === 'list' ? 1.5 : 0),
  };
});

const ListItem = styled(MuiListItem)(({ theme }) => {
  const { spacing } = theme;
  return {
    flex: 'none',
    gap: spacing(1),
    backgroundColor: 'transparent',
    padding: spacing(2),
    overflow: 'visible',
  };
}) as typeof MuiListItem;

const ListItemIcon = styled(MuiListItemIcon)(({ theme }) => {
  return {
    color: 'inherit',
    fontSize: '1.5rem',
    width: '1em',
    height: '1em',
    display: 'inline-flex',
    alignItems: 'center',
    justifyContent: 'center',
  };
}) as typeof MuiListItemIcon;

const ListItemText = styled(MuiListItemText)(({ theme }) => {
  return {
    position: 'relative',
    display: 'block',
    height: '100%',
  };
}) as typeof MuiListItemText;

const ListItemStatus = styled(motion.div)<{ variant: NodeListVariant }>(({ theme, variant }) => {
  return {
    fontSize: variant === 'list' ? '1.5rem' : '1em',
    width: '1em',
    height: '1em',
    display: 'inline-flex',
    alignItems: 'center',
    justifyContent: 'center',
    transformOrigin: 'center',
  };
});

const STATUS_COLOR_MAP: Record<NodeStatus, 'info' | 'success' | 'error'> = {
  waiting: 'info',
  available: 'success',
  failed: 'error',
};

const STATUS_ICON_MAP: Record<
  NodeStatus,
  ForwardRefExoticComponent<IconProps & RefAttributes<Icon>>
> = {
  waiting: IconLoader,
  available: IconCheck,
  failed: IconX,
};

const DashRoot = styled(Box)(({ theme }) => {
  const { spacing } = theme;
  return {
    position: 'relative',
    width: '1.5rem',
    height: '3.25rem', // For dash array last position, must to be minus 0.25rem(4px)
    overflow: 'hidden',
  };
});

const Dash: FC<{ animate?: boolean }> = ({ animate }) => {
  const width = 2;
  const height = 256;
  return (
    <DashRoot>
      <svg
        style={{ position: 'absolute', top: 0, left: '50%', transform: 'translateX(-50%)' }}
        width={width}
        height={height}
        viewBox={`0 0 2 ${height}`}
        fill='none'
      >
        <line x1='1' y1='0' x2='1' y2={height} stroke='#9B9B9B' strokeWidth='2' strokeDasharray='4'>
          {animate && (
            <animate
              attributeName='stroke-dashoffset'
              from={height}
              to='0'
              dur={`${height / 32}s`}
              repeatCount='indefinite'
            ></animate>
          )}
        </line>
      </svg>
    </DashRoot>
  );
};

const Node: FC<{ variant: NodeListVariant; node?: NodeInfo }> = ({ variant, node }) => {
  const { id, status, gpuNumber, gpuName, gpuMemory } = node || { status: 'waiting' };
  const { palette } = useTheme();
  const { main, lighter } =
    status === 'waiting' ?
      { main: palette.grey[800], lighter: palette.grey[250] }
    : palette[STATUS_COLOR_MAP[status]];
  const opacity = status === 'failed' ? 0.2 : undefined;

  const IconStatus = STATUS_ICON_MAP[status];

  return (
    <ListItem
      component={variant === 'list' ? Paper : Box}
      variant='outlined'
      sx={{
        opacity,
        padding: variant === 'menu' ? 0 : undefined,
        height: variant === 'menu' ? '2.5rem' : undefined,
      }}
    >
      <ListItemIcon>
        <IconDevices2 />
      </ListItemIcon>

      <ListItemText>
        <Stack
          sx={
            variant === 'menu' ?
              {
                position: 'absolute',
                top: '50%',
                left: 0,
                right: 0,
                transform: 'translateY(-50%)',
              }
            : undefined
          }
        >
          {(node && (
            <>
              <Typography variant='body1' sx={{ fontWeight: 500 }}>
                {[
                  (gpuNumber && gpuNumber > 1 && `${gpuNumber}x`) || '',
                  gpuName,
                  (gpuMemory && `${gpuMemory}GB`) || '',
                ]
                  .filter(Boolean)
                  .join(' ')}
              </Typography>
              {/* <Typography variant='caption' color='text.disabled'>
                Rancho Cordova, United States
              </Typography> */}
            </>
          )) || (
            <>
              <Skeleton height='1lh' />
            </>
          )}
        </Stack>
      </ListItemText>

      {node && (
        <ListItemStatus
          sx={{ color: main }}
          {...(status === 'waiting' && {
            animate: { rotate: 360 },
            transition: {
              repeat: Infinity,
              ease: 'linear',
              duration: 2,
            },
          })}
          variant={variant}
        >
          {variant === 'list' && <IconStatus size={18} />}
          {variant === 'menu' && <IconCircleFilled size={10} />}
        </ListItemStatus>
      )}
    </ListItem>
  );
};

export type NodeListVariant = 'list' | 'menu';

export interface NodeListProps {
  variant?: NodeListVariant;
}

export const NodeList: FC<NodeListProps & StackProps> = ({ variant = 'list', ...rest }) => {
  const [
    {
      clusterInfo: { status: clusterStatus, initNodesNumber },
      nodeInfoList,
    },
  ] = useCluster();
  const [{ status: chatStatus }] = useChat();

  const { length: nodesNumber } = nodeInfoList;
  // const nodesNumber = 0;

  const generating = chatStatus === 'generating';
  let dashIndex = 0;
  const renderDash =
    variant === 'menu' ?
      (key: string) => {
        return dashIndex++ > 0 && <Dash key={key} animate={generating} />;
      }
    : () => undefined;

  return (
    <NodeListRoot {...rest}>
      <List variant={variant}>
        {nodeInfoList.map((node, index) => [
          renderDash(`${node.id}-dash`),
          <Node key={node.id} variant={variant} node={node} />,

          // renderDash(`${node.id}-dash-mock-0`),
          // <Node key={`${node.id}-mock-0`} variant={variant} node={node} />,

          // renderDash(`${node.id}-dash-mock-1`),
          // <Node key={`${node.id}-mock-1`} variant={variant} node={node} />,

          // renderDash(`${node.id}-dash-mock-2`),
          // <Node key={`${node.id}-mock-2`} variant={variant} node={node} />,
        ])}

        {clusterStatus !== 'idle'
          && initNodesNumber > nodesNumber
          && Array.from({ length: initNodesNumber - nodesNumber }).map((_, index) => [
            renderDash(`${index}-dash`),
            <Node key={index} variant={variant} />,
          ])}
      </List>
    </NodeListRoot>
  );
};


================================================================================
File: src/frontend/src/components/inputs/number-input.tsx
Size: 3.2 kB
================================================================================

import type { ChangeEventHandler, FC, Ref } from 'react';
import { forwardRef, useRef } from 'react';
import {
  IconButton,
  InputAdornment,
  InputBase,
  FilledInput,
  OutlinedInput,
  useForkRef,
  type OutlinedInputProps,
} from '@mui/material';
import { IconMinus, IconPlus } from '@tabler/icons-react';
import { useRefCallback } from '../../hooks';

export interface NumberInputProps extends OutlinedInputProps {
  value: number;
}

export const NumberInput: FC<NumberInputProps> = ({
  inputRef: inputRefProp,
  slotProps,
  onChange: onChangeProp,
  ...rest
}) => {
  const inputRefInner = useRef<HTMLInputElement>(null);
  const inputRef = useForkRef(inputRefInner, inputRefProp);

  const onChange = useRefCallback<ChangeEventHandler<HTMLTextAreaElement | HTMLInputElement>>(
    (event) => {
      event.target.value = `${Math.max(1, parseInt(event.target.value) || 1)}`;
      onChangeProp?.(event);
    },
  );

  const triggerChange = useRefCallback((newValue: number) => {
    if (onChangeProp) {
      // Create a synthetic event that mimics a real input change
      const syntheticEvent = {
        target: { value: newValue.toString() },
        currentTarget: { value: newValue.toString() },
        type: 'change',
        bubbles: true,
        cancelable: true,
        preventDefault: () => {},
        stopPropagation: () => {},
      } as React.ChangeEvent<HTMLInputElement>;

      onChangeProp(syntheticEvent);
    }
  });

  const onMinus = useRefCallback(() => {
    const { current: input } = inputRefInner;
    if (!input) {
      return;
    }
    const prev = Number(input.value);
    const next = Math.max(1, prev - 1);
    input.value = next.toString();
    triggerChange(next);
  });

  const onPlus = useRefCallback(() => {
    const { current: input } = inputRefInner;
    if (!input) {
      return;
    }
    const prev = Number(input.value);
    const next = Math.max(1, prev + 1);
    input.value = next.toString();
    triggerChange(next);
  });

  return (
    <OutlinedInput
      {...rest}
      type='number'
      sx={{
        '& input': {
          textAlign: 'center',
          width: '2.5rem',
        },
      }}
      // startAdornment={
      //   <InputAdornment position='start'>
      //     <IconButton>
      //       <IconMinus />
      //     </IconButton>
      //   </InputAdornment>
      // }
      // endAdornment={
      //   <InputAdornment position='end'>
      //     <IconButton>
      //       <IconPlus />
      //     </IconButton>
      //   </InputAdornment>
      // }
      inputRef={inputRef}
      onChange={onChange}
      startAdornment={
        <InputAdornment position='start'>
          <IconButton onClick={onMinus}>
            <IconMinus />
          </IconButton>
        </InputAdornment>
      }
      endAdornment={
        <InputAdornment position='end'>
          <IconButton onClick={onPlus}>
            <IconPlus />
          </IconButton>
        </InputAdornment>
      }
      slotProps={{
        input: {
          step: 1,
          sx: { textAlign: 'center', ...slotProps?.input?.sx },
          min: 1,
          ...slotProps?.input,
        },
        ...slotProps,
      }}
    />
  );
};


================================================================================
File: src/frontend/src/components/mui/alert/index.ts
Size: 32 B
================================================================================

export * from './notification';


================================================================================
File: src/frontend/src/components/mui/alert/notification.tsx
Size: 3.79 kB
================================================================================

import type { EventHandler, FC, ReactNode, SyntheticEvent } from 'react';
import { useEffect, useState } from 'react';
import { forwardRef } from 'react';
import type { AlertProps } from '@mui/material';
import { Alert, AlertTitle, Button, LinearProgress } from '@mui/material';
import { styled } from '@mui/material';

const NotificationActions = styled('div')(({ theme }) => {
  const { palette } = theme;
  return {
    display: 'flex',
    flexFlow: 'row nowrap',
    alignItems: 'center',
    justifyContent: 'flex-start',
    gap: '0.75rem',
    paddingBlockStart: '0.75rem',
    color: palette.grey[700],
  };
});

const NotificationCountdown = styled('div')(({ theme }) => {
  const { palette } = theme;
  return {
    position: 'absolute',
    bottom: 0,
    left: '-1rem',
    right: '-1rem',
  };
});

const Compensation = 150;

const Countdown: FC<NotificationProps> = (props) => {
  const { severity, autoHideDuration = 0 } = props;
  const [progress, setProgress] = useState(0);

  useEffect(() => {
    if (autoHideDuration <= 0) {
      return;
    }

    const step = autoHideDuration / 100;
    let frameId: number | undefined;

    let startTime: number;
    const animate = () =>
      (frameId = requestAnimationFrame((currentTime) => {
        if (!startTime) {
          startTime = currentTime;
        }

        const progress = Math.min(100, Math.ceil((currentTime + Compensation - startTime) / step));
        setProgress(progress);

        if (progress < 100) {
          animate();
        }
      }));
    animate();

    return () => {
      if (frameId) {
        cancelAnimationFrame(frameId);
      }
    };
  }, [autoHideDuration]);

  return (
    <NotificationCountdown>
      <LinearProgress variant='determinate' color={severity} value={progress} />
    </NotificationCountdown>
  );
};

export interface NotificationProps extends Omit<AlertProps, 'title'> {
  /**
   * The title of the notification.
   */
  title?: ReactNode;

  /**
   * @default 'Dismiss'
   */
  dismissLabel?: ReactNode;

  /**
   * If provided, a dismiss button will be displayed in the bottom.
   */
  onDismiss?: EventHandler<SyntheticEvent>;

  /**
   * Display a progress bar to countdown the `autoHideDuration`.
   */
  autoHideDuration?: number;
}

export const Notification = forwardRef<HTMLDivElement, NotificationProps>((props, ref) => {
  const {
    autoHideDuration,
    variant,
    title: titleProp,
    children: childrenProp,
    onClose,
    action,
    dismissLabel = 'Dismiss',
    onDismiss,
    ...rest
  } = props;

  let title = titleProp;
  let children = childrenProp;

  if (!title && children) {
    title = children;
    children = undefined;
  }

  // onClose = onClose || onDismiss;

  const nodeDismiss = onDismiss && (
    <Button variant='text' color='inherit' onClick={onDismiss}>
      {dismissLabel}
    </Button>
  );

  const nodeActions =
    ((!!action || !!nodeDismiss) && (
      <>
        {nodeDismiss}
        {action}
      </>
    ))
    || undefined;

  // if (!variant) {
  //   if (
  //     (title && children)
  //     || ((nodeDismiss || action) && onClose)
  //     || (autoHideDuration && autoHideDuration > 0)
  //   ) {
  //     variant = 'notification';
  //   }
  // }

  if (variant === 'notification') {
    return (
      <Alert {...rest} {...{ variant, onClose }} ref={ref}>
        {!!title && <AlertTitle>{title}</AlertTitle>}
        {children}
        {nodeActions && <NotificationActions>{nodeActions}</NotificationActions>}
        {autoHideDuration && autoHideDuration > 0 && <Countdown {...props} />}
      </Alert>
    );
  }

  return (
    <Alert {...rest} {...{ variant, onClose, action: nodeActions }} ref={ref}>
      {!!title && <AlertTitle>{title}</AlertTitle>}
      {children}
    </Alert>
  );
});


================================================================================
File: src/frontend/src/components/mui/dialog/alert-dialog.tsx
Size: 10.2 kB
================================================================================

/* eslint-disable @typescript-eslint/no-explicit-any */
'use client';

import type { DOMAttributes, FC, ForwardRefExoticComponent, ReactNode, RefAttributes } from 'react';
import { useState } from 'react';

import {
  Button,
  type ButtonProps,
  Dialog,
  DialogActions,
  DialogContent,
  DialogContentText,
  type DialogProps,
  DialogTitle,
  IconButton,
  Typography,
  Box,
} from '@mui/material';
import {
  type Icon,
  IconAlertCircle,
  IconCircleCheck,
  IconInfoCircle,
  type IconProps,
  IconX,
} from '@tabler/icons-react';
import type { SubmitErrorHandler, SubmitHandler, UseFormHandleSubmit } from 'react-hook-form';
import { TitleIcon, TitleIconForm } from '../title-icon';

const COLOR_ICON_MAP: Record<
  NonNullable<AlertDialogProps['color']>,
  ForwardRefExoticComponent<IconProps & RefAttributes<Icon>>
> = {
  primary: IconInfoCircle,
  secondary: IconInfoCircle,
  info: IconInfoCircle,
  error: IconAlertCircle,
  warning: IconAlertCircle,
  success: IconCircleCheck,
};

export type AlertDialogControl = boolean | { closeDialog?: boolean };

export type AlertDialogColor = 'primary' | 'secondary' | 'error' | 'info' | 'success' | 'warning';

export interface AlertDialogProps extends Omit<DialogProps, 'onClose' | 'title' | 'content'> {
  /**
   * Whether the dialog is open.
   */
  open: boolean;

  /**
   * The callback function to be called when the dialog is closed.
   */
  onClose?: () => void;

  /**
   * The id of the title element that label the dialog.
   */
  titleId?: string;

  /**
   * The title of the dialog.
   */
  title: ReactNode;

  /**
   * The id of the content element that describe the dialog.
   */
  contentId?: string;

  /**
   * The content of the dialog.
   */
  content: ReactNode;

  /**
   * The color for title icon and confirm button.
   */
  color?: AlertDialogColor;

  /**
   * The color for title icon.
   * @default 'warning'
   */
  titleIconColor?: AlertDialogColor;

  /**
   * Whether to show the title icon.
   * If 'form', the icon will be a special form icon.
   */
  titleIcon?: boolean | 'form' | ReactNode;

  /**
   * The content to be rendered in the cancel button.
   */
  cancelLabel?: ReactNode;

  /**
   * The callback function to be called when the cancel button is clicked.
   * If the function returns a promise, the dialog will be loading status
   * and wait for the promise to be resolved before closing the dialog.
   * Return `false` or `{ closeDialog: false }` to prevent dialog from closing.
   */
  onCancel?: () => void | Promise<void> | AlertDialogControl | Promise<AlertDialogControl>;

  /**
   * The content to be rendered in the confirm button.
   */
  confirmLabel?: ReactNode;

  /**
   * The callback function to be called when the confirm button is clicked.
   * If the function returns a promise, the dialog will be loading status
   * and wait for the promise to be resolved before closing the dialog.
   * Return `false` or `{ closeDialog: false }` to prevent dialog from closing.
   */
  onConfirm?: () => void | Promise<void> | AlertDialogControl | Promise<AlertDialogControl>;

  /**
   * The content to be rendered in the submit button.
   * If `submitLabel` is provided, the dialog will be a form dialog.
   */
  submitLabel?: ReactNode;

  /**
   * The callback function to be called when the form is submitted and the form is valid.
   * Return `false` or `{ closeDialog: false }` to prevent dialog from closing.
   */
  onSubmitValid?: SubmitHandler<any>;

  /**
   * The callback function to be called when the form is submitted and the form is invalid.
   * The dialog will not close when the form invalid.
   */
  onSubmitInvalid?: SubmitErrorHandler<any>;

  /**
   * Come from `react-hook-form`.
   * If `submitLabel` is provided, the dialog will be a form dialog.
   * The `handleSubmit` function will be used to handle the form submission,
   * it accepts two functions: `onSubmitValid` and `onSubmitInvalid`,
   * and return a `submit` event handler function to be used in the element `<form>`.
   * The `onSubmitValid` function will be called when the form is submitted and the form is valid.
   * The `onSubmitInvalid` function will be called when the form is submitted and the form is invalid.
   *
   * @example
   * ```tsx
   * import { useForm } from 'react-hook-form';
   *
   * const { handleSubmit } = useForm();
   *
   * const onSubmit = (data: any) => {
   *   console.log(data);
   * };
   *
   * <AlertDialog
   *  handleSubmit={handleSubmit}
   *  onSubmitValid={onSubmit}
   *  onSubmitInvalid={onSubmitInvalid}
   * />
   * ```
   */
  handleSubmit?: UseFormHandleSubmit<any>;

  secondaryAction?: ReactNode;
}

export const AlertDialog: FC<AlertDialogProps> = (props) => {
  const {
    open,
    onClose = () => {},

    titleId,
    title,
    contentId,
    content,

    color = 'primary',
    titleIconColor = 'warning',
    titleIcon = true,

    cancelLabel,
    onCancel = () => {},

    confirmLabel,
    onConfirm = () => {},

    submitLabel,
    onSubmitValid,
    onSubmitInvalid,
    handleSubmit,
    secondaryAction,

    ...rest
  } = props;

  if (import.meta.env.DEV) {
    if (submitLabel && (!handleSubmit || !onSubmitValid)) {
      throw new Error('handleSubmit and onSubmitValid are required when submitLabel is provided');
    }
  }

  const SlotIcon = COLOR_ICON_MAP[color];

  const [loading, setLoading] = useState(false);

  // The function to wrap the action callbacks
  const handleActionResult = (result: any): boolean => {
    // If return `false` or `{ closeDialog: false }`, do not close the dialog
    if (result === false || (typeof result === 'object' && result.closeDialog === false)) {
      return false;
    }
    // If return `true`, close the dialog
    return true;
  };

  const pipeHandlers =
    <F extends (...args: any[]) => any>(handler: F) =>
    async (...args: Parameters<F>): Promise<void> => {
      if (loading) {
        return;
      }

      try {
        const result = handler(...args);

        if (result instanceof Promise) {
          setLoading(true); // Dialog switch to loading status.
          const resolvedResult = await result;
          setLoading(false);

          if (handleActionResult(resolvedResult)) {
            onClose();
          }
        } else {
          if (handleActionResult(result)) {
            onClose();
          }
        }
      } catch (error) {
        setLoading(false);
        // Do not close the dialog when error
        console.error('Error in dialog action:', error);
      }
    };

  let formProps: DOMAttributes<HTMLFormElement> | undefined;
  const actionButtonPropsList: ButtonProps[] = [];

  if (cancelLabel) {
    actionButtonPropsList.push({
      key: 'cancel',
      children: cancelLabel,
      color: 'secondary',
      onClick: pipeHandlers(onCancel),
      loading,
    });
  }

  if (confirmLabel) {
    actionButtonPropsList.push({
      key: 'confirm',
      children: confirmLabel,
      color,
      onClick: pipeHandlers(onConfirm),
      loading,
    });
  }

  if (submitLabel) {
    actionButtonPropsList.push({
      key: 'submit',
      children: submitLabel,
      type: 'submit',
      loading,
    });

    if (handleSubmit && onSubmitValid) {
      formProps = {
        onSubmit: (e) => {
          // Prevent the page from being refreshed when the form is submitted.
          e.preventDefault();
          e.stopPropagation();

          if (loading) {
            return;
          }

          setLoading(true);
          handleSubmit(
            async (...args) => {
              try {
                const result = await onSubmitValid(...args);
                setLoading(false);

                // å¤„ç†æäº¤ç»“æžœï¼Œå†³å®šæ˜¯å¦å…³é—­
                if (handleActionResult(result)) {
                  onClose();
                }
              } catch (error) {
                setLoading(false);
                // The dialog will not close when error
                console.error('Error in form submission:', error);
              }
            },
            async (...args) => {
              try {
                // The dialog will not close when the form is invalid.
                await onSubmitInvalid?.(...args);
              } catch (error) {
                console.error('Error in form validation:', error);
              } finally {
                setLoading(false);
              }
            },
          )();
        },
      };
    }
  }

  const safeOnClose = () => {
    if (loading) {
      return;
    }
    onClose();
  };

  return (
    <Dialog
      open={open}
      onClose={safeOnClose}
      aria-labelledby={titleId}
      aria-describedby={contentId}
      {...rest}
      component={(formProps && 'form') || undefined}
      {...(formProps as any)}
      slotProps={{
      paper: {
        sx: {
          p: 1.5,
        },
      },
    }}
    >
      <DialogTitle id={(!SlotIcon && titleId) || undefined}>
        {(titleIcon === 'form' && <TitleIconForm />)
          || (titleIcon === true && (
            <TitleIcon variant='circle' color={titleIconColor}>
              <SlotIcon />
            </TitleIcon>
          )) || (
            <TitleIcon variant='circle' color={titleIconColor}>
              {titleIcon}
            </TitleIcon>
          )
          || undefined}

        {!titleIcon && title}

        <IconButton size='small' onClick={safeOnClose}>
          <IconX size='1.25rem' />
        </IconButton>
      </DialogTitle>

      <DialogContent>
        {titleIcon && (
          <Typography variant='subtitle1' id={titleId} sx={{ fontSize: '1.125rem', fontWeight: 600, mr: 'auto' }}>
            {title}
          </Typography>
        )}

        <DialogContentText id={contentId}>{content}</DialogContentText>
      </DialogContent>

      {actionButtonPropsList.length > 0 && (
        <DialogActions>
          {secondaryAction}
          {actionButtonPropsList.map((props) => (
            <Button
              {...props}
              key={props.key}
              {...(actionButtonPropsList.length === 1 && { fullWidth: true })}
            />
          ))}
        </DialogActions>
      )}
    </Dialog>
  );
};


================================================================================
File: src/frontend/src/components/mui/dialog/index.ts
Size: 62 B
================================================================================

export * from './alert-dialog';
export * from './use-dialog';


================================================================================
File: src/frontend/src/components/mui/dialog/use-dialog.tsx
Size: 1.28 kB
================================================================================

'use client';

import type { JSX } from 'react';
import { useMemo, useState } from 'react';
import { AlertDialog, type AlertDialogProps } from './alert-dialog';

export interface AlertDialogActions {
  open: () => void;
  close: () => void;
}

// eslint-disable-next-line @typescript-eslint/no-empty-object-type
export interface AlertDialogOptions {}

/**
 * A hook to manage the `open` state of `AlertDialog` and forward the render node.
 * @see AlertDialog
 * @example
 * ```tsx
 * const [node, actions] = useAlertDialog({
 *   title: 'Title',
 *   content: 'Content',
 *   cancelLabel: 'OK'
 * });
 *
 * <div>
 *   <Button onClick={actions.open}>Open</Button>
 *   <Button onClick={actions.close}>Close</Button>
 *   {node}
 * </div>
 * ```
 */
export const useAlertDialog = (
  props: Omit<AlertDialogProps, 'open'>,
  options?: AlertDialogOptions,
): [JSX.Element, AlertDialogActions] => {
  const [open, setOpen] = useState(false);

  const node = (
    <AlertDialog
      {...props}
      open={open}
      onClose={() => {
        setOpen(false);
        props.onClose?.();
      }}
    />
  );

  const actions: AlertDialogActions = useMemo(
    () => ({
      open: () => setOpen(true),
      close: () => setOpen(false),
    }),
    [],
  );

  return [node, actions];
};


================================================================================
File: src/frontend/src/components/mui/index.ts
Size: 81 B
================================================================================

export * from './title-icon';
export * from './alert';
export * from './dialog';


================================================================================
File: src/frontend/src/components/mui/title-icon/index.ts
Size: 139 B
================================================================================

export * from './title-icon-classes';
export * from './title-icon-props';
export * from './title-icon';
export * from './title-icon-form';


================================================================================
File: src/frontend/src/components/mui/title-icon/title-icon-classes.ts
Size: 1.54 kB
================================================================================

import { generateUtilityClass, generateUtilityClasses } from '@mui/material';

export interface TitleIconClasses {
  /** Styles applied to the root element. */
  root: string;
  /** Styles applied to the root element if `color="inherit"`. */
  colorInherit: string;
  /** Styles applied to the root element if `color="primary"`. */
  colorPrimary: string;
  /** Styles applied to the root element if `color="secondary"`. */
  colorSecondary: string;
  /** Styles applied to the root element if `color="error"`. */
  colorError: string;
  /** Styles applied to the root element if `color="info"`. */
  colorInfo: string;
  /** Styles applied to the root element if `color="success"`. */
  colorSuccess: string;
  /** Styles applied to the root element if `color="warning"`. */
  colorWarning: string;
  // /** Styles applied to the root element if `size="small"`. */
  // sizeSmall: string;
  // /** Styles applied to the root element if `size="medium"`. */
  // sizeMedium: string;
  // /** Styles applied to the root element if `size="large"`. */
  // sizeLarge: string;
}

export type TitleIconClassKey = keyof TitleIconClasses;

export function getTitleIconUtilityClass(slot: string): string {
  return generateUtilityClass('MuiTitleIcon', slot);
}

export const titleIconClasses: TitleIconClasses = generateUtilityClasses('MuiTitleIcon', [
  'root',
  'colorInherit',
  'colorPrimary',
  'colorSecondary',
  'colorError',
  'colorInfo',
  'colorSuccess',
  'colorWarning',
  // 'sizeSmall',
  // 'sizeMedium',
  // 'sizeLarge',
]);


================================================================================
File: src/frontend/src/components/mui/title-icon/title-icon-form.tsx
Size: 1.01 kB
================================================================================

import type { FC } from 'react';
import { styled } from '@mui/material';

const TitleIconFormRoot = styled('div', {
  name: 'MuiTitleIconForm',
  slot: 'Root',
})(({ theme }) => {
  const { palette } = theme;

  return {
    flex: 'none',
    width: '3rem',
    height: '3rem',
    aspectRatio: '1 / 1',
    display: 'flex',
    alignItems: 'center',
    justifyContent: 'center',
    borderRadius: '0.375rem',
    backgroundColor: palette.grey[250],
    overflow: 'hidden',
  };
});

const TitleIconFormImage = styled('div', {
  name: 'MuiTitleIconForm',
  slot: 'Image',
})(({ theme }) => {
  const { palette } = theme;

  return {
    width: '100%',
    height: '100%',
    display: 'flex',
    alignItems: 'center',
    justifyContent: 'center',
    background: `url(/assets/common/form-half-one.png) center / cover no-repeat`,
    mixBlendMode: 'multiply',
  };
});

export const TitleIconForm: FC = () => {
  return (
    <TitleIconFormRoot>
      <TitleIconFormImage />
    </TitleIconFormRoot>
  );
};


================================================================================
File: src/frontend/src/components/mui/title-icon/title-icon-props.ts
Size: 2.12 kB
================================================================================

/* eslint-disable @typescript-eslint/no-empty-object-type */
import type { Theme, SxProps } from '@mui/material';
import type { OverridableComponent, OverridableStringUnion, OverrideProps } from '@mui/types';
import type { TitleIconClasses } from './title-icon-classes';

export interface TitleIconOwnerState extends Omit<TitleIconProps, 'slots' | 'slotProps'> {}

export interface TitleIconPropsVariantOverrides {}

export interface TitleIconPropsColorOverrides {}

// export interface TitleIconSizeOverrides {}

export interface TitleIconOwnProps {
  /**
   * The icon to display.
   */
  children?: React.ReactNode;

  /**
   * Override or extend the styles applied to the component.
   */
  classes?: Partial<TitleIconClasses>;

  /**
   * The variant to use.
   * @default 'circle'
   */
  variant?: OverridableStringUnion<'circle' | 'square', TitleIconPropsVariantOverrides>;

  /**
   * The color of the component.
   * It supports both default and custom theme colors, which can be added as shown in the
   * [palette customization guide](https://mui.com/material-ui/customization/palette/#custom-colors).
   * @default 'default'
   */
  color?: OverridableStringUnion<
    'inherit' | 'default' | 'primary' | 'secondary' | 'error' | 'info' | 'success' | 'warning',
    TitleIconPropsColorOverrides
  >;

  // /**
  //  * The size of the component.
  //  * `small` is equivalent to the dense icon styling.
  //  * @default 'medium'
  //  */
  // size?: OverridableStringUnion<'small' | 'medium' | 'large', TitleIconSizeOverrides>;

  /**
   * The system prop that allows defining system overrides as well as additional CSS styles.
   */
  sx?: SxProps<Theme>;
}

export type TitleIconTypeMap<
  AdditionalProps = {},
  RootComponent extends React.ElementType = 'div',
> = {
  props: AdditionalProps & TitleIconOwnProps;
  defaultComponent: RootComponent;
};

export type TitleIconProps<
  RootComponent extends React.ElementType = TitleIconTypeMap['defaultComponent'],
  AdditionalProps = {},
> = OverrideProps<TitleIconTypeMap<AdditionalProps, RootComponent>, RootComponent> & {
  component?: React.ElementType;
};


================================================================================
File: src/frontend/src/components/mui/title-icon/title-icon.tsx
Size: 4.66 kB
================================================================================

import type { Ref } from 'react';
import { forwardRef } from 'react';
import { clsx } from 'clsx';
import composeClasses from '@mui/utils/composeClasses';
import { capitalize } from '@mui/material/utils';
import type { CSSInterpolation } from '@mui/material';
import { iconClasses, styled, useThemeProps } from '@mui/material';

import { getTitleIconUtilityClass, type TitleIconClassKey } from './title-icon-classes';
import type {
  TitleIconOwnerState,
  TitleIconOwnProps,
  TitleIconProps,
  TitleIconTypeMap,
} from './title-icon-props';

const useUtilityClasses = (ownerState: TitleIconOwnerState) => {
  const { classes, variant, color } = ownerState;
  const slots = {
    root: [
      'root',
      variant && `variant${capitalize(variant)}`,
      color && `color${capitalize(color)}`,
    ],
    outlineOuter: ['outlineOuter'],
    outlineInner: ['outlineInner'],
  };
  return composeClasses(slots, getTitleIconUtilityClass, classes);
};

const COLORS: NonNullable<NonNullable<TitleIconProps['color']>>[] = [
  'inherit',
  'primary',
  'secondary',
  'error',
  'info',
  'success',
  'warning',
];

const TitleIconRoot = styled('div', {
  name: 'MuiTitleIcon',
  slot: 'Root',
  overridesResolver: (
    props: { ownerState: TitleIconOwnerState },
    styles: Record<TitleIconClassKey, CSSInterpolation>,
  ) => {
    const {
      ownerState: { color, variant },
    } = props;
    return [
      styles.root,
      color && styles[`color${capitalize(color)}` as TitleIconClassKey],
      variant && styles[`variant${capitalize(variant)}` as TitleIconClassKey],
    ];
  },
})<{ ownerState: TitleIconOwnerState }>(({ theme }) => {
  const { palette } = theme;
  return {
    position: 'relative',
    width: '2.25rem',
    height: '2.25rem',

    display: 'inline-flex',
    justifyContent: 'center',
    alignItems: 'center',

    color: 'inherit',
    backgroundColor: 'transparent',
    border: 'none',
    outline: 'none',

    fontSize: '1.25rem',
    [`& .${iconClasses.root}, & .tabler-icon, & svg`]: {
      width: '1em',
      height: '1em',
    },

    variants: [
      ...COLORS.map((color) => ({
        props: { color },
        style: {
          color:
            color === 'inherit' ? 'inherit'
            : color === 'default' ? palette.text.primary
            : palette[color].main,
        },
      })),
    ],
  };
});

const TitleIconOutlineOuter = styled('div', {
  name: 'MuiTitleIcon',
  slot: 'OutlineOuter',
})<{ ownerState: TitleIconOwnerState }>(() => {
  return {
    boxSizing: 'border-box',
    width: '2.25rem',
    height: '2.25rem',

    position: 'absolute',
    top: '50%',
    left: '50%',
    transform: 'translate(-50%, -50%)',

    opacity: 0.1,
    borderWidth: '1.667px',
    borderStyle: 'solid',
    borderColor: 'currentcolor',

    variants: [
      {
        props: { variant: 'circle' },
        style: {
          borderRadius: '50%',
        },
      },
      {
        props: { variant: 'square' },
        style: {
          borderRadius: '0.38rem',
        },
      },
    ],
  };
});

const TitleIconOutlineInner = styled('div', {
  name: 'MuiTitleIcon',
  slot: 'OutlineInner',
})<{ ownerState: TitleIconOwnerState }>(() => {
  return {
    boxSizing: 'border-box',
    width: '1.75rem',
    height: '1.75rem',

    position: 'absolute',
    top: '50%',
    left: '50%',
    transform: 'translate(-50%, -50%)',

    opacity: 0.3,
    borderWidth: '1.667px',
    borderStyle: 'solid',
    borderColor: 'currentcolor',

    variants: [
      {
        props: { variant: 'circle' },
        style: {
          borderRadius: '50%',
        },
      },
      {
        props: { variant: 'square' },
        style: {
          borderRadius: '0.25em',
        },
      },
    ],
  };
});

export const TitleIcon = forwardRef<HTMLDivElement, TitleIconProps>(
  function TitleIcon(inProps, ref) {
    const props = useThemeProps({ props: inProps, name: 'MuiTitleIcon' });
    const {
      children,
      classes: classesProp,
      variant = 'circle',
      color = 'default',
      className,
      ...rest
    } = props;
    const ownerState: TitleIconOwnerState = {
      ...props,
      variant,
      color,
    };

    const classes = useUtilityClasses(ownerState);

    return (
      <TitleIconRoot
        ref={ref}
        className={clsx(classes.root, className)}
        ownerState={ownerState}
        role='img'
        {...rest}
      >
        <TitleIconOutlineOuter className={classes.outlineOuter} ownerState={ownerState} />
        <TitleIconOutlineInner className={classes.outlineInner} ownerState={ownerState} />
        {children}
      </TitleIconRoot>
    );
  },
);


================================================================================
File: src/frontend/src/global.css
Size: 363 B
================================================================================

*,
*::before,
*::after {
  box-sizing: border-box;
}

:root {
  font-size: 16px;
}

html,
body,
#root {
  position: relative;
  width: 100vw;
  height: 100vh;
  margin: 0;
  padding: 0;
  overflow: hidden;
}

input::-webkit-outer-spin-button,
input::-webkit-inner-spin-button {
  -webkit-appearance: none;
}
input[type='number'] {
  -moz-appearance: textfield;
}


================================================================================
File: src/frontend/src/hooks/index.ts
Size: 61 B
================================================================================

export * from './use-const';
export * from './use-callback';


================================================================================
File: src/frontend/src/hooks/use-callback.ts
Size: 636 B
================================================================================

import { useRef } from 'react';

// eslint-disable-next-line @typescript-eslint/no-explicit-any
export const useConstCallback = <F extends (...args: any[]) => any>(callback: F) => {
  const ref = useRef<{ callback: F }>(undefined);
  if (!ref.current) {
    ref.current = { callback };
  }
  return ref.current.callback;
};

// eslint-disable-next-line @typescript-eslint/no-explicit-any
export const useRefCallback = <F extends (...args: any[]) => any>(callback: F) => {
  const ref = useRef<F>(undefined);
  ref.current = callback;
  const invoke = useConstCallback<F>(((...args) => ref.current?.(...args)) as F);
  return invoke;
};


================================================================================
File: src/frontend/src/hooks/use-const.ts
Size: 284 B
================================================================================

import { useRef } from 'react';

export const useConst = <T>(value: T | (() => T)): T => {
  const ref = useRef<{ value: T }>(undefined);
  if (!ref.current) {
    ref.current = { value: typeof value === 'function' ? (value as () => T)() : value };
  }
  return ref.current.value;
};


================================================================================
File: src/frontend/src/main.tsx
Size: 141 B
================================================================================

import { createRoot } from 'react-dom/client';
import { Main } from './App';

createRoot(document.getElementById('root')!).render(<Main />);


================================================================================
File: src/frontend/src/pages/chat.tsx
Size: 292 B
================================================================================

import { Box } from '@mui/material';
import { DrawerLayout } from '../components/common';
import { ChatInput, ChatMessages } from '../components/inputs';

export default function PageChat() {
  return (
    <DrawerLayout>
      <ChatMessages />
      <ChatInput />
    </DrawerLayout>
  );
}


================================================================================
File: src/frontend/src/pages/join.tsx
Size: 3.42 kB
================================================================================

import { useMemo, useState } from 'react';
import { Link as RouterLink } from 'react-router-dom';
import {
  Alert,
  Button,
  ButtonGroup,
  FormControl,
  FormControlLabel,
  FormLabel,
  MenuItem,
  Select,
  Stack as MuiStack,
  TextField,
  ToggleButton,
  ToggleButtonGroup,
  Typography,
  styled,
} from '@mui/material';
import { IconArrowLeft } from '@tabler/icons-react';
import { MainLayout } from '../components/common';
import { JoinCommand, ModelSelect, NodeList } from '../components/inputs';
import { useCluster } from '../services';

const Stack = styled(MuiStack)(({ theme }) => {
  const { spacing } = theme;
  return {
    overflowY: 'auto',
  };
});

export default function PageJoin() {
  const [
    {
      config: { modelInfo },
      clusterInfo: { status: clusterStatus, initNodesNumber, needMoreNodes },
      nodeInfoList,
    },
  ] = useCluster();

  const isError = useMemo(() => {
    if (
      initNodesNumber > 0
      && nodeInfoList.length >= initNodesNumber
      && nodeInfoList.every((node) => node.status === 'available')
      && clusterStatus === 'waiting'
    ) {
      return true;
    }
    return false;
  }, [clusterStatus, initNodesNumber, nodeInfoList]);

  return (
    <MainLayout
      contentStart={
        <Button
          component={RouterLink}
          to='/setup'
          size='medium'
          color='secondary'
          startIcon={<IconArrowLeft />}
        >
          Back
        </Button>
      }
    >
      <Typography variant='h1'>Get Your Nodes Running</Typography>

      <Stack gap={6} sx={{ overflow: 'hidden' }}>
        <Stack gap={2}>
          <Stack gap={1}>
            <Typography variant='body1'>Step 1 - Run join command on all nodes</Typography>
          </Stack>
          <JoinCommand />
        </Stack>

        <Stack gap={2} flex={1}>
          <Stack gap={1}>
            <Typography variant='body1'>Step 2 - Check your node status</Typography>
            <Typography variant='body2' color='text.secondary' fontWeight='regular'>
              After you successfully start your nodes, you should see them start to show up below
              with their status. Once all nodes are connected, you will automatically be directed to
              the chat interface.
            </Typography>
          </Stack>

          {(isError && (
            <Alert key='error' severity='error' variant='standard'>
              Your selected model requires more nodes. Please go back to the previous step to add
              more nodes, or choose a smaller model.
            </Alert>
          )) || (
            <Alert key='info' severity='info' variant='standard'>
              If your nodes cannot connect properly, retry the above join command to restart the
              server.
            </Alert>
          )}

          {!!modelInfo && modelInfo.vram > 0 && needMoreNodes && (
            <Alert key='vram-warning' severity='warning' variant='standard'>
              <Typography variant='inherit'>
                {[
                  `Your selected model requires more nodes.`,
                  `Youâ€™ll need a `,
                  <strong>{`minimum of ${modelInfo.vram} GB of total VRAM`}</strong>,
                  ` to host this model.`,
                ]}
              </Typography>
            </Alert>
          )}

          <NodeList key='node-list' />
        </Stack>
      </Stack>
    </MainLayout>
  );
}


================================================================================
File: src/frontend/src/pages/setup.tsx
Size: 4.64 kB
================================================================================

import { useState } from 'react';
import { Link as RouterLink, useNavigate } from 'react-router-dom';
import {
  Alert,
  Button,
  ButtonGroup,
  FormControl,
  FormControlLabel,
  FormLabel,
  MenuItem,
  Select,
  Stack,
  styled,
  TextField,
  ToggleButton,
  ToggleButtonGroup,
  Typography,
} from '@mui/material';
import { MainLayout } from '../components/common';
import { ModelSelect, NumberInput } from '../components/inputs';
import { useCluster } from '../services';
import { useRefCallback } from '../hooks';

export default function PageSetup() {
  const [
    {
      config: { networkType, initNodesNumber, modelInfo },
      clusterInfo: { status: clusterStatus },
    },
    {
      config: { setNetworkType, setInitNodesNumber },
      init,
    },
  ] = useCluster();

  const navigate = useNavigate();

  const [loading, setLoading] = useState(false);

  const onContinue = useRefCallback(async () => {
    if (clusterStatus === 'idle' || clusterStatus === 'failed') {
      setLoading(true);
      Promise.resolve()
        .then(() => init())
        .then(() => navigate('/join'))
        .catch((e) => console.error(e))
        .finally(() => setLoading(false));
      return;
    } else {
      navigate('/join');
    }
  });

  return (
    <MainLayout>
      <Typography variant='h1'>Build Your Own AI Cluster</Typography>

      <Stack gap={2.5}>
        <Stack gap={0.5}>
          <Typography variant='body1'>Step 1 - Specify the initial number of nodes</Typography>
          <Typography variant='body2' color='text.secondary' fontWeight='regular'>
            Parallax runs and hosts model distributedly on your everyday hardware. Select the number
            of nodes you would like to add to your cluster with their connection types.{' '}
          </Typography>
        </Stack>

        <Stack direction='row' justifyContent='space-between' alignItems='center' gap={2}>
          <Typography color='text.secondary'>Node Number</Typography>
          <NumberInput
            sx={{ width: '10rem', boxShadow: 'none', bgcolor: 'transparent' }}
            slotProps={{
              root: {
                sx: {
                  bgcolor: 'transparent',
                  '&:hover': { bgcolor: 'transparent' },
                  '&:focus-within': { bgcolor: 'transparent' },
                },
              },
              input: {
                sx: {
                  bgcolor: 'transparent !important',
                  '&:focus': { outline: 'none' },
                },
              },
            }}
            value={initNodesNumber}
            onChange={(e) => setInitNodesNumber(Number(e.target.value))}
          />
        </Stack>

        <Stack direction='row' justifyContent='space-between' alignItems='center' gap={2}>
          <Typography color='text.secondary'>
            Are you nodes within the same local network?
          </Typography>
          <ToggleButtonGroup
            sx={{ width: '10rem', textTransform: 'none' }}
            exclusive
            value={networkType}
            onChange={(_, value) => value && setNetworkType(value)}
          >
            <ToggleButton value='local' sx={{ textTransform: 'none' }}>
              Local
            </ToggleButton>
            <ToggleButton value='remote' sx={{ textTransform: 'none' }}>
              Remote
            </ToggleButton>
          </ToggleButtonGroup>
        </Stack>
      </Stack>

      <Stack gap={2.5}>
        <Stack gap={0.5}>
          <Typography variant='body1'>Step 2 - Select the model you would like to host</Typography>
          <Typography variant='body2' color='text.secondary' fontWeight='regular'>
            Currently we support a handful of state-of-the-art open source models. Do keep in mind
            that larger models require more nodes to host, so If this is your first time trying
            Parallax, we suggest you to start with smaller models.
          </Typography>
        </Stack>

        <ModelSelect />

        {!!modelInfo && modelInfo.vram > 0 && (
          <Alert key='vram-warning' severity='warning' variant='standard'>
            <Typography variant='inherit'>
              {[
                `Youâ€™ll need a `,
                <strong>{`minimum of ${modelInfo.vram} GB of total VRAM`}</strong>,
                ` to host this model.`,
              ]}
            </Typography>
          </Alert>
        )}
      </Stack>

      <Stack direction='row' justifyContent='flex-end' alignItems='center' gap={2}>
        <Button loading={loading} onClick={onContinue}>
          Continue
        </Button>
      </Stack>
    </MainLayout>
  );
}


================================================================================
File: src/frontend/src/router/chat.tsx
Size: 1.1 kB
================================================================================

// src/router/index.tsx
import { lazy, Suspense, useEffect } from 'react';
import { useLocation, useRoutes, useNavigate } from 'react-router-dom';

const PATH_CHAT = '/chat';

const PageChat = lazy(() => import('../pages/chat'));

const debugLog = (...args: any[]) => {
  console.log('%c router.tsx ', 'color: white; background: purple;', ...args);
};

export const ChatRouter = () => {
  const navigate = useNavigate();
  const { pathname } = useLocation();

  useEffect(() => {
    const lazyNavigate = (path: string) => {
      const timer = setTimeout(() => {
        debugLog('navigate to', path);
        navigate(path);
      }, 300);
      return () => clearTimeout(timer);
    };

    if (!pathname.startsWith(PATH_CHAT)) {
      return lazyNavigate(PATH_CHAT);
    }
  }, [navigate, pathname]);

  const routes = useRoutes([
    {
      path: PATH_CHAT,
      element: (
        <Suspense fallback={<div>Loading...</div>}>
          <PageChat />
        </Suspense>
      ),
    },
    {
      path: '*',
      element: <div>404 - Page Not Found</div>,
    },
  ]);
  return routes;
};


================================================================================
File: src/frontend/src/router/index.tsx
Size: 48 B
================================================================================

export * from './main';
export * from './chat';


================================================================================
File: src/frontend/src/router/main.tsx
Size: 2.04 kB
================================================================================

// src/router/index.tsx
import { lazy, Suspense, useEffect } from 'react';
import { useLocation, useRoutes, Navigate, useNavigate } from 'react-router-dom';
import { useCluster } from '../services';
import { useConstCallback, useRefCallback } from '../hooks';

const PATH_SETUP = '/setup';
const PATH_JOIN = '/join';
const PATH_CHAT = '/chat';

const PageSetup = lazy(() => import('../pages/setup'));
const PageJoin = lazy(() => import('../pages/join'));
const PageChat = lazy(() => import('../pages/chat'));

const debugLog = (...args: any[]) => {
  console.log('%c router.tsx ', 'color: white; background: purple;', ...args);
};

export const MainRouter = () => {
  const navigate = useNavigate();
  const { pathname } = useLocation();

  const [
    {
      clusterInfo: { status },
    },
  ] = useCluster();

  useEffect(() => {
    const lazyNavigate = (path: string) => {
      const timer = setTimeout(() => {
        debugLog('navigate to', path);
        navigate(path);
      }, 300);
      return () => clearTimeout(timer);
    };

    if (pathname === '/') {
      return lazyNavigate(PATH_SETUP);
    }
    debugLog('pathname', pathname, 'cluster status', status);
    if (status === 'idle' && pathname.startsWith(PATH_CHAT)) {
      return lazyNavigate(PATH_SETUP);
    }
    if (status === 'available' && !pathname.startsWith(PATH_CHAT)) {
      return lazyNavigate(PATH_CHAT);
    }
  }, [navigate, pathname, status]);

  const routes = useRoutes([
    {
      path: PATH_SETUP,
      element: (
        <Suspense fallback={<div>Loading...</div>}>
          <PageSetup />
        </Suspense>
      ),
    },
    {
      path: PATH_JOIN,
      element: (
        <Suspense fallback={<div>Loading...</div>}>
          <PageJoin />
        </Suspense>
      ),
    },
    {
      path: PATH_CHAT,
      element: (
        <Suspense fallback={<div>Loading...</div>}>
          <PageChat />
        </Suspense>
      ),
    },
    {
      path: '*',
      element: <div>404 - Page Not Found</div>,
    },
  ]);
  return routes;
};


================================================================================
File: src/frontend/src/services/api.ts
Size: 1.03 kB
================================================================================

import { createHttpStreamFactory } from './http-stream';

export const API_BASE_URL = import.meta.env.DEV ? '/proxy-api' : '';

export const getModelList = async (): Promise<readonly any[]> => {
  const response = await fetch(`${API_BASE_URL}/model/list`, { method: 'GET' });
  const message = await response.json();
  if (message.type !== 'model_list') {
    throw new Error(`Invalid message type: ${message.type}.`);
  }
  return message.data;
};

export const initScheduler = async (params: {
  model_name: string;
  init_nodes_num: number;
  is_local_network: boolean;
}): Promise<void> => {
  const response = await fetch(`${API_BASE_URL}/scheduler/init`, {
    method: 'POST',
    body: JSON.stringify(params),
  });
  const message = await response.json();
  if (message.type !== 'scheduler_init') {
    throw new Error(`Invalid message type: ${message.type}.`);
  }
  return message.data;
};

export const createStreamClusterStatus = createHttpStreamFactory({
  url: `${API_BASE_URL}/cluster/status`,
  method: 'GET',
});


================================================================================
File: src/frontend/src/services/chat-helper.tsx
Size: 1.43 kB
================================================================================

/**
 * Mapping GPT generation massages.
 * The key is the channel name, the value is the message.
 */
export interface GptGenerationMap {
  analysis: string;
  final: string;
  [key: string]: string;
}

export function parseGenerationGpt(buffer: string): GptGenerationMap {
  buffer = buffer.trim();

  const map: GptGenerationMap = { analysis: '', final: '' };

  const regex = /<\|channel\|>([^<]+)<\|message\|>(.*?)(<\|end\|>|$)/gs;
  let match: RegExpExecArray | null;
  while ((match = regex.exec(buffer)) !== null) {
    map[match[1]] = match[2]?.trim() || '';
  }

  return map;
}

export interface QwenGenerationMap {
  think: string;
  content: string;
}

const THINK_TAG_OPEN = '<think>';
const THINK_TAG_CLOSE = '</think>';

export function parseGenerationQwen(buffer: string): QwenGenerationMap {
  buffer = buffer.trim();

  const map: QwenGenerationMap = { think: '', content: '' };

  while (buffer.includes(THINK_TAG_OPEN)) {
    const thinkStart = buffer.indexOf(THINK_TAG_OPEN);
    const thinkEnd = buffer.indexOf(THINK_TAG_CLOSE);
    const think = buffer.substring(
      thinkStart + THINK_TAG_OPEN.length,
      thinkEnd > thinkStart ? thinkEnd : buffer.length,
    );
    buffer = buffer.replace(
      THINK_TAG_OPEN + think + (thinkEnd > thinkStart ? THINK_TAG_CLOSE : ''),
      '',
    );
    map.think += '\n\n' + think;
  }
  map.think = map.think.trim();
  map.content = buffer.trim();

  return map;
}


================================================================================
File: src/frontend/src/services/chat.tsx
Size: 14.17 kB
================================================================================

/* eslint-disable react-refresh/only-export-components */
import {
  createContext,
  useContext,
  useMemo,
  useState,
  type Dispatch,
  type FC,
  type PropsWithChildren,
  type SetStateAction,
} from 'react';
import { API_BASE_URL } from './api';
import { useConst, useRefCallback } from '../hooks';
import { useCluster } from './cluster';
import { parseGenerationGpt, parseGenerationQwen } from './chat-helper';

const debugLog = async (...args: any[]) => {
  if (import.meta.env.DEV) {
    console.log('%c chat.tsx ', 'color: white; background: orange;', ...args);
  }
};

export type ChatMessageRole = 'user' | 'assistant';

export type ChatMessageStatus = 'waiting' | 'thinking' | 'generating' | 'done' | 'error';

export interface ChatMessage {
  readonly id: string;
  readonly role: ChatMessageRole;
  readonly status: ChatMessageStatus;

  /**
   * The content from user input or assistant generating.
   */
  readonly content: string;

  /**
   * The raw content from model response.
   */
  readonly raw?: string;

  /**
   * The thinking content in assistant generating.
   */
  readonly thinking?: string;
  readonly createdAt: number;
}

export type ChatStatus = 'closed' | 'opened' | 'generating' | 'error';

export interface ChatStates {
  readonly input: string;
  readonly status: ChatStatus;
  readonly messages: readonly ChatMessage[];
}

export interface ChatActions {
  readonly setInput: Dispatch<SetStateAction<string>>;
  readonly generate: (message?: ChatMessage) => void;
  readonly stop: () => void;
  readonly clear: () => void;
}

export const ChatProvider: FC<PropsWithChildren> = ({ children }) => {
  const [
    {
      clusterInfo: { status: clusterStatus, modelName },
    },
  ] = useCluster();

  const [input, setInput] = useState<string>('');

  const [status, _setStatus] = useState<ChatStatus>('closed');
  const setStatus = useRefCallback<typeof _setStatus>((value) => {
    _setStatus((prev) => {
      const next = typeof value === 'function' ? value(prev) : value;
      if (next !== prev) {
        debugLog('setStatus', 'status', next);
      }
      return next;
    });
  });

  const [messages, setMessages] = useState<readonly ChatMessage[]>([]);

  const sse = useConst(() =>
    createSSE({
      onOpen: () => {
        debugLog('SSE OPEN');
        setStatus('opened');
      },
      onClose: () => {
        debugLog('SSE CLOSE');
        setMessages((prev) => {
          const lastMessage = prev[prev.length - 1];
          const { id, raw, thinking, content } = lastMessage;
          debugLog('GENERATING DONE', 'lastMessage:', lastMessage);
          debugLog('GENERATING DONE', 'id:', id);
          debugLog('GENERATING DONE', 'raw:', raw);
          debugLog('GENERATING DONE', 'thinking:', thinking);
          debugLog('GENERATING DONE', 'content:', content);
          return [
            ...prev.slice(0, -1),
            {
              ...lastMessage,
              status: 'done',
            },
          ];
        });
        setStatus('closed');
      },
      onError: (error) => {
        debugLog('SSE ERROR', error);
        // Set last message to done
        setMessages((prev) => {
          const lastMessage = prev[prev.length - 1];
          const { id, raw, thinking, content } = lastMessage;
          debugLog('GENERATING ERROR', 'lastMessage:', lastMessage);
          debugLog('GENERATING ERROR', 'id:', id);
          debugLog('GENERATING ERROR', 'raw:', raw);
          debugLog('GENERATING ERROR', 'thinking:', thinking);
          debugLog('GENERATING ERROR', 'content:', content);
          return [
            ...prev.slice(0, -1),
            {
              ...lastMessage,
              status: 'done',
            },
          ];
        });
        debugLog('SSE ERROR', error);
        setStatus('error');
      },
      onMessage: (message) => {
        // debugLog('onMessage', message);
        // const example = {
        //   id: 'd410014e-3308-450d-bbd2-0ec4e0c0a345',
        //   object: 'chat.completion.chunk',
        //   model: 'default',
        //   created: 1758842801.822061,
        //   choices: [
        //     {
        //       index: 0,
        //       logprobs: null,
        //       finish_reason: null,
        //       matched_stop: null,
        //       delta: { role: null, content: ' the' },
        //     },
        //   ],
        //   usage: null,
        // };
        const {
          data: { id, object, model, created, choices, usage },
        } = message;
        if (object === 'chat.completion.chunk' && choices?.length > 0) {
          if (choices[0].delta.content) {
            setStatus('generating');
          }
          setMessages((prev) => {
            let next = prev;
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            choices.forEach(({ delta: { role, content: rawDelta } = {} }: any) => {
              if (typeof rawDelta !== 'string' || !rawDelta) {
                return;
              }
              role = role || 'assistant';
              let lastMessage = next[next.length - 1];
              if (lastMessage && lastMessage.role === role) {
                const raw = lastMessage.raw + rawDelta;
                lastMessage = {
                  ...lastMessage,
                  raw: raw,
                  content: raw,
                };
                next = [...next.slice(0, -1), lastMessage];
              } else {
                lastMessage = {
                  id,
                  role,
                  status: 'thinking',
                  raw: rawDelta,
                  content: rawDelta,
                  createdAt: created,
                };
                next = [...next, lastMessage];
              }
              // debugLog('onMessage', 'update last message', lastMessage.content);
            });

            // Parse generation and extract thinking and content
            if (next !== prev && typeof model === 'string') {
              let lastMessage = next[next.length - 1];
              let thinking = '';
              let content = '';
              const modelLowerCase = model.toLowerCase();
              if (modelLowerCase.includes('gpt-oss')) {
                ({ analysis: thinking, final: content } = parseGenerationGpt(
                  lastMessage.raw || '',
                ));
              } else if (modelLowerCase.includes('qwen')) {
                ({ think: thinking, content } = parseGenerationQwen(lastMessage.raw || ''));
              } else {
                content = lastMessage.raw || '';
              }
              lastMessage = {
                ...lastMessage,
                status: (content && 'generating') || 'thinking',
                thinking,
                content,
              };
              next = [...next.slice(0, -1), lastMessage];
            }

            return next;
          });
        }
      },
    }),
  );

  const generate = useRefCallback<ChatActions['generate']>((message) => {
    if (clusterStatus !== 'available' || status === 'opened' || status === 'generating') {
      return;
    }

    if (!modelName) {
      return;
    }

    let nextMessages: readonly ChatMessage[] = messages;
    if (message) {
      // Regenerate
      const finalMessageIndex = messages.findIndex((m) => m.id === message.id);
      const finalMessage = messages[finalMessageIndex];
      if (!finalMessage) {
        return;
      }
      nextMessages = nextMessages.slice(
        0,
        finalMessageIndex + (finalMessage.role === 'user' ? 1 : 0),
      );
      debugLog('generate', 'regenerate', nextMessages);
    } else {
      // Generate for new input
      const finalInput = input.trim();
      if (!finalInput) {
        return;
      }
      setInput('');
      const now = performance.now();
      nextMessages = [
        ...nextMessages,
        { id: now.toString(), role: 'user', status: 'done', content: finalInput, createdAt: now },
      ];
      debugLog('generate', 'new', nextMessages);
    }
    setMessages(nextMessages);

    sse.connect(
      modelName,
      nextMessages.map(({ id, role, content }) => ({ id, role, content })),
    );
  });

  const stop = useRefCallback<ChatActions['stop']>(() => {
    debugLog('stop', 'status', status);
    if (status === 'closed' || status === 'error') {
      return;
    }
    sse.disconnect();
  });

  const clear = useRefCallback<ChatActions['clear']>(() => {
    debugLog('clear', 'status', status);
    stop();
    if (status === 'opened' || status === 'generating') {
      return;
    }
    setMessages([]);
  });

  const actions = useConst<ChatActions>({
    setInput,
    generate,
    stop,
    clear,
  });

  const value = useMemo<readonly [ChatStates, ChatActions]>(
    () => [
      {
        input,
        status,
        messages,
      },
      actions,
    ],
    [input, status, messages, actions],
  );

  return <context.Provider value={value}>{children}</context.Provider>;
};

const context = createContext<readonly [ChatStates, ChatActions] | undefined>(undefined);

export const useChat = (): readonly [ChatStates, ChatActions] => {
  const value = useContext(context);
  if (!value) {
    throw new Error('useChat must be used within a ChatProvider');
  }
  return value;
};

// ================================================================
// SSE

interface SSEOptions {
  onOpen?: () => void;
  onClose?: () => void;
  onError?: (error: Error) => void;
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  onMessage?: (message: { event: string; id?: string; data: any }) => void;
}

interface RequestMessage {
  readonly id: string;
  readonly role: ChatMessageRole;
  readonly content: string;
}

const createSSE = (options: SSEOptions) => {
  const { onOpen, onClose, onError, onMessage } = options;

  const decoder = new TextDecoder();
  let reader: ReadableStreamDefaultReader<Uint8Array> | undefined;
  let abortController: AbortController | undefined;

  const connect = (model: string, messages: readonly RequestMessage[]) => {
    abortController = new AbortController();
    const url = `${API_BASE_URL}/v1/chat/completions`;

    onOpen?.();

    fetch(url, {
      method: 'POST',
      body: JSON.stringify({
        stream: true,
        model,
        messages,
        max_tokens: 2048,
        sampling_params: {
          top_k: 3,
        },
      }),
      signal: abortController.signal,
    })
      .then(async (response) => {
        const statusCode = response.status;
        const contentType = response.headers.get('Content-Type');
        if (statusCode !== 200) {
          onError?.(new Error(`[SSE] Failed to connect: ${statusCode}`));
          return;
        }
        if (!contentType?.includes('text/event-stream')) {
          onError?.(new Error(`[SSE] Invalid content type: ${contentType}`));
          return;
        }

        reader = response.body?.getReader();
        if (!reader) {
          onError?.(new Error(`[SSE] Failed to get reader`));
          return;
        }

        let buffer = '';

        const processLines = (lines: string[]) => {
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          const message: { event: string; id?: string; data: any } = {
            event: 'message',
            data: undefined,
          };
          lines.forEach((line) => {
            const colonIndex = line.indexOf(':');
            if (colonIndex <= 0) {
              // No colon, skip
              return;
            }

            const field = line.slice(0, colonIndex).trim();
            const value = line.slice(colonIndex + 1).trim();

            if (value.startsWith(':')) {
              // Comment line
              return;
            }

            switch (field) {
              case 'event':
                message.event = value;
                break;
              case 'id':
                message.id = value;
                break;
              case 'data':
                try {
                  // Try to parse as JSON object
                  const data = JSON.parse(value);
                  // eslint-disable-next-line @typescript-eslint/no-explicit-any
                  const walk = (data: any) => {
                    if (!data) {
                      return;
                    }
                    if (Array.isArray(data)) {
                      data.forEach((item, i) => {
                        if (item === null) {
                          data[i] = undefined;
                        } else {
                          walk(item);
                        }
                      });
                    } else if (typeof data === 'object') {
                      Object.keys(data).forEach((key) => {
                        if (data[key] === null) {
                          delete data[key];
                        } else {
                          walk(data[key]);
                        }
                      });
                    }
                  };
                  walk(data);
                  message.data = data;
                } catch (error) {
                  // Parse failed, use original data
                  message.data = value;
                }
                break;
            }

            if (message.data !== undefined) {
              onMessage?.(message);
            }
          });
        };

        while (true) {
          const { done, value } = await reader.read();
          if (done) {
            onClose?.();
            return;
          }

          const chunk = decoder.decode(value);
          buffer += chunk;

          const lines = buffer.split('\n');
          buffer = lines.pop() || '';

          processLines(lines);
        }
      })
      .catch((error: Error) => {
        if (error instanceof Error && error.name === 'AbortError') {
          onClose?.();
          return;
        }
        onError?.(error);
      });
  };

  const disconnect = () => {
    reader?.cancel();
    reader = undefined;
    abortController?.abort('stop');
    abortController = undefined;

    onClose?.();
  };

  return { connect, disconnect };
};


================================================================================
File: src/frontend/src/services/cluster.tsx
Size: 9.11 kB
================================================================================

/* eslint-disable react-refresh/only-export-components */
import type { Dispatch, SetStateAction, FC, PropsWithChildren } from 'react';
import { createContext, useContext, useEffect, useMemo, useState } from 'react';
import { useRefCallback } from '../hooks';
import { createStreamClusterStatus, getModelList, initScheduler } from './api';
import { useHost } from './host';

import logoUrlOpenAI from '../assets/models/OpenAI-black-monoblossom.svg';
import logoUrlQwen from '../assets/models/Qwen3.png';
import logoUrlNvidia from '../assets/models/NVIDIA.png';
import logoUrlMoonshotAI from '../assets/models/MoonshotAI.png';
import logoUrlDeepseek from '../assets/models/DeepSeek.png';
import logoUrlZai from '../assets/models/Zai.png';
import logoUrlMiniMax from '../assets/models/MiniMax.png';

const logoUrlMap: Readonly<Record<string, string>> = {
  openai: logoUrlOpenAI,
  qwen: logoUrlQwen,
  nvidia: logoUrlNvidia,
  moonshotai: logoUrlMoonshotAI,
  deepseek: logoUrlDeepseek,
  zai: logoUrlZai,
  minimaxai: logoUrlMiniMax,
};

const getLogoUrl = (name: string) => {
  name = name.toLowerCase();
  const parts = name.split(/[-/]/);
  return logoUrlMap[parts[0]] || '';
};

const debugLog = (...args: any[]) => {
  console.log('%c cluster.tsx ', 'color: white; background: darkcyan;', ...args);
};

export interface ModelInfo {
  readonly name: string;
  readonly displayName: string;
  readonly logoUrl: string;

  /**
   * The VRAM required for the model in GB.
   */
  readonly vram: number;
}

export type ClusterStatus = 'idle' | 'waiting' | 'available' | 'rebalancing' | 'failed';

export interface ClusterInfo {
  readonly id: string;
  readonly status: ClusterStatus;
  readonly modelName: string;
  readonly modelInfo: ModelInfo | undefined;
  readonly nodeJoinCommand: Readonly<Record<string, string>>;
  readonly initNodesNumber: number;
  readonly needMoreNodes: boolean;
}

const INITIAL_CLUSTER_INFO: ClusterInfo = {
  id: '',
  status: 'idle',
  modelName: '',
  modelInfo: undefined,
  nodeJoinCommand: {},
  initNodesNumber: 4,
  needMoreNodes: false,
};

export type NodeStatus = 'waiting' | 'available' | 'failed';

export interface NodeInfo {
  readonly id: string;
  readonly status: NodeStatus;
  readonly gpuNumber: number;
  readonly gpuName: string;
  readonly gpuMemory: number;
}

// Configs

export type NetworkType = 'local' | 'remote';

export interface ClusterConfig {
  readonly networkType: NetworkType;
  readonly initNodesNumber: number;
  readonly modelName: string;
  readonly modelInfo: ModelInfo | undefined;
  readonly modelInfoList: readonly ModelInfo[];
}

export interface ClusterConfigSetters {
  readonly setNetworkType: Dispatch<SetStateAction<NetworkType>>;
  readonly setInitNodesNumber: Dispatch<SetStateAction<number>>;
  readonly setModelName: Dispatch<SetStateAction<string>>;
}

// Interface

export interface ClusterStates {
  readonly config: ClusterConfig;
  readonly clusterInfo: ClusterInfo;
  readonly nodeInfoList: readonly NodeInfo[];
}

export interface ClusterActions {
  readonly config: ClusterConfigSetters;
  readonly init: () => Promise<void>;
}

// Implementation

const context = createContext<readonly [ClusterStates, ClusterActions] | undefined>(undefined);

const { Provider } = context;

export const ClusterProvider: FC<PropsWithChildren> = ({ children }) => {
  const [{ type: hostType }] = useHost();

  // Configs
  const [networkType, setNetworkType] = useState<NetworkType>('local');
  const [initNodesNumber, setInitNodesNumber] = useState(1);
  const [modelName, setModelName] = useState<string>('');

  // Model List
  const [modelInfoList, setModelInfoList] = useState<readonly ModelInfo[]>([]);

  const updateModelList = useRefCallback(async () => {
    if (hostType === 'node') {
      return;
    }
    let succeed = false;
    while (!succeed) {
      try {
        const rawList = await getModelList();
        setModelInfoList((prev) => {
          const next = rawList.map<ModelInfo>(({ name, vram_gb }) => {
            name = name || '';
            vram_gb = vram_gb || 0;
            return {
              name,
              displayName: name,
              logoUrl: getLogoUrl(name),
              vram: vram_gb,
            };
          });
          if (JSON.stringify(next) !== JSON.stringify(prev)) {
            debugLog('setModelInfoList', next);
            return next;
          }
          return prev;
        });
        succeed = true;
      } catch (error) {
        console.error('getModelList error', error);
        await new Promise((resolve) => setTimeout(resolve, 2000));
      }
    }
  });

  useEffect(() => {
    updateModelList();
  }, []);

  useEffect(() => {
    if (modelInfoList.length) {
      setModelName(modelInfoList[0].name);
    }
  }, [modelInfoList]);

  // Cluster and Nodes
  const [clusterInfo, setClusterInfo] = useState<ClusterInfo>(INITIAL_CLUSTER_INFO);
  const [nodeInfoList, setNodeInfoList] = useState<readonly NodeInfo[]>([]);

  const reset = useRefCallback(() => {
    debugLog('reset');
    setClusterInfo(INITIAL_CLUSTER_INFO);
    setNodeInfoList([]);
  });

  const streamClusterStatus = useMemo(() => {
    const onMessage = (message: any) => {
      if (message.type === 'cluster_status') {
        const {
          data: {
            status,
            init_nodes_num,
            model_name,
            node_join_command,
            node_list,
            need_more_nodes,
          },
        } = message;
        setClusterInfo((prev) => {
          const next = {
            ...prev,
            status: (model_name && status) || 'idle',
            initNodesNumber: init_nodes_num || 0,
            modelName: model_name || '',
            modelInfo: modelInfoList.find((model) => model.name === model_name),
            nodeJoinCommand: node_join_command || {},
            needMoreNodes: need_more_nodes || false,
          };
          if (JSON.stringify(next) !== JSON.stringify(prev)) {
            debugLog('setClusterInfo', next);
            return next;
          }
          return prev;
        });
        setNodeInfoList((prev) => {
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          let next = (node_list as any[]).map<NodeInfo>(
            ({ node_id, status, gpu_num, gpu_name, gpu_memory }: any) => ({
              id: node_id,
              status,
              gpuNumber: gpu_num || 1,
              gpuName: gpu_name || '',
              gpuMemory: gpu_memory || 0,
            }),
          );

          const prevOnlineNodes = prev.filter((preNode) =>
            next.some((nextNode) => nextNode.id === preNode.id),
          );
          const prevOfflineNodes = prev
            .filter((preNode) => !next.some((nextNode) => nextNode.id === preNode.id))
            .map<NodeInfo>((offlineNode) => ({
              ...offlineNode,
              status: 'failed',
            }));

          if (JSON.stringify(next) === JSON.stringify(prevOnlineNodes)) {
            next = [...next, ...prevOfflineNodes];
          }

          if (JSON.stringify(next) !== JSON.stringify(prev)) {
            debugLog('setNodeInfoList', next);
            return next;
          }
          return prev;
        });
      }
    };
    const stream = createStreamClusterStatus({
      debugName: 'ClusterStatus',
      autoReconnect: true,
      onMessage,
      onError: reset,
    });
    return stream;
  }, []);

  useEffect(() => {
    streamClusterStatus.send();
  }, []);

  // Init

  const init = useRefCallback(async () => {
    if (initNodesNumber < 1) {
      throw new Error('initNodesNumber must be greater than 0');
    }
    if (!modelName) {
      throw new Error('modelName is required');
    }

    const params: Parameters<typeof initScheduler>[0] = {
      model_name: modelName,
      init_nodes_num: initNodesNumber,
      is_local_network: networkType === 'local',
    };

    debugLog('initScheduler', params);
    await initScheduler(params);
    // setClusterInfo((prev) => ({
    //   ...prev,
    //   status: 'waiting',
    // }));
  });

  // Forwards

  const actions: ClusterActions = useMemo(() => {
    return {
      config: {
        setNetworkType,
        setInitNodesNumber,
        setModelName,
      },
      init,
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  const value = useMemo<readonly [ClusterStates, ClusterActions]>(
    () => [
      {
        config: {
          networkType,
          initNodesNumber,
          modelName,
          modelInfo: modelInfoList.find((model) => model.name === modelName),
          modelInfoList,
        },
        clusterInfo,
        nodeInfoList,
      },
      actions,
    ],
    [networkType, initNodesNumber, modelName, modelInfoList, clusterInfo, nodeInfoList, actions],
  );

  return <Provider value={value}>{children}</Provider>;
};

export const useCluster = (): readonly [ClusterStates, ClusterActions] => {
  const value = useContext(context);
  if (!value) {
    throw new Error('useCluster must be used within a ClusterProvider');
  }
  return value;
};


================================================================================
File: src/frontend/src/services/host.tsx
Size: 1.22 kB
================================================================================

/* eslint-disable react-refresh/only-export-components */
import type { FC, PropsWithChildren } from 'react';
import { createContext, useContext, useMemo } from 'react';
import { useConst } from '../hooks';

export type HostType = 'cluster' | 'node';

export interface HostProps {
  readonly type: HostType;
}

// eslint-disable-next-line @typescript-eslint/no-empty-object-type
export interface HostStates extends HostProps {}

// eslint-disable-next-line @typescript-eslint/no-empty-object-type
export interface HostActions {}

const context = createContext<readonly [HostStates, HostActions] | undefined>(undefined);

const { Provider } = context;

export const HostProvider: FC<PropsWithChildren<HostProps>> = ({ children, type }) => {
  const actions: HostActions = useConst(() => ({}));

  const value = useMemo<readonly [HostStates, HostActions]>(
    () => [
      {
        type,
      },
      actions,
    ],
    [type, actions],
  );

  return <Provider value={value}>{children}</Provider>;
};

export const useHost = (): readonly [HostStates, HostActions] => {
  const value = useContext(context);
  if (!value) {
    throw new Error('useHost must be used within a HostProvider');
  }
  return value;
};


================================================================================
File: src/frontend/src/services/http-common.ts
Size: 405 B
================================================================================

export interface HttpCommonOptions {
  /**
   * The URL to fetch.
   */
  url: string | URL;

  /**
   * The HTTP method to fetch.
   * @default "GET"
   */
  method?: "GET" | "POST";

  /**
   * The callback function to process the data after fetch.
   * @param data The data of the fetch response.
   * @returns The new reference data that has been processed.
   */
  afterFetch?: (data: any) => any;
}


================================================================================
File: src/frontend/src/services/http-sse.ts
Size: 0 B
================================================================================



================================================================================
File: src/frontend/src/services/http-stream.ts
Size: 8.07 kB
================================================================================

import type { HttpCommonOptions } from './http-common';

export type HttpStreamClientStatus = 'connecting' | 'connected' | 'disconnected' | 'error';

export interface HttpStreamClientOptions {
  debugName?: string;

  /**
   * Whether the client can re-connect automatically when close or error.
   * @default false
   */
  autoReconnect?: boolean;

  /**
   * The interval time (ms) of auto-reconnect.
   * @default 2000
   */
  autoReconnectInterval?: number;

  /**
   * The callback function to handle the connection status change.
   * @param status The status of the client.
   */
  onStatusChange?: (status: HttpStreamClientStatus) => void;

  /**
   * The callback function to handle the message.
   * @param message The message data.
   */
  onMessage?: (message: any) => void;
  onError?: (error: unknown) => void;
}

export interface HttpStreamClientSendOptions {
  searchParams?: Record<string, string>;
  headers?: Record<string, string>;
  data?: any;
  body?: string | Blob | ArrayBuffer | ArrayBufferView;
}

export interface HttpStreamClient {
  /**
   * Connect HTTP stream and send data to the server.
   * @param data The data to send.
   */
  send: (data?: HttpStreamClientSendOptions) => void;

  /**
   * Abort the HTTP stream.
   */
  abort: () => void;
}

/**
 * Factory for stream client via HTTP (NDJSON format).
 */
export const createHttpStreamFactory =
  (commonOptions: HttpCommonOptions) =>
  (options: HttpStreamClientOptions): HttpStreamClient => {
    const { url: _url, method = 'GET', afterFetch = (data) => data } = commonOptions;
    const {
      debugName,
      autoReconnect = false,
      autoReconnectInterval = 2000,
      onStatusChange: onStatusChangeCallback,
      onMessage: onMessageCallback,
      onError: onErrorCallback,
    } = options;

    const decoder = new TextDecoder();
    let abortController: AbortController | undefined;
    let reader: ReadableStreamDefaultReader | undefined;

    const debugLog = (...args: any[]) => {
      console.log(
        `%c http-stream.ts ${method} ${_url} ${debugName ?? ''}`,
        'color: white; background: darkcyan; padding: 2px 4px; border-radius: 2px;',
        ...args,
      );
    };

    const onStatusChange = (status: HttpStreamClientStatus): void => {
      debugLog('onStatusChange', status);
      if (autoReconnect && (status === 'disconnected' || status === 'error')) {
        setTimeout(() => {
          debugLog('auto-reconnect');
          send(previousSendOptions);
        }, autoReconnectInterval);
      }
      try {
        onStatusChangeCallback?.(status);
      } catch (error) {
        debugLog('onStatusChange Error', error);
      }
    };

    const onMessage = (message: any): void => {
      // debugLog(
      //   `onMessage ${message.type}`,
      //   ...(Array.isArray(message.data) ? message.data : [message.data])
      // );
      try {
        onMessageCallback?.(message);
      } catch (error) {
        debugLog('onMessage Error', error);
      }
    };

    const cleanupReader = async () => {
      try {
        if (reader) {
          await reader.cancel();
          reader = undefined;
        }
      } catch (err) {
        debugLog('reader cancel error', err);
      }
    };

    let previousSendOptions: HttpStreamClientSendOptions | undefined;
    const send = (options?: HttpStreamClientSendOptions) => {
      previousSendOptions = options;
      const { headers, body, data } = options || {};

      onStatusChange('connecting');
      debugLog('send', data);

      abortController = new AbortController();
      let timeoutId: ReturnType<typeof setTimeout> | undefined;

      const isQueueInfo = data?.type === 'queue_info';
      const timeoutMs = isQueueInfo ? 0 : 5000;

      if (timeoutMs > 0) {
        timeoutId = setTimeout(() => {
          debugLog(`fetch timeout: triggering abort after ${timeoutMs}ms`);
          try {
            abortController?.abort();
          } catch (err) {
            debugLog('abort error during timeout', err);
          }
          onStatusChange('error');
          onErrorCallback?.(new Error('The request timed out. Please try again later.'));
        }, timeoutMs);
      }

      fetch(_url, {
        headers: {
          'Content-Type': 'application/json',
          ...headers,
        },
        method,
        body: (data && JSON.stringify(data)) || body,
        signal: abortController.signal,
      })
        .then(async (response) => {
          if (timeoutId) {
            clearTimeout(timeoutId);
            timeoutId = undefined;
          }

          if (
            response.status !== 200
            || response.headers.get('content-type') !== 'application/x-ndjson'
          ) {
            debugLog('fetch status error or content-type error', response);
            onStatusChange('error');
            return;
          }

          reader = response.body?.getReader();
          if (!reader) {
            debugLog('getReader error', response);
            onStatusChange('error');
            return;
          }

          onStatusChange('connected');

          let buffer = '';
          while (true) {
            const { done, value } = await reader.read();

            if (done) {
              await cleanupReader();
              onStatusChange('disconnected');
              break;
            }

            const chunk = decoder.decode(value);
            buffer += chunk;
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            lines.forEach((line) => {
              try {
                const message = afterFetch(JSON.parse(line));
                onMessage(message);
              } catch (error) {
                debugLog('Parse Message Error', error);
              }
            });
          }
        })
        .catch(async (error) => {
          if (timeoutId) {
            clearTimeout(timeoutId);
            timeoutId = undefined;
          }

          await cleanupReader();
          debugLog('fetch error', error);
          onStatusChange('error');
          onErrorCallback?.(error);
        })
        .finally(async () => {
          if (timeoutId) {
            clearTimeout(timeoutId);
            timeoutId = undefined;
          }

          await cleanupReader();
          abortController = undefined;
        });
    };

    const abort = () => {
      try {
        abortController?.abort();
        abortController = undefined;
      } catch (error) {
        debugLog('abort error', error);
      }
      reader?.cancel();
      reader = undefined;
      onStatusChange('disconnected');
    };

    const instance: HttpStreamClient = { send, abort };
    return Object.freeze(instance);
  };

// export const createEventSourceClient = (options: HttpStreamClientOptions): HttpStreamClient => {
//   const { url, onMessage: onMessageCallback, onStatusChange: onStatusChangeCallback } = options;

//   let client: EventSource | undefined = undefined;

//   const onStatusChange = (status: HttpStreamClientStatus) => {
//     debugLog("onStatusChange", status);
//     onStatusChangeCallback?.(status);
//   };

//   const onMessage = (event: MessageEvent) => {
//     try {
//       const data = JSON.parse(event.data);
//       debugLog("onMessage", data);
//       onMessageCallback?.(data);
//     } catch (error) {
//       debugLog("onMessage Error", error);
//     }
//   };

//   const onOpen = () => {
//     onStatusChange('connected');
//   };

//   const onError = (event: Event) => {
//     onStatusChange('error');
//   };

//   const connect = () => {
//     if (client) {
//       return;
//     }

//     client = new EventSource(url);

//     client.onmessage = onMessage;
//     client.onopen = onOpen;
//     client.onerror = onError;
//     onStatusChange('connecting');
//   };

//   const disconnect = () => {
//     if (!client) {
//       return;
//     }

//     client.close();
//     client = undefined;
//     onStatusChange('disconnected');
//   };

//   return {
//     connect,
//     disconnect,
//   };
// }


================================================================================
File: src/frontend/src/services/index.ts
Size: 75 B
================================================================================

export * from './chat';
export * from './cluster';
export * from './host';


================================================================================
File: src/frontend/src/services/temp.chat.101301.json
Size: 12.39 kB
================================================================================

[
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": "assistant", "content": "" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "Sure" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "!" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " Here" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "'s" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " a" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " light" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "-hearted" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " one" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " for" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " you" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": ":\n\n" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "Why" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " don" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "'t" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " skeletons" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " fight" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " each" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " other" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "?\n\n" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "Because" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " they" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " don" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "\u2019t" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " have" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " the" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " *" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "g" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "uts" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "*" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": "!" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": null,
        "matched_stop": null,
        "delta": { "role": null, "content": " \ud83d\ude04" }
      }
    ],
    "usage": null
  },
  {
    "id": "f03cc5bf-cd69-4f93-9da5-46daec27f5a9",
    "object": "chat.completion.chunk",
    "model": "default",
    "created": 1760346524.2859468,
    "choices": [
      {
        "index": 0,
        "logprobs": null,
        "finish_reason": "stop",
        "matched_stop": 0,
        "delta": { "role": null, "content": null }
      }
    ],
    "usage": null
  }
]


================================================================================
File: src/frontend/src/themes/components/constants.ts
Size: 324 B
================================================================================

export const INPUT_SIZE_REM_MAP = {
  small: 2.25,
  medium: 2.75,
  large: 3.5,
} as const;

export const INPUT_SIZE_PADDING_REM_MAP = {
  small: 0.75,
  medium: 1,
  large: 1.25,
} as const;

export const INPUT_ICON_SIZE_REM_MAP = {
  small: 1,
  medium: 1.25,
  large: 1.5,
} as const;

export const INPUT_RADIUS = 0.38;


================================================================================
File: src/frontend/src/themes/components/css.tsx
Size: 716 B
================================================================================

import GeistMonoTTF from '../../assets/fonts/GeistMono.ttf';
import InterTTF from '../../assets/fonts/Inter.ttf';
import FKGroteskNeueWoff2 from '../../assets/fonts/FKGroteskNeue.woff2';

import type { Components, Theme } from '@mui/material';

export const MuiCssBaseline = (theme: Theme): Components<Theme>['MuiCssBaseline'] => {
  return {
    styleOverrides: `
      @font-face {
        font-family: 'GeistMono';
        src: url(${GeistMonoTTF}) format('ttf');
      }
      @font-face {
        font-family: 'Inter';
        src: url(${InterTTF}) format('ttf');
      }
      @font-face {
        font-family: 'FK Grotesk Neue';
        src: url(${FKGroteskNeueWoff2}) format('woff2');
      }
    `,
  };
};


================================================================================
File: src/frontend/src/themes/components/data-display/chip.ts
Size: 2.42 kB
================================================================================

import type { ChipProps, Components, Theme, Palette, PaletteColor } from '@mui/material';

const SIZE_PADDING_INLINE_REM_MAP: Record<NonNullable<ChipProps['size']>, number> = {
  small: 0.625,
  medium: 0.75,
};

const SIZE_PADDING_RADIUS_REM_MAP: Record<NonNullable<ChipProps['size']>, number> = {
  small: 0.375,
  medium: 0.25,
};

const COLORS: readonly NonNullable<ChipProps['color']>[] = [
  'secondary',
  'info',
  'brand',
  'primary',
  'error',
  'success',
  'warning',
];

export const MuiChip = (theme: Theme): Components<Theme>['MuiChip'] => {
  const containedColorVariants = COLORS.map((color) => {
    const paletteColor = theme.palette[color as keyof Palette] as PaletteColor;
    return (['filled', 'outlined'] as NonNullable<ChipProps['variant']>[]).map((variant) => ({
      props: { variant: variant as ChipProps['variant'], color },
      style: {
        backgroundColor: paletteColor.chip,
        color: paletteColor.chipText,
        // '&:hover': {
        //   backgroundColor: paletteColor.light,
        // },
        // '&:active': {
        //   backgroundColor: paletteColor.main,
        // },
        // '&.Mui-disabled': {
        //   color: theme.palette.text.disabled,
        //   borderColor: paletteColor.lighter,
        //   backgroundColor: paletteColor.lighter,
        // },
      },
    }));
  }).flat(1);

  return {
    defaultProps: {
      variant: 'filled',
    },
    styleOverrides: {
      root: {
        fontWeight: 700,
        letterSpacing: 0,
        textTransform: 'capitalize',
        variants: [...containedColorVariants],
      },
      deleteIcon: {
        marginLeft: '0.375rem',
        marginRight: 0,
        color: theme.palette.grey[700],
        '&:hover': {
          color: theme.palette.grey[700],
        },
      },
      sizeSmall: {
        minWidth: `auto`,
        height: '1.625rem',
        paddingInline: `${SIZE_PADDING_INLINE_REM_MAP.small}rem`,
        borderRadius: `${SIZE_PADDING_RADIUS_REM_MAP.small}rem`,
        '.MuiChip-label': {
          padding: '0 0',
        },
        fontSize: theme.typography.body2.fontSize,
      },
      sizeMedium: {
        minWidth: `auto`,
        height: '1.375rem',
        paddingInline: `${SIZE_PADDING_INLINE_REM_MAP.medium}rem`,
        borderRadius: `${SIZE_PADDING_RADIUS_REM_MAP.medium}rem`,
        '.MuiChip-label': {
          padding: '0 0',
        },
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/data-display/divider.ts
Size: 523 B
================================================================================

import type { Components, Theme } from '@mui/material';
import { INPUT_ICON_SIZE_REM_MAP } from '../constants';

export const MuiDivider = (theme: Theme): Components<Theme>['MuiDivider'] => {
  const { spacing } = theme;

  return {
    styleOverrides: {
      root: {
        variants: [
          {
            props: { variant: 'inset' },
            style: {
              margin: 0,
              marginInlineStart: `${INPUT_ICON_SIZE_REM_MAP.small}rem`,
            },
          },
        ],
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/data-display/index.ts
Size: 130 B
================================================================================

export * from './chip';
export * from './divider';
export * from './list';
export * from './table';
export * from './typography';


================================================================================
File: src/frontend/src/themes/components/data-display/list.ts
Size: 3.09 kB
================================================================================

import type { Components, Theme } from '@mui/material';
import { INPUT_ICON_SIZE_REM_MAP, INPUT_SIZE_REM_MAP } from '../constants';

export const MuiList = (theme: Theme): Components<Theme>['MuiList'] => {
  const { palette, typography, spacing } = theme;

  return {
    defaultProps: {
      disablePadding: true,
    },
    styleOverrides: {
      root: {
        ...typography.subtitle2,
        fontWeight: typography.fontWeightMedium,
        color: palette.text.primary,

        display: 'flex',
        flexFlow: 'column nowrap',
        justifyContent: 'flex-start',
        alignItems: 'stretch',
        gap: spacing(0.5),

        variants: [
          {
            props: ({ ownerState }) => !ownerState.disablePadding,
            style: {
              paddingTop: spacing(1),
              paddingBottom: spacing(1),
            },
          },
        ],
      },
    },
  };
};

export const MuiListItem = (theme: Theme): Components<Theme>['MuiListItem'] => {
  const { spacing } = theme;
  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        padding: spacing(1),
        gap: spacing(0.75),
        variants: [
          {
            props: { disablePadding: true },
            style: {
              paddingBlock: 0,
            },
          },
        ],
      },
    },
  };
};

export const MuiListItemButton = (theme: Theme): Components<Theme>['MuiListItemButton'] => {
  const { palette, spacing } = theme;
  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        padding: spacing(1),
        gap: spacing(0.75),
        '&:hover': {
          backgroundColor: palette.grey[100],
        },
      },
    },
  };
};

export const MuiListItemIcon = (theme: Theme): Components<Theme>['MuiListItemIcon'] => {
  const { palette } = theme;
  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        fontSize: `${INPUT_ICON_SIZE_REM_MAP.small}rem`,
        minWidth: `${INPUT_ICON_SIZE_REM_MAP.small}rem`,
        color: palette.text.secondary,
        '& .tabler-icon': {
          width: '1em',
          height: '1em',
        },
      },
    },
  };
};

export const MuiListItemText = (theme: Theme): Components<Theme>['MuiListItemText'] => {
  const { palette, spacing, typography } = theme;
  return {
    defaultProps: {
      slotProps: {
        primary: {
          component: 'span',
          variant: 'subtitle2',
          fontWeight: 'medium',
          color: 'text.primary',
        },
        secondary: {
          component: 'span',
          variant: 'body2',
          fontWeight: 'medium',
          color: 'grey.600',
        },
      },
    },
    styleOverrides: {
      root: {
        margin: 0,
        variants: [
          {
            props: { inset: true },
            style: {
              padding: 0,
              paddingInlineStart: spacing(3),
            },
          },
        ],
      },
    },
  };
};

export const MuiListItemAvatar = (theme: Theme): Components<Theme>['MuiListItemAvatar'] => {
  return {
    defaultProps: {},
    styleOverrides: {},
  };
};


================================================================================
File: src/frontend/src/themes/components/data-display/table.ts
Size: 2.74 kB
================================================================================

import type { Components, Theme } from '@mui/material';
import { Paper, tableCellClasses, tableRowClasses } from '@mui/material';

export const MuiTableContainer = (theme: Theme): Components<Theme>['MuiTableContainer'] => {
  return {
    defaultProps: {
      component: Paper,
    },
    styleOverrides: {},
  };
};

export const MuiTable = (theme: Theme): Components<Theme>['MuiTable'] => {
  return {
    defaultProps: {},
    styleOverrides: {},
  };
};

export const MuiTableHead = (theme: Theme): Components<Theme>['MuiTableHead'] => {
  const { palette } = theme;
  return {
    defaultProps: {},
    styleOverrides: {
      root: {},
    },
  };
};

export const MuiTableBody = (theme: Theme): Components<Theme>['MuiTableBody'] => {
  return {
    defaultProps: {},
    styleOverrides: {
      root: {},
    },
  };
};

export const MuiTableRow = (theme: Theme): Components<Theme>['MuiTableRow'] => {
  const { palette } = theme;

  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        [`&:hover, &.${tableRowClasses.hover}:hover`]: {
          backgroundColor: palette.grey[100],
        },
        [`&.${tableRowClasses.selected}`]: {
          backgroundColor: palette.grey[50],
          '&:hover': {
            backgroundColor: palette.grey[200],
          },
        },
        [`&:last-child>.${tableCellClasses.body}`]: {
          border: 0,
        },
      },
    },
  };
};

export const MuiTableCell = (theme: Theme): Components<Theme>['MuiTableCell'] => {
  const { palette, typography } = theme;

  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        padding: '1rem',
        borderBottom: `1px solid ${palette.divider}`,
        variants: [
          {
            props: { variant: 'head' },
            style: {
              backgroundColor: palette.background.area,
            },
          },
          {
            props: ({ ownerState }) => !!ownerState.stickyHeader,
            style: {
              backgroundColor: palette.background.area,
            },
          },
        ],
      },
    },
  };
};

export const MuiTableFooter = (theme: Theme): Components<Theme>['MuiTableFooter'] => {
  return {
    defaultProps: {},
    styleOverrides: {},
  };
};

export const MuiTablePagination = (theme: Theme): Components<Theme>['MuiTablePagination'] => {
  return {
    defaultProps: {},
    styleOverrides: {},
  };
};

export const MuiTablePaginationActions = (
  theme: Theme,
): Components<Theme>['MuiTablePaginationActions'] => {
  return {
    defaultProps: {},
    styleOverrides: {},
  };
};

export const MuiTableSortLabel = (theme: Theme): Components<Theme>['MuiTableSortLabel'] => {
  return {
    defaultProps: {},
    styleOverrides: {},
  };
};


================================================================================
File: src/frontend/src/themes/components/data-display/typography.ts
Size: 648 B
================================================================================

import type { Components, Theme } from '@mui/material';

declare module '@mui/material' {
  interface TypographyPropsVariantOverrides {
    pre: true;
  }
}

export const MuiTypography = (theme: Theme): Components<Theme>['MuiTypography'] => {
  return {
    defaultProps: {
      variant: 'inherit',
      color: 'inherit',
      variantMapping: {
        pre: 'pre',
      },
    },
    styleOverrides: {
      root: {
        flex: 'none',
        variants: [
          {
            props: { variant: 'pre' },
            style: {
              fontFamily: 'Geist Mono, monospace',
            },
          },
        ],
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/feedback/alert.tsx
Size: 4.56 kB
================================================================================

import type { AlertProps, Components, Theme } from '@mui/material';
import { alertClasses } from '@mui/material';
import { IconAlertCircle, IconCircleCheck, IconInfoCircle, IconX } from '@tabler/icons-react';

declare module '@mui/material/Alert' {
  interface AlertPropsVariantOverrides {
    notification: true;
  }
}

const COLORS: NonNullable<AlertProps['severity']>[] = ['error', 'warning', 'info', 'success'];

export const MuiAlertTitle = (theme: Theme): Components<Theme>['MuiAlertTitle'] => {
  const { palette, typography } = theme;
  return {
    styleOverrides: {
      root: {
        ...typography.subtitle1,
        padding: 0,
        margin: 0,
        color: palette.text.primary,
      },
    },
  };
};

export const MuiAlert = (theme: Theme): Components<Theme>['MuiAlert'] => {
  const { palette, spacing, typography } = theme;

  return {
    defaultProps: {
      variant: 'standard',
      iconMapping: {
        info: <IconInfoCircle />,
        success: <IconCircleCheck />,
        warning: <IconAlertCircle />,
        error: <IconAlertCircle />,
      },
      slots: {
        closeIcon: () => <IconX fontSize='1.25rem' />,
      },
      slotProps: {
        root: {
          // set paper to outlined
          variant: 'outlined',
        },
      },
    },
    styleOverrides: {
      root: {
        ...typography.subtitle2,
        fontWeight: typography.fontWeightRegular,

        flex: 'none',
        position: 'relative',
        display: 'flex',
        flexFlow: 'row nowrap',
        justifyContent: 'flex-start',
        justifyItems: 'flex-start',
        alignItems: 'center',
        gap: '0.75rem',
        padding: 0,
        paddingBlock: '0.75rem',
        paddingInline: '0.75rem 1.25rem',
        overflow: 'hidden',

        variants: [
          ...COLORS.map((color) => ({
            props: { variant: 'outlined' as const, severity: color },
            style: {
              color: palette.text.secondary,
              backgroundColor: palette.background.default,
              borderColor: palette.divider,
            },
          })),
          ...COLORS.map((color) => ({
            props: { variant: 'standard' as const, severity: color },
            style: {
              alignItems: 'flex-start',
              gap: spacing(0.5),

              color: (color === 'info' && palette.text.disabled) || palette[color].main,
              backgroundColor:
                (color === 'info' && palette.background.area) || palette[color].lighter,
              border: 'none',

              [`& .${alertClasses.icon}`]: {
                color: 'inherit',
              },
            },
          })),
          {
            props: { variant: 'notification' },
            style: {
              flexWrap: 'wrap',
              justifyContent: 'space-between',
              gap: 0,
              padding: 0,
              paddingBlock: 0,
              paddingInline: 0,
              alignItems: 'stretch',
            },
          },
        ],
      },
      icon: {
        fontSize: 'inherit',
        lineHeight: 'inherit',

        flex: 'none',
        width: '1em',
        height: '1lh',

        display: 'inline-flex',
        alignItems: 'center',
        marginRight: 0,
        padding: 0,

        '& .tabler-icon': {
          width: '1em',
          height: '1em',
        },

        variants: [
          {
            props: { variant: 'notification' },
            style: {
              order: 1,
              padding: '1rem',
            },
          },
        ],
      },
      message: {
        flex: 1,
        display: 'inline-flex',
        flexFlow: 'column nowrap',
        justifyContent: 'flex-start',
        alignItems: 'stretch',
        gap: '0.25rem',
        padding: 0,
        margin: 0,
        variants: [
          {
            props: { variant: 'notification' },
            style: {
              order: 3,
              flex: '2 0 100%',
              padding: '0 1rem 1rem',
            },
          },
        ],
      },
      action: {
        flex: 'none',
        display: 'inline-flex',
        flexFlow: 'row nowrap',
        alignItems: 'center',
        justifyContent: 'flex-end',
        gap: '0.75rem',
        padding: 0,
        margin: 0,
        marginInlineStart: 'auto',
        color: palette.grey[700],
        variants: [
          {
            props: { variant: 'notification' },
            style: {
              order: 2,
              padding: '1rem',
            },
          },
        ],
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/feedback/backdrop.ts
Size: 410 B
================================================================================

import type { Components, Theme } from '@mui/material';

export const MuiBackdrop = (theme: Theme): Components<Theme>['MuiBackdrop'] => {
  return {
    styleOverrides: {
      root: {
        backgroundColor: 'rgba(25, 24, 24, 0.5)',
        variants: [
          {
            props: { invisible: true },
            style: { backgroundColor: 'transparent' },
          },
        ],
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/feedback/dialog.ts
Size: 2.93 kB
================================================================================

import type { Components, Theme } from '@mui/material';
import { dialogContentClasses, dialogTitleClasses, Stack } from '@mui/material';

export const MuiDialogActions = (theme: Theme): Components<Theme>['MuiDialogActions'] => {
  return {
    styleOverrides: {
      root: {
        padding: '1rem',
        [`.${dialogContentClasses.root}:not(.${dialogContentClasses.dividers}) + &`]: {
          marginTop: '-0.25rem',
        },
        variants: [
          {
            props: ({ ownerState }) => !ownerState.disableSpacing,
            style: {
              // gap: '0.75rem',
              '& > :not(style) ~ :not(style)': {
                marginLeft: 0,
              },
            },
          },
        ],
      },
    },
  };
};

export const MuiDialogTitle = (theme: Theme): Components<Theme>['MuiDialogTitle'] => {
  const { typography } = theme;
  return {
    styleOverrides: {
      root: {
        display: 'flex',
        flexFlow: 'row nowrap',
        justifyContent: 'space-between',
        alignContent: 'flex-start',
        alignItems: 'center',

        gap: '0.75rem',
        padding: '1rem',
        ...typography.subtitle1,
      },
    },
  };
};

export const MuiDialogContent = (theme: Theme): Components<Theme>['MuiDialogContent'] => {
  const { palette } = theme;

  return {
    styleOverrides: {
      root: {
        display: 'flex',
        flexFlow: 'column nowrap',
        justifyContent: 'flex-start',
        alignItems: 'stretch',

        gap: '0.5rem',
        padding: '1rem',

        variants: [
          {
            props: ({ ownerState }) => !!ownerState.dividers,
            style: {
              padding: '1rem',
              borderTop: `1px solid ${palette.divider}`,
              borderBottom: `1px solid ${palette.divider}`,
            },
          },
          {
            props: ({ ownerState }) => !ownerState.dividers,
            style: {
              [`.${dialogTitleClasses.root} + &`]: {
                paddingTop: 0,
              },
              [`& + &`]: {
                paddingTop: 0,
                marginTop: '-0.25rem',
              },
            },
          },
        ],
      },
    },
  };
};

export const MuiDialogContentText = (theme: Theme): Components<Theme>['MuiDialogContentText'] => {
  const { palette, spacing, typography } = theme;
  return {
    defaultProps: {
      component: Stack,
    },
    styleOverrides: {
      root: {
        gap: spacing(1),

        color: palette.text.primary,
        ...typography.body2,
      },
    },
  };
};

export const MuiDialog = (theme: Theme): Components<Theme>['MuiDialog'] => {
  const { palette } = theme;
  return {
    defaultProps: {
      slotProps: {
        paper: {
          variant: 'overlay',
          style: {
            backgroundColor: palette.background.default,
          },
        },
      },
    },
    styleOverrides: {
      root: {},
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/feedback/index.ts
Size: 164 B
================================================================================

export * from './alert';
export * from './backdrop';
export * from './dialog';
export * from './progress';
export * from './snackbar';
export * from './notistack';


================================================================================
File: src/frontend/src/themes/components/feedback/notistack.tsx
Size: 2.65 kB
================================================================================

import type { FC, PropsWithChildren, ReactNode } from 'react';
import { forwardRef } from 'react';
import type { InternalSnack, SnackbarProviderProps } from 'notistack';
import { SnackbarProvider, useSnackbar } from 'notistack';
import { Notification } from '../../../components/mui';

declare module 'notistack' {
  export interface OptionsObject<V extends VariantType = VariantType> extends SharedProps<V> {
    title?: ReactNode;
    message?: ReactNode;

    /**
     * Set the variant of alert to notification.
     * It has header with icon and close button, title and content,
     * bottom action buttons and progress bar.
     * @default true
     */
    notification?: boolean;

    /**
     * Show the close button in the header or not.
     * @default depends on the prop notification
     */
    closable?: boolean;

    /**
     * Show the dismiss action button or not.
     */
    dismissAble?: boolean;
  }
}

const Simple = forwardRef<HTMLDivElement, InternalSnack>((props, ref) => {
  const {
    id,
    variant,
    title,
    message,
    action: propsAction,
    notification = true,
    closable = notification,
    dismissAble,
    persist,
    autoHideDuration: propsAutoHideDuration,
    hideIconVariant,
    className,
    style,
  } = props;

  const { closeSnackbar } = useSnackbar();

  const severity = variant === 'default' ? 'info' : variant;
  const action = typeof propsAction === 'function' ? propsAction(id) : propsAction;
  const onClose = (closable && (() => closeSnackbar(id))) || undefined;
  const onDismiss = (dismissAble && (() => closeSnackbar(id))) || undefined;
  const autoHideDuration = propsAutoHideDuration || undefined;

  return (
    <Notification
      variant={(notification && 'notification') || 'outlined'}
      severity={severity}
      icon={hideIconVariant ? false : undefined}
      onClose={onClose}
      onDismiss={onDismiss}
      action={action}
      title={title}
      autoHideDuration={autoHideDuration}
      ref={ref}
      className={className}
      style={style}
    >
      {message}
    </Notification>
  );
});

const Components: SnackbarProviderProps['Components'] = {
  default: Simple,
  info: Simple,
  success: Simple,
  warning: Simple,
  error: Simple,
};

const Provider: FC<PropsWithChildren> = ({ children }) => {
  return (
    <SnackbarProvider
      anchorOrigin={{
        vertical: 'bottom',
        horizontal: 'right',
      }}
      maxSnack={5}
      autoHideDuration={5000}
      Components={Components}
      style={{
        width: '21.25rem',
      }}
    >
      {children}
    </SnackbarProvider>
  );
};

export { Provider as SnackbarProvider };


================================================================================
File: src/frontend/src/themes/components/feedback/progress.ts
Size: 1.05 kB
================================================================================

import type { Components, Theme } from '@mui/material';
import type { LinearProgressProps } from '@mui/material';

declare module '@mui/material/LinearProgress' {
  interface LinearProgressPropsColorOverrides {
    brand: true;
  }
}

const COLORS: NonNullable<LinearProgressProps['color']>[] = [
  'primary',
  'secondary',
  'brand',
  'info',
  'error',
  'success',
  'warning',
];

export const MuiLinearProgress = (theme: Theme): Components<Theme>['MuiLinearProgress'] => {
  const { palette } = theme;
  return {
    styleOverrides: {
      root: {
        height: '0.375rem',
        borderRadius: '0.1875rem',
        variants: [
          ...COLORS.map((color) => ({
            props: { color },
            style: { backgroundColor: palette.grey[300] },
          })),
        ],
      },
      bar1: {
        variants: [
          ...COLORS.map((color) => ({
            props: { color },
            style: { backgroundColor: color === 'inherit' ? 'currentColor' : palette[color].main },
          })),
        ],
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/feedback/snackbar.ts
Size: 768 B
================================================================================

import type { Components, Theme } from '@mui/material';

export const MuiSnackbarContent = (theme: Theme): Components<Theme>['MuiSnackbarContent'] => {
  const { palette, typography } = theme;

  return {
    defaultProps: {
      variant: 'overlay',
    },
    styleOverrides: {
      root: {
        ...typography.subtitle1,
        color: palette.text.primary,
        backgroundColor: palette.background.default,
        padding: 0,
      },
      message: {
        padding: 0,
      },
    },
  };
};

export const MuiSnackbar = (theme: Theme): Components<Theme>['MuiSnackbar'] => {
  return {
    defaultProps: {
      anchorOrigin: {
        vertical: 'bottom',
        horizontal: 'right',
      },
    },
    styleOverrides: {
      root: {},
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/button.ts
Size: 10.68 kB
================================================================================

import type { ButtonProps, Components, Palette, PaletteColor, Theme } from '@mui/material';
import { buttonClasses } from '@mui/material';
import { INPUT_ICON_SIZE_REM_MAP, INPUT_RADIUS, INPUT_SIZE_REM_MAP } from '../constants';

declare module '@mui/material/Button' {
  interface ButtonPropsColorOverrides {
    brand: true;
  }

  interface ButtonPropsVariantOverrides {
    containedRounded: true;
    containedSquare: true;
    containedCircle: true;
  }
}

declare module '@mui/material/IconButton' {
  interface IconButtonPropsColorOverrides {
    brand: true;
  }

  interface IconButtonPropsSizeOverrides {
    /**
     * Set the size to 1rem (the font size of root element).
     */
    mini: true;

    /**
     * Set the size to `1em` (the font size of parent element).
     */
    em: true;

    /**
     * Set the size to `1lh` (the line height of parent element).
     */
    lh: true;
  }
}

const COLORS: readonly NonNullable<ButtonProps['color']>[] = [
  'brand',
  'primary',
  'secondary',
  'error',
  'info',
  'success',
  'warning',
];

const SIZES: readonly NonNullable<ButtonProps['size']>[] = ['small', 'medium', 'large'];

const SIZE_REM_MAP: Record<NonNullable<ButtonProps['size']>, number> = INPUT_SIZE_REM_MAP;

const MIN_WIDTH_MULTIPLY = 3 / 2.25;

const SIZE_PADDING_INLINE_REM_MAP: Record<NonNullable<ButtonProps['size']>, number> = {
  small: 0.875,
  medium: 1.125,
  large: 1.5,
};

export const MuiButton = (theme: Theme): Components<Theme>['MuiButton'] => {
  const { overlays } = theme;

  const colorShadowMap: Partial<Record<NonNullable<ButtonProps['color']>, string>> = {
    primary: overlays.buttonShadeActiveDark,
    secondary: overlays.buttonShadeActiveLight,
    brand: overlays.buttonShadeBrand,
    error: overlays.buttonShadeError,
  };

  const containedBoxShadowVariants = COLORS.map((color) =>
    (
      ['contained', 'containedRounded', 'containedSquare', 'containedCircle'] as NonNullable<
        ButtonProps['variant']
      >[]
    ).map((variant) => ({
      props: { variant, color },
      style: {
        boxShadow: overlays.buttonShadeDefault,
        '&:hover': {
          boxShadow: overlays.buttonShadeDefault,
        },
        '&:active': {
          boxShadow: colorShadowMap[color] || colorShadowMap.secondary,
        },
        '&.Mui-disabled': {
          boxShadow: overlays.buttonShadeDefault,
        },
      },
    })),
  ).flat(1);

  const containedColorVariants = COLORS.map((color) => {
    const paletteColor = theme.palette[color as keyof Palette] as PaletteColor;
    return (
      ['contained', 'containedRounded', 'containedSquare', 'containedCircle'] as NonNullable<
        ButtonProps['variant']
      >[]
    ).map((variant) => ({
      props: { variant: variant as ButtonProps['variant'], color },
      style: {
        color: paletteColor.contrastText,
        border: '1px solid',
        borderColor: paletteColor.darker,
        backgroundColor: paletteColor.main,
        '&:hover': {
          backgroundColor: paletteColor.light,
        },
        '&:active': {
          backgroundColor: paletteColor.main,
        },
        '&.Mui-disabled': {
          color: theme.palette.text.disabled,
          borderColor: paletteColor.lighter,
          backgroundColor: paletteColor.lighter,
        },
      },
    }));
  }).flat(1);

  const iconFontSizeVariants = SIZES.map((size) => ({
    props: { size },
    style: {
      '&>*:nth-of-type(1)': {
        fontSize: `${INPUT_ICON_SIZE_REM_MAP[size]}rem`,
      },
    },
  }));

  return {
    defaultProps: {
      size: 'small',
      variant: 'contained',

      disableRipple: true,
    },
    styleOverrides: {
      root: {
        gap: '0.25rem',
        borderRadius: `${INPUT_RADIUS}rem`,

        fontWeight: 400,
        letterSpacing: 0,
        textTransform: 'none',

        variants: [...containedBoxShadowVariants, ...containedColorVariants],

        [`&.${buttonClasses.text}`]: {
          fontFamily: 'inherit',
          fontWeight: 'inherit',
          fontSize: 'inherit',
          lineHeight: 'inherit',
          color: 'inherit',
          background: 'transparent',
          '&:hover': {
            background: 'transparent',
          },
        },
      },

      startIcon: {
        marginLeft: 0,
        marginRight: 0,
        '& .tabler-icon': {
          width: '1em',
          height: '1em',
        },
        variants: iconFontSizeVariants,
      },
      endIcon: {
        '& .table-icon': {
          width: '1em',
          height: '1em',
        },
        variants: iconFontSizeVariants,
      },

      sizeSmall: {
        minWidth: `${SIZE_REM_MAP.small * MIN_WIDTH_MULTIPLY}rem`,
        height: `${SIZE_REM_MAP.small}rem`,
        fontSize: theme.typography.subtitle2.fontSize,
        lineHeight: theme.typography.subtitle2.lineHeight,
        padding: 0,
        paddingBlock: 0,
        paddingInline: `${SIZE_PADDING_INLINE_REM_MAP.small}rem`,
        [`&.${buttonClasses.text}`]: {
          minWidth: 0,
          height: '1lh',
          paddingInline: 0,
        },
        '&.MuiButton-containedRounded': {
          borderRadius: `${SIZE_REM_MAP.small / 2}rem`,
        },
        '&.MuiButton-containedSquare': {
          width: `${SIZE_REM_MAP.small}rem`,
          minWidth: `${SIZE_REM_MAP.small}rem`,
          maxWidth: `${SIZE_REM_MAP.small}rem`,
          paddingInline: 0,
        },
        '&.MuiButton-containedCircle': {
          width: `${SIZE_REM_MAP.small}rem`,
          minWidth: `${SIZE_REM_MAP.small}rem`,
          maxWidth: `${SIZE_REM_MAP.small}rem`,
          paddingInline: 0,
          borderRadius: '50%',
        },
      },
      sizeMedium: {
        minWidth: `${SIZE_REM_MAP.medium * MIN_WIDTH_MULTIPLY}rem`,
        height: `${SIZE_REM_MAP.medium}rem`,
        fontSize: theme.typography.h3.fontSize,
        lineHeight: theme.typography.h3.lineHeight,
        padding: 0,
        paddingBlock: 0,
        paddingInline: `${SIZE_PADDING_INLINE_REM_MAP.medium}rem`,
        [`&.${buttonClasses.text}`]: {
          minWidth: 0,
          height: '1lh',
          paddingInline: 0,
        },
        '&.MuiButton-containedRounded': {
          borderRadius: `${SIZE_REM_MAP.medium / 2}rem`,
        },
        '&.MuiButton-containedSquare': {
          width: `${SIZE_REM_MAP.medium}rem`,
          minWidth: `${SIZE_REM_MAP.medium}rem`,
          maxWidth: `${SIZE_REM_MAP.medium}rem`,
          paddingInline: 0,
        },
        '&.MuiButton-containedCircle': {
          width: `${SIZE_REM_MAP.medium}rem`,
          minWidth: `${SIZE_REM_MAP.medium}rem`,
          maxWidth: `${SIZE_REM_MAP.medium}rem`,
          paddingInline: 0,
          borderRadius: '50%',
        },
      },
      sizeLarge: {
        minWidth: `${SIZE_REM_MAP.large * MIN_WIDTH_MULTIPLY}rem`,
        height: `${SIZE_REM_MAP.large}rem`,
        fontSize: theme.typography.h3.fontSize,
        lineHeight: theme.typography.h3.lineHeight,
        padding: 0,
        paddingBlock: 0,
        paddingInline: `${SIZE_PADDING_INLINE_REM_MAP.large}rem`,
        [`&.${buttonClasses.text}`]: {
          minWidth: 0,
          height: '1lh',
          paddingInline: 0,
        },
        '&.MuiButton-containedRounded': {
          borderRadius: `${SIZE_REM_MAP.large / 2}rem`,
        },
        '&.MuiButton-containedSquare': {
          width: `${SIZE_REM_MAP.large}rem`,
          minWidth: `${SIZE_REM_MAP.large}rem`,
          maxWidth: `${SIZE_REM_MAP.large}rem`,
          paddingInline: 0,
        },
        '&.MuiButton-containedCircle': {
          width: `${SIZE_REM_MAP.large}rem`,
          minWidth: `${SIZE_REM_MAP.large}rem`,
          maxWidth: `${SIZE_REM_MAP.large}rem`,
          paddingInline: 0,
          borderRadius: '50%',
        },
      },
    },
  };
};

export const MuiButtonBase = (theme: Theme): Components<Theme>['MuiButtonBase'] => {
  return {
    defaultProps: {
      disableRipple: true,
    },
    styleOverrides: {
      root: {
        borderRadius: '6px',
        textTransform: 'none',
      },
    },
  };
};

export const MuiButtonGroup = (theme: Theme): Components<Theme>['MuiButtonGroup'] => {
  return {
    defaultProps: {
      color: 'primary',
      variant: 'contained',
      size: 'small',
    },
  };
};

export const MuiIconButton = (theme: Theme): Components<Theme>['MuiIconButton'] => {
  const { palette } = theme;

  return {
    defaultProps: {
      size: 'mini',
      color: 'inherit',
    },
    styleOverrides: {
      root: {
        display: 'inline-flex',
        justifyContent: 'center',
        alignItems: 'center',
        padding: 0,
        borderRadius: `${INPUT_RADIUS}rem`,

        textTransform: 'none',

        '&:hover': {
          backgroundColor: palette.grey[200],
          color: palette.grey[800],
        },

        '& .tabler-icon': {
          width: '1em',
          height: '1em',
        },

        variants: [
          {
            props: { size: 'mini' },
            style: {
              width: '1em',
              height: '1em',
              fontSize: '1rem',
              borderRadius: '0.25rem',
            },
          },
          {
            props: { size: 'em' },
            style: {
              width: '1em',
              height: '1em',
              fontSize: 'inherit',
              lineHeight: 'inherit',
              borderRadius: '0.25rem',
            },
          },
          {
            props: { size: 'lh' },
            style: {
              width: '1lh',
              height: '1lh',
              fontSize: '1lh',
              lineHeight: '1lh',
              borderRadius: '0.25rem',
            },
          },
          {
            props: { color: 'primary' },
            style: {
              color: palette.grey[800],
            },
          },
          {
            props: { color: 'secondary' },
            style: {
              color: palette.grey[500],
            },
          },
          {
            props: { color: 'info' },
            style: {
              color: palette.grey[700],
            },
          },
        ],
      },
      sizeSmall: {
        width: `${SIZE_REM_MAP.small}rem`,
        height: `${SIZE_REM_MAP.small}rem`,
        fontSize: `${INPUT_ICON_SIZE_REM_MAP.small}rem`,
      },
      sizeMedium: {
        width: `${SIZE_REM_MAP.medium}rem`,
        height: `${SIZE_REM_MAP.medium}rem`,
        fontSize: `${INPUT_ICON_SIZE_REM_MAP.medium}rem`,
      },
      sizeLarge: {
        width: `${SIZE_REM_MAP.large}rem`,
        height: `${SIZE_REM_MAP.large}rem`,
        fontSize: `${INPUT_ICON_SIZE_REM_MAP.large}rem`,
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/form-control.ts
Size: 532 B
================================================================================

import type { Components, Theme } from '@mui/material';

export const MuiFormControl = (theme: Theme): Components<Theme>['MuiFormControl'] => {
  const { spacing } = theme;
  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        gap: '0.25rem',
      },
    },
  };
};

export const MuiFormLabel = (theme: Theme): Components<Theme>['MuiFormLabel'] => {
  const { typography } = theme;
  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        ...typography.subtitle2,
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/form-helper-text.ts
Size: 384 B
================================================================================

import type { Components, Theme } from '@mui/material';

export const MuiFormHelperText = (theme: Theme): Components<Theme>['MuiFormHelperText'] => {
  const {
    palette: { text },
    typography: { body2 },
  } = theme;

  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        margin: 0,
        ...body2,
        color: text.disabled,
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/index.ts
Size: 343 B
================================================================================

export * from './button';

export * from './form-control';
export * from './form-helper-text';

export * from './input-label';
export * from './input-adornment';
export * from './input-base';
export * from './outline-input';
export * from './text-field';

export * from './select';

export * from './slider';

export * from './toggle-button';


================================================================================
File: src/frontend/src/themes/components/form/input-adornment.ts
Size: 699 B
================================================================================

import type { Components, Theme } from '@mui/material';

export const MuiInputAdornment = (theme: Theme): Components<Theme>['MuiInputAdornment'] => {
  const { palette, typography } = theme;
  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        ...typography.body2,
        color: palette.text.disabled,
        variants: [
          {
            props: {
              position: 'start',
            },
            style: {
              marginRight: 0,
            },
          },
          {
            props: {
              position: 'end',
            },
            style: {
              marginLeft: 0,
            },
          },
        ],
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/input-base.ts
Size: 1.42 kB
================================================================================

import type { Components, InputBaseProps, Theme } from '@mui/material';
import { INPUT_RADIUS, INPUT_SIZE_PADDING_REM_MAP, INPUT_SIZE_REM_MAP } from '../constants';

const SIZES: NonNullable<InputBaseProps['size']>[] = ['small', 'medium'];

export const MuiInputBase = (theme: Theme): Components<Theme>['MuiInputBase'] => {
  const {
    palette: { grey, text },
    spacing,
    typography: { body2 },
  } = theme;

  const sizeVariants = SIZES.map((size) => [
    {
      props: { size },
      style: {
        height: `${INPUT_SIZE_REM_MAP[size]}rem`,
        paddingInline: `${INPUT_SIZE_PADDING_REM_MAP[size]}rem`,
      },
    },
    {
      props: { size, multiline: true },
      style: {
        height: 'auto',
        paddingBlock: `${INPUT_SIZE_PADDING_REM_MAP[size]}rem`,
      },
    },
  ]).flat(1);

  return {
    defaultProps: {
      size: 'small',
    },
    styleOverrides: {
      root: {
        padding: 0,
        borderRadius: `${INPUT_RADIUS}rem !important`,
        gap: spacing(1),
        variants: [...sizeVariants],
      },

      input: {
        ...body2,
        color: text.primary,
        padding: 0,
        textAlign: 'start',

        '&:placeholder': {
          color: grey[400],
        },

        variants: [
          {
            props: { type: 'number' },
            style: {
              textAlign: 'end',
            },
          },
        ],
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/input-label.ts
Size: 705 B
================================================================================

import type { Components, InputLabelOwnProps, Theme } from '@mui/material';

export const MuiInputLabel = (theme: Theme): Components<Theme>['MuiInputLabel'] => {
  const {
    palette: { text },
    typography: { body2 },
  } = theme;
  return {
    defaultProps: {
      shrink: true,
      variant: 'outlined',
    },
    styleOverrides: {
      root: {
        position: 'relative',
        inset: 'unset',
        top: 'unset',
        left: 'unset',
        right: 'unset',
        bottom: 'unset',
        transform: 'none',
        transformOrigin: 'unset',
        maxWidth: 'unset',
        ...body2,
        color: text.secondary,
        backgroundColor: 'transparent',
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/outline-input.ts
Size: 1.9 kB
================================================================================

import type { Components, Theme } from '@mui/material';
import { outlinedInputClasses } from '@mui/material';

export const MuiOutlinedInput = (theme: Theme): Components<Theme>['MuiOutlinedInput'] => {
  const {
    palette: { grey, error, text, divider, background },
    typography: { body2 },
    overlays,
  } = theme;
  return {
    defaultProps: {
      size: 'small',
    },
    styleOverrides: {
      root: {
        backgroundColor: background.default,
        boxShadow: overlays.shadowInput,

        '&:hover': {
          backgroundColor: grey[50],
        },

        [`&.${outlinedInputClasses.focused}`]: {
          boxShadow: overlays.shadowInputActive,
        },

        [`&.${outlinedInputClasses.disabled}`]: {
          '&, &:hover': {
            backgroundColor: grey[200],
          },
        },

        [`&, &.${outlinedInputClasses.focused}, &.${outlinedInputClasses.disabled}`]: {
          '&, &:hover': {
            [`.${outlinedInputClasses.notchedOutline}`]: {
              borderWidth: 1,
              borderColor: divider,
            },
          },
        },

        [`&.${outlinedInputClasses.error}`]: {
          boxShadow: overlays.shadowInputError,
          '&, &:hover': {
            [`.${outlinedInputClasses.notchedOutline}`]: {
              borderWidth: 1,
              borderColor: error.main,
            },
          },
        },
      },

      input: {
        ...body2,
        color: text.primary,
        padding: 0,
      },

      notchedOutline: {
        top: 0,
        left: 0,
        right: 0,
        bottom: 0,

        border: `1px solid`,
        borderColor: divider,

        variants: [
          {
            props: { disabled: true },
            style: {
              borderColor: divider,
            },
          },
        ],

        [`legend`]: {
          display: 'none',
        },
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/select.tsx
Size: 1.47 kB
================================================================================

import type { Components, SelectProps, Theme } from '@mui/material';
import { IconButton } from '@mui/material';
import { IconChevronDown } from '@tabler/icons-react';
import { INPUT_SIZE_PADDING_REM_MAP } from '../constants';

const SIZES: NonNullable<SelectProps['size']>[] = ['small', 'medium'];

export const MuiSelect = (theme: Theme): Components<Theme>['MuiSelect'] => {
  const { palette } = theme;
  return {
    defaultProps: {
      size: 'small',
      variant: 'outlined',
      IconComponent: ({ ownerState, ...props }: { ownerState: SelectProps }) => (
        <IconButton {...(ownerState as any)} {...props}>
          <IconChevronDown />
        </IconButton>
      ),
    },
    styleOverrides: {
      root: {},
      select: {
        // position: 'absolute',
        // inset: 0,
        // top: 0,
        // left: 0,
        // right: 0,
        // bottom: 0,
        // display: 'flex'
      },
      icon: {
        top: '50%',
        transform: 'translateY(-50%)',
        color: palette.text.disabled,

        variants: [
          {
            props: ({ ownerState }: { ownerState: SelectProps }) => !!ownerState.open,
            style: {
              transform: 'translateY(-50%) rotate(180deg)',
            },
          },
          ...SIZES.map((size) => ({
            props: { size },
            style: {
              right: `${INPUT_SIZE_PADDING_REM_MAP[size]}rem`,
            },
          })),
        ],
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/slider.ts
Size: 2.64 kB
================================================================================

import type { Components, Palette, PaletteColor, SliderProps, Theme } from '@mui/material';

declare module '@mui/material/Slider' {
  interface SliderPropsColorOverrides {
    brand: true;
  }
}

const Sizes: readonly NonNullable<SliderProps['size']>[] = ['small', 'medium'];

// unit rem
const SizeMaps: Record<
  NonNullable<SliderProps['size']>,
  {
    thumb: number;
    track: number;
    rail: number;
    padding: number;
  }
> = {
  small: {
    thumb: 0.75,
    track: 0.3125,
    rail: 0.3125,
    padding: 0.125,
  },
  medium: {
    thumb: 1,
    track: 0.625,
    rail: 0.625,
    padding: 0.125,
  },
};

const COLORS: readonly NonNullable<SliderProps['color']>[] = [
  'brand',
  'primary',
  'secondary',
  'error',
  'info',
  'success',
  'warning',
];

export const MuiSlider = (theme: Theme): Components<Theme>['MuiSlider'] => {
  const colorVariants = COLORS.map((color) => {
    const paletteColor = theme.palette[color as keyof Palette] as PaletteColor;
    return Sizes.map((size) => {
      const sizeClassKey = `&.MuiSlider-size${size.charAt(0).toUpperCase() + size.slice(1)}`;
      return {
        props: { color, size },
        style: {
          color: paletteColor.main,
          '& .MuiSlider-thumb': {
            backgroundColor: paletteColor.main,
            '&.Mui-disabled': {
              backgroundColor: theme.palette.grey[250],
              '&::before': {
                boxShadow: 'none',
              },
            },

            '&:hover, &.Mui-focusVisible, &.Mui-active': {
              boxShadow: '0px 0px 0px 0.25rem rgba(5, 170, 108, 0.16)',
            },
          },

          // size
          [sizeClassKey]: {
            '.MuiSlider-thumb': {
              width: `${SizeMaps[size].thumb}rem`,
              height: `${SizeMaps[size].thumb}rem`,
            },
            '.MuiSlider-track': {
              height: `${SizeMaps[size].track - SizeMaps[size].padding * 2}rem`,
              marginLeft: `${SizeMaps[size].padding}rem`,
            },
            '.MuiSlider-rail': {
              height: `${SizeMaps[size].rail}rem`,
              padding: `0 ${SizeMaps[size].padding}rem`,
              backgroundColor: theme.palette.grey[250],
              opacity: 1,
            },
          },
        },
      };
    });
  }).flat(1);

  return {
    defaultProps: {
      size: 'medium',
      color: 'brand',
    },
    styleOverrides: {
      root: {
        paddingBottom: 0,
      },
      thumb: {
        borderWidth: '0.2rem',
        borderStyle: 'solid',
        borderColor: theme.palette.common.white,
      },
    },
    variants: colorVariants,
  };
};


================================================================================
File: src/frontend/src/themes/components/form/text-field.ts
Size: 415 B
================================================================================

import type { Components, OutlinedInputProps, Theme } from '@mui/material';

export const MuiTextField = (theme: Theme): Components<Theme>['MuiTextField'] => {
  return {
    defaultProps: {
      size: 'small',
      variant: 'outlined',
      slotProps: {
        input: {} as Partial<OutlinedInputProps>,
        inputLabel: { shrink: true },
      },
    },
    styleOverrides: {
      root: {},
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/form/toggle-button.ts
Size: 1.61 kB
================================================================================

import { toggleButtonClasses, type Components, type Theme } from '@mui/material';
import { INPUT_SIZE_REM_MAP } from '../constants';

export const MuiToggleButton = (theme: Theme): Components<Theme>['MuiToggleButton'] => {
  const { palette, typography } = theme;

  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        ...typography.subtitle2,

        flex: 1,
        height: `${INPUT_SIZE_REM_MAP.small}rem`,

        borderColor: palette.divider,

        color: palette.grey[400],
        backgroundColor: palette.grey[200],
        '&:hover': {
          color: palette.text.secondary,
          backgroundColor: palette.background.area,
          '@media (hover: none)': {
            backgroundColor: palette.grey[200],
          },
        },
        variants: [
          {
            props: { color: 'standard' },
            style: {
              [`&.${toggleButtonClasses.selected}`]: {
                color: palette.text.primary,
                backgroundColor: palette.background.default,
                '&:hover': {
                  backgroundColor: palette.background.default,
                  // Reset on touch devices, it doesn't add specificity
                  '@media (hover: none)': {
                    backgroundColor: palette.background.default,
                  },
                },
              },
            },
          },
        ],
      },
    },
  };
};

export const MuiToggleButtonGroup = (theme: Theme): Components<Theme>['MuiToggleButtonGroup'] => {
  return {
    defaultProps: {},
    styleOverrides: {
      root: {},
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/index.ts
Size: 205 B
================================================================================

export * from './css';
export * from './form';
export * from './data-display';
export * from './feedback';
export * from './surfaces';
export * from './navigation';

// export * from './date-time-picker';


================================================================================
File: src/frontend/src/themes/components/navigation/index.ts
Size: 24 B
================================================================================

export * from './menu';


================================================================================
File: src/frontend/src/themes/components/navigation/menu.ts
Size: 1.56 kB
================================================================================

import type { Components, Theme } from '@mui/material';
import { dividerClasses, listItemIconClasses } from '@mui/material';
import { INPUT_ICON_SIZE_REM_MAP } from '../constants';

export const MuiMenu = (theme: Theme): Components<Theme>['MuiMenu'] => {
  const { spacing } = theme;

  return {
    defaultProps: {
      slotProps: {
        paper: {
          variant: 'overlay',
        },
        backdrop: {
          invisible: true,
        },
      },
    },
    styleOverrides: {
      root: {
        [`& .${dividerClasses.root}`]: {
          marginBlock: spacing(1.5),
        },
      },
      paper: {
        padding: spacing(1),
      },
    },
  };
};

export const MuiMenuList = (theme: Theme): Components<Theme>['MuiMenuList'] => {
  const { spacing } = theme;

  return {
    defaultProps: {},
    styleOverrides: {},
  };
};

export const MuiMenuItem = (theme: Theme): Components<Theme>['MuiMenuItem'] => {
  const { palette, spacing } = theme;

  return {
    defaultProps: {},
    styleOverrides: {
      root: {
        padding: spacing(1),
        gap: spacing(0.75),
        '&:hover': {
          backgroundColor: palette.grey[200],
        },
        [`& + .${dividerClasses.root}`]: {
          margin: 0,
          marginBlock: spacing(1.5),
        },
        [`& .${dividerClasses.inset}`]: {
          margin: 0,
          marginInline: `${INPUT_ICON_SIZE_REM_MAP.small}rem`,
        },
        [`& .${listItemIconClasses.root}`]: {
          minWidth: `${INPUT_ICON_SIZE_REM_MAP.small}rem`,
        },
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/components/surfaces/index.ts
Size: 25 B
================================================================================

export * from './paper';


================================================================================
File: src/frontend/src/themes/components/surfaces/paper.ts
Size: 958 B
================================================================================

import type { Components, Theme } from '@mui/material';

declare module '@mui/material/Paper' {
  interface PaperPropsVariantOverrides {
    overlay: true;
  }
}

export const MuiPaper = (theme: Theme): Components<Theme>['MuiPaper'] => {
  const { palette, overlays } = theme;

  return {
    defaultProps: {
      variant: 'outlined',
    },
    styleOverrides: {
      root: {
        variants: [
          {
            props: ({ ownerState }) => !ownerState.square,
            style: {
              borderRadius: '0.75rem',
            },
          },
          {
            props: { variant: 'outlined' },
            style: {
              border: `1px solid ${palette.divider}`,
            },
          },
          {
            props: { variant: 'overlay' },
            style: {
              border: `1px solid ${palette.divider}`,
              boxShadow: overlays.shadowMiddle,
            },
          },
        ],
      },
    },
  };
};


================================================================================
File: src/frontend/src/themes/index.tsx
Size: 1.71 kB
================================================================================

'use client';

import type { FC, PropsWithChildren } from 'react';

import type { Theme } from '@mui/material';
import { createTheme, THEME_ID, ThemeProvider } from '@mui/material';

import { LocalizationProvider } from '@mui/x-date-pickers/LocalizationProvider';
import { AdapterDayjs } from '@mui/x-date-pickers/AdapterDayjs';

import { palette } from './palette';
import { HTML_FONT_SIZE, typography } from './typography';
import { overlays } from './shadows';
import * as themeComponents from './components';
import { SnackbarProvider } from './components';

const materialThemeLight = createTheme({
  palette,
  typography,
  spacing: (factor: number) => `${(factor * 8) / HTML_FONT_SIZE}rem`,
  overlays,
});

materialThemeLight.components = materialThemeLight.components || {};

(
  Object.entries(themeComponents) as [
    keyof NonNullable<Theme['components']>,
    (theme: Theme) => NonNullable<Theme['components']>[keyof NonNullable<Theme['components']>],
  ][]
).forEach(
  <K extends keyof NonNullable<Theme['components']>>([compName, generate]: [
    K,
    (theme: Theme) => NonNullable<Theme['components']>[K],
  ]) => {
    if (compName.startsWith('Mui') && typeof generate === 'function') {
      materialThemeLight.components![compName] = generate(materialThemeLight);
    }
  },
);

const Provider: FC<PropsWithChildren> = ({ children }) => {
  return (
    <ThemeProvider theme={{ [THEME_ID]: materialThemeLight }}>
      <SnackbarProvider>
        <LocalizationProvider dateAdapter={AdapterDayjs} localeText={{ okButtonLabel: 'Apply' }}>
          {children}
        </LocalizationProvider>
      </SnackbarProvider>
    </ThemeProvider>
  );
};

export { Provider as ThemeProvider };


================================================================================
File: src/frontend/src/themes/palette.ts
Size: 3.26 kB
================================================================================

import type { Color, PaletteColorOptions, PaletteOptions } from '@mui/material';

declare module '@mui/material' {
  interface Color {
    150: string;
    250: string;
    1000: string;
  }
  interface PaletteColor {
    darker: string;
    lighter: string;
    chip: string;
    chipText: string;
  }
  interface SimplePaletteColorOptions {
    darker: string;
    lighter: string;
    chip: string;
    chipText: string;
  }

  interface Palette {
    brand: PaletteColor;
  }
  interface PaletteOptions {
    brand?: PaletteColorOptions;
  }

  interface TypeBackground {
    /** Use for some small area. */
    area: string;
  }
}

declare module '@mui/material/Chip' {
  interface ChipPropsColorOverrides {
    brand: true;
  }
}

const white = '#ffffff';
const black = '#000000';

const brand: Partial<Color> = {
  500: '#05aa6c',
  100: '#cdeee4',
  50: '#dff0e7',
};
const grey: Partial<Color> = {
  50: '#FAFAFA',
  100: '#F7F7F7',
  //150: '#f1f3eb',
  200: '#F1F1F1',
  250: '#e4e4e4',
  300: '#D6D6D6',
  400: '#BBBBBB',
  500: '#A0A0A0',
  600: '#858585',
  700: '#6A6969',
  800: '#4F4E4E',
  900: '#343333',
  1000: '#191818',
};
const red: Partial<Color> = {
  100: '#f5e5dd',
  200: '#efbcb3',
  400: '#f06a64',
  500: '#d92d20',
};
const orange: Partial<Color> = {
  100: '#f5e9cf',
  200: '#e99f55',
  500: '#ec7804',
};
const green: Partial<Color> = {
  100: '#e0f0e2',
  200: '#b0dbc3',
  500: '#079455',
};

export const palette: PaletteOptions = {
  mode: 'light',
  common: {
    white,
    black,
  },
  grey,
  primary: {
    main: grey[900]!,
    darker: grey[1000]!,
    dark: grey[1000]!,
    light: grey[800]!,
    lighter: grey[700]!,
    contrastText: grey[100]!,
    chip: grey[100]!,
    chipText: grey[800]!,
  },
  secondary: {
    main: grey[100]!,
    darker: grey[250]!,
    dark: grey[200]!,
    light: grey[200]!,
    lighter: grey[300]!,
    contrastText: grey[900]!,
    chip: brand[100]!,
    chipText: brand[500]!,
  },
  brand: {
    // primary in design
    main: brand[500]!,
    darker: brand[500]!,
    dark: brand[500]!,
    light: brand[500]!,
    lighter: brand[50]!,
    contrastText: grey[100]!,
    chip: brand[100]!,
    chipText: brand[500]!,
  },
  info: {
    main: white,
    dark: grey[500]!,
    darker: grey[250]!,
    light: grey[100]!,
    lighter: grey[100]!,
    contrastText: grey[800]!,
    chip: grey[200]!,
    chipText: grey[800]!,
  },
  error: {
    main: red[500]!,
    dark: red[500]!,
    darker: red[500]!,
    light: red[400]!,
    lighter: red[100]!,
    contrastText: grey[100]!,
    chip: red[100]!,
    chipText: red[500]!,
  },
  warning: {
    main: orange[500]!,
    dark: orange[500]!,
    darker: orange[500]!,
    light: orange[500]!,
    lighter: orange[100]!,
    contrastText: grey[100]!,
    chip: orange[100]!,
    chipText: orange[500]!,
  },
  success: {
    main: green[500]!,
    dark: green[500]!,
    darker: green[500]!,
    light: green[500]!,
    lighter: green[100]!,
    contrastText: grey[100]!,
    chip: red[100]!,
    chipText: brand[500]!,
  },
  text: {
    primary: grey[900]!,
    secondary: grey[700]!,
    disabled: grey[500]!,
  },
  divider: grey[250]!,
  background: {
    default: white,
    paper: white,
    area: grey[200]!,
  },
};


================================================================================
File: src/frontend/src/themes/shadows.ts
Size: 1.61 kB
================================================================================

export interface Overlays {
  shadowXSmall: string;
  shadowSmall: string;
  shadowMiddle: string;

  shadowInput: string;
  shadowInputActive: string;
  shadowInputError: string;

  buttonShadeDefault: string;
  buttonShadeActiveLight: string;
  buttonShadeActiveDark: string;
  buttonShadeBrand: string;
  buttonShadeError: string;

  cardHover: string;
}

declare module '@mui/material' {
  interface Theme {
    overlays: Overlays;
  }

  interface ThemeOptions {
    overlays: Overlays;
  }
}

export const overlays: Overlays = {
  shadowXSmall: '0px 1px 2px 0px rgba(16, 24, 40, 0.05)',
  shadowSmall: '0px 1px 2px 0px rgba(49, 49, 48, 0.05)',
  shadowMiddle:
    '0px 8px 8px -4px rgba(52, 51, 51, 0.03), 0px 20px 24px -4px rgba(52, 51, 51, 0.08)',

  // shadowInput: '0px 1px 2px 0px rgba(49, 49, 48, 0.05)',
  shadowInput: 'none',
  shadowInputActive:
    '0px 0px 0px 4px rgba(49, 49, 48, 0.05), 0px 1px 2px 0px rgba(49, 49, 48, 0.05)',
  shadowInputError:
    '0px 0px 0px 4px rgba(221, 82, 76, 0.12), 0px 1px 2px 0px rgba(221, 82, 76, 0.05)',

  buttonShadeDefault: '0px 1px 2px 0px rgba(49, 49, 48, 0.05)',
  buttonShadeActiveLight:
    '0px 0px 0px 4px rgba(49, 49, 48, 0.05), 0px 1px 2px 0px rgba(49, 49, 48, 0.05)',
  buttonShadeActiveDark:
    '0px 0px 0px 4px rgba(49, 49, 48, 0.24), 0px 1px 2px 0px rgba(49, 49, 48, 0.32)',
  buttonShadeBrand: '0 1px 2px 0 rgba(7, 139, 89, 0.10), 0 0 0 4px rgba(7, 139, 89, 0.10)',
  buttonShadeError:
    '0px 0px 0px 4px rgba(221, 82, 76, 0.12), 0px 1px 2px 0px rgba(221, 82, 76, 0.05)',

  cardHover: '0px 4px 16px 0px rgba(160, 160, 160, 0.25)',
};


================================================================================
File: src/frontend/src/themes/typography.ts
Size: 1.85 kB
================================================================================

import type { TypographyVariantsOptions } from '@mui/material';

/**
 * DO NOT CHANGE THIS VALUE
 */
export const HTML_FONT_SIZE = 16;

// export const pxToRem = (px: number) => `${px / HTML_FONT_SIZE}rem`;

export const typography = (): TypographyVariantsOptions => ({
  fontFamily: [
    'Inter',
    '-apple-system',
    'BlinkMacSystemFont',
    '"Segoe UI"',
    'Roboto',
    '"Helvetica Neue"',
    'Arial',
    'sans-serif',
    '"Apple Color Emoji"',
    '"Segoe UI Emoji"',
    '"Segoe UI Symbol"',
  ].join(','),

  h1: {
    // 28/16=1.75
    fontSize: '1.75rem',
    fontStyle: 'normal',
    fontWeight: 700,
    // 48/32=1.5
    lineHeight: 1.5,
  },
  h2: {
    // 20/16=1.25
    fontSize: '1.25rem',
    fontStyle: 'normal',
    fontWeight: 700,
    // 30/20=1.5
    lineHeight: 1.5,
  },
  h3: {
    // 16/16=1
    fontSize: '1rem',
    fontStyle: 'normal',
    fontWeight: 700,
    // 24/16=1.5
    lineHeight: 1.5,
  },
  h4: undefined,
  h5: undefined,
  h6: undefined,
  subtitle1: {
    // 14/16=0.875
    fontSize: '0.875rem',
    fontStyle: 'normal',
    fontWeight: 700,
    // 22/14=1.5714285714285714
    lineHeight: 1.5714285714285714,
  },
  subtitle2: {
    // 13/16=0.8125
    fontSize: '0.8125rem',
    fontStyle: 'normal',
    fontWeight: 700,
    // 20/13=1.5384615384615385
    lineHeight: 1.5384615384615385,
  },
  body1: {
    // 14/16=0.875
    fontSize: '0.875rem',
    fontStyle: 'normal',
    fontWeight: 500,
    // 22/14=1.5714285714285714
    lineHeight: 1.5714285714285714,
  },
  body2: {
    // 12/16=0.75
    fontSize: '0.75rem',
    fontStyle: 'normal',
    fontWeight: 500,
    // 18/12=1.5
    lineHeight: 1.5,
  },

  fontSize: 12,

  fontWeightLight: 400,
  fontWeightRegular: 400,
  fontWeightMedium: 500,
  fontWeightBold: 700,

  // DO NOT CHANGE THIS VALUE
  htmlFontSize: HTML_FONT_SIZE,
});


================================================================================
File: src/frontend/src/vite-env.d.ts
Size: 38 B
================================================================================

/// <reference types="vite/client" />


================================================================================
File: src/frontend/tsconfig.app.json
Size: 704 B
================================================================================

{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
    "target": "ES2022",
    "useDefineForClassFields": true,
    "lib": ["ES2022", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": false,
    "noUnusedParameters": false,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["src"]
}


================================================================================
File: src/frontend/tsconfig.json
Size: 119 B
================================================================================

{
  "files": [],
  "references": [
    { "path": "./tsconfig.app.json" },
    { "path": "./tsconfig.node.json" }
  ]
}


================================================================================
File: src/frontend/tsconfig.node.json
Size: 632 B
================================================================================

{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
    "target": "ES2023",
    "lib": ["ES2023"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": false,
    "noUnusedParameters": false,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["vite.config.ts"]
}


================================================================================
File: src/frontend/vite.config.ts
Size: 870 B
================================================================================

import { dirname, resolve } from 'node:path';
import { fileURLToPath } from 'node:url';

import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react-swc';

const __dirname = dirname(fileURLToPath(import.meta.url));

// https://vite.dev/config/
export default defineConfig({
  build: {
    rollupOptions: {
      input: {
        main: resolve(__dirname, 'index.html'),
        chat: resolve(__dirname, 'chat.html'),
      },
    },
  },
  plugins: [react()],
  server: {
    proxy: {
      '/proxy-api/v1/chat/completions': {
        target: 'http://localhost:3001',
        // target: 'https://ztrxxhzxdt3bn6-3000.proxy.runpod.net',
        rewrite: (path) => path.replace(/^\/proxy-api/, ''),
      },
      '/proxy-api': {
        target: 'http://localhost:3001',
        rewrite: (path) => path.replace(/^\/proxy-api/, ''),
      },
    },
  },
});


================================================================================
File: src/parallax/__init__.py
Size: 0 B
================================================================================



================================================================================
File: src/parallax/cli.py
Size: 16.45 kB
================================================================================

#!/usr/bin/env python3
"""
Parallax CLI - Command line interface for Parallax distributed LLM serving.

This module provides the main CLI entry point for Parallax, supporting
commands like 'run' and 'join' that mirror the functionality of the
bash scripts.
"""

import argparse
import base64
import json
import os
import signal
import subprocess
import sys

import requests

from parallax_utils.file_util import get_project_root
from parallax_utils.logging_config import get_logger
from parallax_utils.version_check import get_current_version

logger = get_logger("parallax.cli")

PUBLIC_INITIAL_PEERS = [
    "/dns4/bootstrap-lattica.gradient.network/udp/18080/quic-v1/p2p/12D3KooWJHXvu8TWkFn6hmSwaxdCLy4ZzFwr4u5mvF9Fe2rMmFXb",
    "/dns4/bootstrap-lattica.gradient.network/tcp/18080/p2p/12D3KooWJHXvu8TWkFn6hmSwaxdCLy4ZzFwr4u5mvF9Fe2rMmFXb",
    "/dns4/bootstrap-lattica-us.gradient.network/udp/18080/quic-v1/p2p/12D3KooWFD8NoyHfmVxLVCocvXJBjwgE9RZ2bgm2p5WAWQax4FoQ",
    "/dns4/bootstrap-lattica-us.gradient.network/tcp/18080/p2p/12D3KooWFD8NoyHfmVxLVCocvXJBjwgE9RZ2bgm2p5WAWQax4FoQ",
    "/dns4/bootstrap-lattica-eu.gradient.network/udp/18080/quic-v1/p2p/12D3KooWCNuEF4ro95VA4Lgq4NvjdWfJFoTcvWsBA7Z6VkBByPtN",
    "/dns4/bootstrap-lattica-eu.gradient.network/tcp/18080/p2p/12D3KooWCNuEF4ro95VA4Lgq4NvjdWfJFoTcvWsBA7Z6VkBByPtN",
]

PUBLIC_RELAY_SERVERS = [
    "/dns4/relay-lattica.gradient.network/udp/18080/quic-v1/p2p/12D3KooWDaqDAsFupYvffBDxjHHuWmEAJE4sMDCXiuZiB8aG8rjf",
    "/dns4/relay-lattica.gradient.network/tcp/18080/p2p/12D3KooWDaqDAsFupYvffBDxjHHuWmEAJE4sMDCXiuZiB8aG8rjf",
    "/dns4/relay-lattica-us.gradient.network/udp/18080/quic-v1/p2p/12D3KooWHMXi6SCfaQzLcFt6Th545EgRt4JNzxqmDeLs1PgGm3LU",
    "/dns4/relay-lattica-us.gradient.network/tcp/18080/p2p/12D3KooWHMXi6SCfaQzLcFt6Th545EgRt4JNzxqmDeLs1PgGm3LU",
    "/dns4/relay-lattica-eu.gradient.network/udp/18080/quic-v1/p2p/12D3KooWRAuR7rMNA7Yd4S1vgKS6akiJfQoRNNexTtzWxYPiWfG5",
    "/dns4/relay-lattica-eu.gradient.network/tcp/18080/p2p/12D3KooWRAuR7rMNA7Yd4S1vgKS6akiJfQoRNNexTtzWxYPiWfG5",
]


def check_python_version():
    """Check if Python version is 3.11 or higher."""
    if sys.version_info < (3, 11) or sys.version_info >= (3, 14):
        logger.info(
            f"Error: Python 3.11 or higher and less than 3.14 is required. Current version is {sys.version_info.major}.{sys.version_info.minor}."
        )
        sys.exit(1)


def _flag_present(args_list: list[str], flag_names: list[str]) -> bool:
    """Return True if any of the given flags is present in args_list.

    Supports forms: "--flag value", "--flag=value", "-f value", "-f=value".
    """
    if not args_list:
        return False
    flags_set = set(flag_names)
    for i, token in enumerate(args_list):
        if token in flags_set:
            return True
        for flag in flags_set:
            if token.startswith(flag + "="):
                return True
    return False


def _find_flag_value(args_list: list[str], flag_names: list[str]) -> str | None:
    """Find the value for the first matching flag in args_list, if present.

    Returns the associated value for forms: "--flag value" or "--flag=value" or
    "-f value" or "-f=value". Returns None if not found or value is missing.
    """
    if not args_list:
        return None
    flags_set = set(flag_names)
    for i, token in enumerate(args_list):
        if token in flags_set:
            # expect value in next token if exists and is not another flag
            if i + 1 < len(args_list) and not args_list[i + 1].startswith("-"):
                return args_list[i + 1]
            return None
        for flag in flags_set:
            prefix = flag + "="
            if token.startswith(prefix):
                return token[len(prefix) :]
    return None


def _execute_with_graceful_shutdown(cmd: list[str], env: dict[str, str] | None = None) -> None:
    """Execute a command in a subprocess and handle graceful shutdown on Ctrl-C.

    This centralizes the common Popen + signal handling logic shared by
    run_command and join_command.
    """
    logger.info(f"Running command: {' '.join(cmd)}")

    sub_process = None
    try:
        # Start in a new session so we can signal the entire process group
        sub_process = subprocess.Popen(cmd, env=env, start_new_session=True)
        # Wait for the subprocess to finish
        return_code = sub_process.wait()
        if return_code != 0:
            logger.error(f"Command failed with exit code {return_code}")
            sys.exit(return_code)
    except KeyboardInterrupt:
        logger.info("Received interrupt signal, shutting down...")

        # If another Ctrl-C arrives during cleanup, force-kill the whole group immediately
        def _force_kill_handler(signum, frame):
            try:
                os.killpg(sub_process.pid, signal.SIGKILL)
            except Exception:
                try:
                    sub_process.kill()
                except Exception:
                    pass
            os._exit(130)

        try:
            signal.signal(signal.SIGINT, _force_kill_handler)
        except Exception:
            pass

        if sub_process is not None:
            try:
                logger.info("Terminating subprocess group...")
                # Gracefully terminate the entire process group
                try:
                    os.killpg(sub_process.pid, signal.SIGINT)
                except Exception:
                    # Fall back to signaling just the child process
                    sub_process.send_signal(signal.SIGINT)

                logger.info("Waiting for subprocess to exit...")
                # Wait for the subprocess to exit gracefully
                try:
                    sub_process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    logger.info("SIGINT timeout; sending SIGTERM to process group...")
                    try:
                        os.killpg(sub_process.pid, signal.SIGTERM)
                    except Exception:
                        sub_process.terminate()
                    try:
                        sub_process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        logger.info("SIGTERM timeout; forcing SIGKILL on process group...")
                        try:
                            os.killpg(sub_process.pid, signal.SIGKILL)
                        except Exception:
                            sub_process.kill()
                        sub_process.wait()
                logger.info("Subprocess exited.")
            except Exception as e:
                logger.error(f"Failed to terminate subprocess: {e}")
        else:
            logger.info("Subprocess not found, skipping shutdown...")
        sys.exit(0)


def _get_relay_params():
    return [
        "--relay-servers",
        *PUBLIC_RELAY_SERVERS,
        "--initial-peers",
        *PUBLIC_INITIAL_PEERS,
    ]


def run_command(args, passthrough_args: list[str] | None = None):
    """Run the scheduler (equivalent to scripts/start.sh)."""
    if not args.skip_upload:
        update_package_info()

    check_python_version()

    project_root = get_project_root()
    backend_main = project_root / "src" / "backend" / "main.py"

    if not backend_main.exists():
        logger.info(f"Error: Backend main.py not found at {backend_main}")
        sys.exit(1)

    # Build the command to run the backend main.py
    passthrough_args = passthrough_args or []
    cmd = [sys.executable, str(backend_main)]
    if not _flag_present(passthrough_args, ["--port"]):
        cmd.extend(["--port", "3001"])

    # Add optional arguments if provided
    if args.model_name:
        cmd.extend(["--model-name", args.model_name])
    if args.init_nodes_num:
        cmd.extend(["--init-nodes-num", str(args.init_nodes_num)])
    if args.use_relay:
        cmd.extend(_get_relay_params())
        logger.info(
            "Using public relay server to help nodes and the scheduler establish a connection (remote mode). Your IP address will be reported to the relay server to help establish the connection."
        )

    # Append any passthrough args (unrecognized by this CLI) directly to the command
    if passthrough_args:
        cmd.extend(passthrough_args)

    _execute_with_graceful_shutdown(cmd)


def join_command(args, passthrough_args: list[str] | None = None):
    """Join a distributed cluster (equivalent to scripts/join.sh)."""
    if not args.skip_upload:
        update_package_info()

    check_python_version()

    project_root = get_project_root()
    launch_script = project_root / "src" / "parallax" / "launch.py"

    if not launch_script.exists():
        logger.info(f"Error: Launch script not found at {launch_script}")
        sys.exit(1)

    # Set environment variable for the subprocess
    env = os.environ.copy()
    env["SGLANG_ENABLE_JIT_DEEPGEMM"] = "0"

    # Build the command to run the launch.py script
    passthrough_args = passthrough_args or []

    cmd = [sys.executable, str(launch_script)]
    if not _flag_present(passthrough_args, ["--max-num-tokens-per-batch"]):
        cmd.extend(["--max-num-tokens-per-batch", "4096"])
    if not _flag_present(passthrough_args, ["--max-sequence-length"]):
        cmd.extend(["--max-sequence-length", "2048"])
    if not _flag_present(passthrough_args, ["--max-batch-size"]):
        cmd.extend(["--max-batch-size", "8"])
    if not _flag_present(passthrough_args, ["--kv-block-size"]):
        cmd.extend(["--kv-block-size", "32"])

    # The scheduler address is now taken directly from the parsed arguments.
    cmd.extend(["--scheduler-addr", args.scheduler_addr])

    # Relay logic based on effective scheduler address
    if args.use_relay or (
        args.scheduler_addr != "auto" and not str(args.scheduler_addr).startswith("/")
    ):
        cmd.extend(_get_relay_params())
        logger.info(
            "Using public relay server to help nodes and the scheduler establish a connection (remote mode). Your IP address will be reported to the relay server to help establish the connection."
        )

    # Append any passthrough args (unrecognized by this CLI) directly to the command
    if passthrough_args:
        cmd.extend(passthrough_args)

    logger.info(f"Scheduler address: {args.scheduler_addr}")
    _execute_with_graceful_shutdown(cmd, env=env)


def chat_command(args, passthrough_args: list[str] | None = None):
    """Start the Parallax chat server (equivalent to scripts/chat.sh)."""
    check_python_version()

    project_root = get_project_root()
    launch_script = project_root / "src" / "parallax" / "launch_chat.py"

    if not launch_script.exists():
        logger.info(f"Error: Launch chat script not found at {launch_script}")
        sys.exit(1)

    # Build the command to run the launch_chat.py script
    passthrough_args = passthrough_args or []
    cmd = [sys.executable, str(launch_script)]

    cmd.extend(["--scheduler-addr", args.scheduler_addr])

    # Relay logic based on effective scheduler address
    if args.use_relay or (
        args.scheduler_addr != "auto" and not str(args.scheduler_addr).startswith("/")
    ):
        cmd.extend(_get_relay_params())
        logger.info(
            "Using public relay server to help chat client and the scheduler establish a connection (remote mode). Your IP address will be reported to the relay server to help establish the connection."
        )

    # Append any passthrough args (unrecognized by this CLI) directly to the command
    if passthrough_args:
        cmd.extend(passthrough_args)

    logger.info(f"Scheduler address: {args.scheduler_addr}")
    _execute_with_graceful_shutdown(cmd)


def update_package_info():
    """Update package information."""
    version = get_current_version()

    try:
        package_info = load_package_info()
        if package_info is not None and package_info["version"] == version:
            return

        save_package_info({"version": version})
    except Exception:
        pass


def load_package_info():
    """Load package information."""
    try:
        project_root = get_project_root()
        if not (project_root / ".cache" / "tmp_key.txt").exists():
            return None
        with open(project_root / ".cache" / "tmp_key.txt", "r") as f:
            return json.loads(reversible_decode_string(f.read()))
    except Exception:
        return None


def save_package_info(usage_info: dict):
    """Save package information."""
    project_root = get_project_root()
    os.makedirs(project_root / ".cache", exist_ok=True)
    with open(project_root / ".cache" / "tmp_key.txt", "w") as f:
        f.write(reversible_encode_string(json.dumps(usage_info)))

    upload_package_info(usage_info)


def upload_package_info(usage_info: dict):
    post_url = "https://chatbe-dev.gradient.network/api/v1/parallax/upload"
    headers = {
        "Content-Type": "application/json",
    }
    try:
        requests.post(post_url, headers=headers, json=usage_info, timeout=5)
        return
    except Exception:
        return


def reversible_encode_string(s: str) -> str:
    return base64.urlsafe_b64encode(s.encode("utf-8")).decode("utf-8")


def reversible_decode_string(encoded: str) -> str:
    return base64.urlsafe_b64decode(encoded.encode("utf-8")).decode("utf-8")


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Parallax - A fully decentralized inference engine developed by Gradient Network",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  parallax run                                                          # Start scheduler with frontend
  parallax run -m {model-name} -n {number-of-worker-nodes}              # Start scheduler without frontend
  parallax run -m Qwen/Qwen3-0.6B -n 2                                  # example
  parallax join                                                         # Join cluster in local network
  parallax join -s {scheduler-address}                                  # Join cluster in public network
  parallax join -s 12D3KooWLX7MWuzi1Txa5LyZS4eTQ2tPaJijheH8faHggB9SxnBu # example
        """,
    )

    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Add 'run' command parser
    run_parser = subparsers.add_parser(
        "run", help="Start the Parallax scheduler (equivalent to scripts/start.sh)"
    )
    run_parser.add_argument("-n", "--init-nodes-num", type=int, help="Number of initial nodes")
    run_parser.add_argument("-m", "--model-name", type=str, help="Model name")
    run_parser.add_argument(
        "-r", "--use-relay", action="store_true", help="Use public relay servers"
    )
    run_parser.add_argument(
        "-u", "--skip-upload", action="store_true", help="Skip upload package info"
    )

    # Add 'join' command parser
    join_parser = subparsers.add_parser(
        "join", help="Join a distributed cluster (equivalent to scripts/join.sh)"
    )
    join_parser.add_argument(
        "-s",
        "--scheduler-addr",
        default="auto",
        type=str,
        help="Scheduler address (required)",
    )
    join_parser.add_argument(
        "-r", "--use-relay", action="store_true", help="Use public relay servers"
    )
    join_parser.add_argument(
        "-u", "--skip-upload", action="store_true", help="Skip upload package info"
    )

    # Add 'chat' command parser
    chat_parser = subparsers.add_parser(
        "chat", help="Start the Parallax chat server (equivalent to scripts/chat.sh)"
    )
    chat_parser.add_argument(
        "-s",
        "--scheduler-addr",
        default="auto",
        type=str,
        help="Scheduler address (required)",
    )
    chat_parser.add_argument(
        "-r", "--use-relay", action="store_true", help="Use public relay servers"
    )

    # Accept unknown args and pass them through to the underlying python command
    args, passthrough_args = parser.parse_known_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    if args.command == "run":
        run_command(args, passthrough_args)
    elif args.command == "join":
        join_command(args, passthrough_args)
    elif args.command == "chat":
        chat_command(args, passthrough_args)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()


================================================================================
File: src/parallax/launch.py
Size: 12.35 kB
================================================================================

"""
Launch the Parallax server.

This script is used to launch the Parallax server.
It will start the following services:
    1.Executor each tp_rank as a subprocess.
    2.HTTP server as a subprocess.
    3.P2P server as a subprocess.

Example command:
python src/parallax/launch.py \
    --model-path Qwen/Qwen3-0.6B \
    --max-num-tokens-per-batch 16384 \
    --max-batch-size 128 \
    --start-layer 0 \
    --end-layer 28
"""

import argparse
import multiprocessing
import os
import tempfile
import time

from parallax.p2p.server import ServerState, launch_p2p_server_process, stop_p2p_server
from parallax.server.executor.factory import run_executor_process, stop_executor_process
from parallax.server.http_server import launch_http_server, stop_http_server
from parallax.server.server_args import parse_args
from parallax.utils.shared_state import SharedState
from parallax.utils.utils import fetch_model_from_hf, initialize_nccl_port
from parallax_utils.ascii_anime import display_parallax_join
from parallax_utils.logging_config import get_logger, set_log_level
from parallax_utils.version_check import check_latest_release

logger = get_logger("parallax.launch")


def _update_args_from_shared_state(args, shared_state: SharedState):
    """Update args with layer allocation from shared state"""
    model_info = shared_state.get_model_info()
    args.start_layer = model_info["block_start_index"]
    args.end_layer = model_info["block_end_index"]
    # Update model_path if provided (always update to support model switching)
    if model_info["model_name"]:
        args.model_path = model_info["model_name"]
        logger.debug(f"Updated model_path to: {args.model_path}")
    # Update tp_size if provided, otherwise keep current value
    args.tp_size = model_info["tp_size"] or args.tp_size


def _stop_executor_processes(executor_subprocs):
    """Stop all executor processes"""
    for executor_process in executor_subprocs:
        if executor_process.is_alive():
            logger.debug(f"Terminating executor process {executor_process.pid}")
            stop_executor_process(executor_process)


def _wait_executors_check_layer_change(shared_state: SharedState, executor_subprocs):
    """Wait for executor processes and check if layer allocation changed.

    Returns:
        True if layer allocation changed (need to reload executors),
        False if all executors exited normally.
    """
    while any(proc.is_alive() for proc in executor_subprocs):
        for proc in executor_subprocs:
            if proc.is_alive():
                proc.join(timeout=1.0)  # Check every second

        if shared_state.get_layer_allocation_changed():
            return True

    # Check race condition: layer allocation changed after all processes exited
    return shared_state.get_layer_allocation_changed()


if __name__ == "__main__":
    multiprocessing.set_start_method("spawn", force=True)

    p2p_server_process = None
    http_server_process = None
    executor_subprocs = []
    # Shared state for layer allocation info (used when P2P server is in subprocess)
    shared_state = SharedState.create()
    shared_state.set_status(ServerState.JOINING.value)

    try:
        args = parse_args()
        set_log_level(args.log_level)
        logger.debug(f"args: {args}")
        args.recv_from_peer_addr = f"ipc://{tempfile.NamedTemporaryFile().name}"
        args.send_to_peer_addr = f"ipc://{tempfile.NamedTemporaryFile().name}"
        args.executor_input_ipc = f"ipc://{tempfile.NamedTemporaryFile().name}"
        args.executor_output_ipc = f"ipc://{tempfile.NamedTemporaryFile().name}"
        if args.nccl_port is None:
            args.nccl_port = initialize_nccl_port()

        # Silence tokenizer warnings
        os.environ["TOKENIZERS_PARALLELISM"] = "false"

        logger.debug(f"executor_input_addr: {args.executor_input_ipc}")
        logger.debug(f"executor_output_addr: {args.executor_output_ipc}")
        logger.debug(f"nccl_port: {args.nccl_port}")
        if args.scheduler_addr is None:
            if args.log_level != "DEBUG":
                display_parallax_join(args.model_path)
            check_latest_release()

            config = fetch_model_from_hf(args.model_path)
            # only launch http server on head node
            if args.start_layer == 0:
                http_server_process = launch_http_server(args)
            # Launch P2P server as subprocess
            p2p_server_process = launch_p2p_server_process(
                initial_peers=args.initial_peers,
                scheduler_addr=args.scheduler_addr,
                relay_servers=args.relay_servers,
                pp_start_layer=args.start_layer,
                pp_end_layer=args.end_layer,
                hidden_layers=config.get("num_hidden_layers"),
                tp_size=args.tp_size,
                tcp_port=args.tcp_port,
                udp_port=args.udp_port,
                dht_prefix=args.dht_prefix,
                announce_maddrs=args.announce_maddrs,
                http_port=args.port,
                notify_url=args.notify_url,
                recv_from_peer_addr=args.recv_from_peer_addr,
                send_to_peer_addr=args.send_to_peer_addr,
                model_name=args.model_path,
                max_batch_size=args.max_batch_size,
                max_sequence_length=args.max_sequence_length,
                param_mem_ratio=args.param_mem_ratio,
                kvcache_mem_ratio=args.kvcache_mem_ratio,
                shared_state=shared_state.dict,  # Pass dict to subprocess
                log_level=args.log_level,
            )

            # Launch all executor processes (including tp_rank=0)
            for tp_rank in range(args.tp_size):
                args_copy = argparse.Namespace(**vars(args))
                args_copy.tp_rank = tp_rank
                proc = multiprocessing.Process(
                    target=run_executor_process,
                    args=(
                        args_copy,
                        shared_state.dict,  # Pass dict to subprocess
                    ),
                )
                proc.start()
                executor_subprocs.append(proc)

            time.sleep(2)  # Give executors time to start
            shared_state.set_status(ServerState.READY.value)

            # Wait for all executor processes
            for proc in executor_subprocs:
                proc.join()
        else:
            # Launch P2P server as subprocess (with scheduler)
            # Pass dict to subprocess (multiprocessing requires serializable objects)
            p2p_server_process = launch_p2p_server_process(
                initial_peers=args.initial_peers,
                scheduler_addr=args.scheduler_addr,
                relay_servers=args.relay_servers,
                pp_start_layer=args.start_layer,
                pp_end_layer=args.end_layer,
                hidden_layers=None,
                tp_size=args.tp_size,
                tcp_port=args.tcp_port,
                udp_port=args.udp_port,
                dht_prefix=args.dht_prefix,
                announce_maddrs=args.announce_maddrs,
                http_port=args.port,
                notify_url=args.notify_url,
                recv_from_peer_addr=args.recv_from_peer_addr,
                send_to_peer_addr=args.send_to_peer_addr,
                model_name=args.model_path,
                max_batch_size=args.max_batch_size,
                max_sequence_length=args.max_sequence_length,
                param_mem_ratio=args.param_mem_ratio,
                kvcache_mem_ratio=args.kvcache_mem_ratio,
                shared_state=shared_state.dict,  # Pass dict to subprocess
                log_level=args.log_level,
            )

            # Wait for layer allocation from scheduler (via shared state)
            logger.debug("Waiting for layer allocation from scheduler...")
            max_wait_time = 300  # 5 minutes
            wait_start = time.time()
            while True:
                model_info = shared_state.get_model_info()
                if (
                    model_info["block_start_index"] is not None
                    and model_info["block_end_index"] is not None
                    and model_info["model_name"] is not None
                ):
                    break
                if time.time() - wait_start > max_wait_time:
                    logger.error("Timeout waiting for layer allocation from scheduler")
                    raise RuntimeError("Failed to get layer allocation from scheduler")
                time.sleep(1)

            # Get layer allocation from shared state
            _update_args_from_shared_state(args, shared_state)

            logger.debug(
                f"Start Executor with start_layer: {args.start_layer}, end_layer: {args.end_layer}, "
                f"model: {args.model_path}"
            )

            if args.log_level != "DEBUG":
                display_parallax_join(args.model_path)
            check_latest_release()

            # Main execution loop with layer reallocation support
            while True:
                try:
                    # only launch http server on head node
                    if args.start_layer == 0:
                        http_server_process = launch_http_server(args)

                    # Launch all executor processes (including tp_rank=0)
                    executor_subprocs = []
                    for tp_rank in range(args.tp_size):
                        args_copy = argparse.Namespace(**vars(args))
                        args_copy.tp_rank = tp_rank
                        proc = multiprocessing.Process(
                            target=run_executor_process,
                            args=(
                                args_copy,
                                shared_state.dict,  # Pass dict to subprocess
                            ),
                        )
                        proc.start()
                        executor_subprocs.append(proc)

                    # Wait for executors and restart if layer allocation changes
                    if _wait_executors_check_layer_change(shared_state, executor_subprocs):
                        logger.warning("Layer allocation changed! Stopping executors to reload...")
                        # Reset flag and set status to INITIALIZING
                        shared_state.update(
                            _layer_allocation_changed=False,
                            status=ServerState.INITIALIZING.value,
                        )
                        _stop_executor_processes(executor_subprocs)
                        if http_server_process is not None:
                            stop_http_server(http_server_process)
                        _update_args_from_shared_state(args, shared_state)
                        logger.info(
                            f"Reloading executor with layers [{args.start_layer}, {args.end_layer})"
                        )
                        continue

                    # All processes exited normally
                    break
                except KeyboardInterrupt:
                    logger.debug("Received interrupt signal, shutting down...")
                    break
                except Exception as e:
                    logger.exception(f"Executor error: {e}")
                    # Shutdown all executor processes on error
                    for proc in executor_subprocs:
                        if proc.is_alive():
                            stop_executor_process(proc)
                    raise
    except KeyboardInterrupt:
        logger.debug("Received interrupt signal, shutting down...")
    except Exception as e:
        logger.exception(e)
    finally:
        # Shutdown all processes
        logger.debug("Shutting down all processes...")

        # Shutdown executor subprocesses
        for executor_process in executor_subprocs:
            if executor_process.is_alive():
                stop_executor_process(executor_process)

        # Shutdown P2P server subprocess
        if p2p_server_process is not None:
            stop_p2p_server(p2p_server_process)

        # Shutdown http server
        if http_server_process is not None:
            stop_http_server(http_server_process)

        logger.debug("All processes shut down.")


================================================================================
File: src/parallax/launch_chat.py
Size: 561 B
================================================================================

from parallax.server.node_chat_http_server import run_node_chat_http_server
from parallax.server.server_args import parse_args
from parallax_utils.logging_config import get_logger, set_log_level

logger = get_logger(__name__)

if __name__ == "__main__":
    try:
        args = parse_args()
        set_log_level(args.log_level)
        logger.debug(f"args: {args}")

        run_node_chat_http_server(args)
    except KeyboardInterrupt:
        logger.debug("Received interrupt signal, shutting down...")
    except Exception as e:
        logger.exception(e)


================================================================================
File: src/parallax/metal/paged_attention/kernel.py
Size: 10.61 kB
================================================================================

import os
from typing import Dict, List, Optional

import mlx.core as mx

# Cache for compiled kernels
_KERNELS: Dict[str, object] = {}


def _get_metal_source(filename):
    path = os.path.join(os.path.dirname(__file__), filename)
    with open(path, "r") as f:
        return f.read()


def _type_to_string(dtype: mx.Dtype) -> str:
    if dtype == mx.float32:
        return "float"
    elif dtype == mx.float16:
        return "half"
    elif dtype == mx.bfloat16:
        # Metal 3.1+ supports bfloat, typically via bfloat16_t or using half
        # For now we map to bfloat16_t assuming compiler support
        return "bfloat16_t"
    else:
        raise ValueError(f"Unsupported dtype for paged attention: {dtype}")


def _get_kernel(
    name: str,
    filename: str,
    input_names: List[str],
    output_names: List[str],
    dtype: mx.Dtype = mx.float32,
):
    type_str = _type_to_string(dtype)
    kernel_key = f"{name}_{type_str}"

    if kernel_key not in _KERNELS:
        source = _get_metal_source(filename)
        # Simple template substitution
        source = source.replace("{{T}}", type_str)

        header = """
#include <metal_stdlib>
using namespace metal;
"""
        _KERNELS[kernel_key] = mx.fast.metal_kernel(
            name=name,  # Internal name for MLX JIT cache (not used for dispatch if we hold the object)
            input_names=input_names,
            output_names=output_names,
            source=source,
            header=header,
        )
    return _KERNELS[kernel_key]


def reshape_and_cache(
    key: mx.array,  # (batch, target_len, num_kv_heads, head_dim)
    value: mx.array,  # ...
    key_cache: mx.array,  # (num_layers, num_blocks, num_kv_heads, block_size, head_dim)
    value_cache: mx.array,
    block_tables: mx.array,  # (batch, max_blocks)
    context_lengths: mx.array,  # (batch,)
    block_size: int,
    layer_idx: int,
    slot_mapping: Optional[mx.array] = None,  # (batch,) or (batch * target_len,)
):
    """
    Writes new keys and values into the Paged KV Cache using a custom Metal kernel.
    NOTE: This performs an in-place update on key_cache/value_cache buffers.

    Supports two modes:
    1. Decode (Single Token): slot_mapping is None. Calculated internally.
       Input shape: (batch, num_kv_heads, 1, head_dim)
    2. Prefill (Batch Tokens): slot_mapping is provided.
       Input shape: (batch, num_kv_heads, target_len, head_dim)
    """
    dtype = key.dtype
    if key_cache.dtype != dtype:
        raise ValueError(f"Key cache dtype {key_cache.dtype} does not match key dtype {dtype}")

    # Handle dimensions based on mode
    if slot_mapping is None:
        # Decode Mode
        batch_size = key.shape[0]
        if key.ndim == 4:
            # (batch, 1, num_kv_heads, head_dim) -> (batch, num_kv_heads, head_dim)
            if key.shape[1] == 1:
                key = key.squeeze(1)
                value = value.squeeze(1)
            elif key.shape[2] == 1:
                # Fallback for old layout (batch, num_kv_heads, 1, head_dim)
                key = key.squeeze(2)
                value = value.squeeze(2)

        num_kv_heads = key.shape[1]
        k_head_dim = key.shape[2]
        v_head_dim = value.shape[2]

        # Compute slot_mapping internally
        indices = context_lengths - 1
        block_indices_in_table = indices // block_size
        offsets = indices % block_size
        batch_indices = mx.arange(batch_size)
        physical_block_numbers = block_tables[batch_indices, block_indices_in_table]
        slot_mapping = physical_block_numbers.astype(mx.int64) * block_size + offsets.astype(
            mx.int64
        )

        num_tokens = batch_size

    else:
        # Prefill Mode
        # Key/Value input shape: (batch, target_len, num_kv_heads, head_dim) = BTHD
        # We need to flatten to: (total_tokens, num_kv_heads, head_dim)
        if key.ndim == 4:
            # Input is (B, T, H, D) from optimized Qwen3
            B, T, H, D = key.shape
            key = key.reshape(B * T, H, D)
            # Value might have different D
            V_D = value.shape[3]
            value = value.reshape(B * T, H, V_D)

        num_tokens = key.shape[0]
        num_kv_heads = key.shape[1]
        k_head_dim = key.shape[2]
        v_head_dim = value.shape[2]

        if slot_mapping.shape[0] != num_tokens:
            raise ValueError(f"Slot mapping length {slot_mapping.shape[0]} != tokens {num_tokens}")

    num_layers = key_cache.shape[0]
    num_blocks = key_cache.shape[1]

    # 2. Prepare Constants
    key_stride = num_kv_heads * k_head_dim
    value_stride = num_kv_heads * v_head_dim

    def mk_int(val):
        return mx.array(val, dtype=mx.int32)

    c_key_stride = mk_int(key_stride)
    c_val_stride = mk_int(value_stride)
    c_num_kv = mk_int(num_kv_heads)
    c_k_head_dim = mk_int(k_head_dim)
    c_v_head_dim = mk_int(v_head_dim)
    c_block_size = mk_int(block_size)
    c_layer_idx = mk_int(layer_idx)
    c_num_layers = mk_int(num_layers)
    c_num_blocks = mk_int(num_blocks)

    # Inputs list
    inputs = [
        key,
        value,
        key_cache,
        value_cache,
        slot_mapping,
        c_key_stride,
        c_val_stride,
        c_num_kv,
        c_k_head_dim,
        c_v_head_dim,
        c_block_size,
        c_layer_idx,
        c_num_layers,
        c_num_blocks,
    ]

    # Input names (just for declaration)
    input_names = [
        "key",
        "value",
        "key_cache",
        "value_cache",
        "slot_mapping",
        "key_stride",
        "value_stride",
        "num_kv_heads",
        "k_head_dim",
        "v_head_dim",
        "block_size",
        "layer_idx",
        "num_layers",
        "num_blocks",
    ]

    # 3. Get and Launch Kernel
    kernel = _get_kernel(
        name="reshape_and_cache_kernel",
        filename="reshape_and_cache.metal",
        input_names=input_names,
        output_names=["dummy_out"],
        dtype=dtype,
    )

    # Grid: (num_kv_heads * max_dim, num_tokens, 1)
    max_dim = max(k_head_dim, v_head_dim)
    grid = (num_kv_heads * max_dim, num_tokens, 1)
    thread_group = (min(1024, num_kv_heads * max_dim), 1, 1)

    # Execute
    # We match output_shapes to the grid dimensions to ensure MLX generates 'index' variable
    # corresponding to (num_tokens, num_kv_heads * max_dim).
    outputs = kernel(
        inputs=inputs,
        grid=grid,
        threadgroup=thread_group,
        output_shapes=[(num_tokens, num_kv_heads * max_dim)],
        output_dtypes=[mx.float32],  # Dummy output dtype usually doesn't matter
        verbose=False,
    )

    mx.eval(outputs)

    return key_cache, value_cache


def paged_attention(
    queries: mx.array,
    key_cache: mx.array,
    value_cache: mx.array,
    block_tables: mx.array,
    context_lengths: mx.array,
    block_size: int,
    scale: float,
    num_kv_heads: int,
    layer_idx: int,
    v_head_dim: Optional[int] = None,
    window_size: Optional[int] = None,
    sinks: Optional[mx.array] = None,
) -> mx.array:
    """
    Paged Attention using Metal Kernel.
    """
    batch_size = queries.shape[0]
    num_heads = queries.shape[1]
    dtype = queries.dtype

    if queries.ndim == 4:
        if queries.shape[2] != 1:
            pass
        queries = queries.squeeze(2)

    k_head_dim = queries.shape[2]
    if v_head_dim is None:
        v_head_dim = k_head_dim

    # Use -1 to represent full attention (infinite window)
    c_window_size_val = window_size if window_size is not None else -1

    num_layers = key_cache.shape[0]
    num_total_blocks = key_cache.shape[1]
    max_blocks = block_tables.shape[1]

    # Prepare Constants
    def mk_int(val):
        return mx.array(val, dtype=mx.int32)

    c_num_heads = mk_int(num_heads)
    c_num_kv_heads = mk_int(num_kv_heads)
    c_k_head_dim = mk_int(k_head_dim)
    c_v_head_dim = mk_int(v_head_dim)
    c_block_size = mk_int(block_size)
    c_max_blocks = mk_int(max_blocks)
    c_layer_idx = mk_int(layer_idx)
    c_num_layers = mk_int(num_layers)
    c_num_total_blocks = mk_int(num_total_blocks)
    c_scale = mx.array(scale, dtype=queries.dtype)
    c_window_size = mk_int(c_window_size_val)

    if sinks is None:
        # Pass -inf if no sinks provided to mask it out
        # Assuming num_heads is enough to cover head_idx access
        c_sinks = mx.full((num_heads,), -float("inf"), dtype=queries.dtype)
    else:
        c_sinks = sinks

    inputs = [
        queries,
        key_cache,
        value_cache,
        block_tables,
        context_lengths,
        c_num_heads,
        c_num_kv_heads,
        c_k_head_dim,
        c_v_head_dim,
        c_block_size,
        c_max_blocks,
        c_layer_idx,
        c_num_layers,
        c_num_total_blocks,
        c_scale,
        c_window_size,
        c_sinks,
    ]

    input_names = [
        "queries",
        "key_cache",
        "value_cache",
        "block_tables",
        "context_lengths",
        "num_heads",
        "num_kv_heads",
        "k_head_dim",
        "v_head_dim",
        "block_size",
        "max_blocks",
        "layer_idx",
        "num_layers",
        "num_total_blocks",
        "scale",
        "window_size",
        "sinks",
    ]

    # For paged_attention, we don't have explicit T in source,
    # but if we use it in future or if we want to support half specialized logic.
    # Currently paged_attention kernel uses `float` for computation but loads from `queries` (T*).
    # Metal implicitly handles T* access if MLX generated correct input types.
    # However, if we use `reshape_and_cache` style template, we should use it here too.
    # But paged_attention_kernel.metal DOES NOT use {{T}} yet.
    # It uses `float q_vec`.
    # Let's keep it as is for now, as Metal handles implicit conversion on load.
    # The only issue is if we write `output` as `float*` but requested `half` output?
    # In Python we should set output_dtypes=[dtype].

    kernel = _get_kernel(
        name="paged_attention_kernel",
        filename="paged_attention_kernel.metal",
        input_names=input_names,
        output_names=["output"],
        dtype=dtype,  # This will generate paged_attention_kernel_half etc.
    )

    grid = (num_heads * 32, batch_size, 1)
    thread_group = (32, 1, 1)

    outputs = kernel(
        inputs=inputs,
        grid=grid,
        threadgroup=thread_group,
        output_shapes=[(batch_size, num_heads, v_head_dim)],
        output_dtypes=[dtype],  # Output matches input dtype
        verbose=False,
    )

    out = outputs[0]
    return out[:, :, None, :]


================================================================================
File: src/parallax/metal/paged_attention/paged_attention_kernel.metal
Size: 5.64 kB
================================================================================


// Inputs:
// queries, key_cache, value_cache, block_tables, context_lengths
// output (output array)
// num_heads, num_kv_heads, k_head_dim, v_head_dim, block_size, max_blocks, layer_idx,
// num_layers, num_total_blocks, scale, window_size, sinks (All pointers)

uint3 gid = thread_position_in_grid;
uint3 tid = thread_position_in_threadgroup;

// Each threadgroup handles one head.
// Assuming threadgroup size is 32 (SIMD width).
// head_idx comes from the group index.
// Or we can calculate it from gid if grid is linear.
// grid.x = num_heads * 32.

int head_idx = gid.x / 32;
int batch_idx = gid.y;

// Dereference constants
int _num_heads = num_heads;
int _num_kv_heads = num_kv_heads;
int _k_head_dim = k_head_dim;
int _v_head_dim = v_head_dim;
int _block_size = block_size;
int _max_blocks = max_blocks;
int _layer_idx = layer_idx;
int _num_total_blocks = num_total_blocks;
float _scale = scale;
int _window_size = window_size;

if (head_idx >= _num_heads)
  return;

int kv_head_idx = head_idx / (_num_heads / _num_kv_heads);

// Load Query
// Q: [batch, num_heads, k_head_dim]
// Thread i loads elements i, i+32, ...

// Support up to 256 head dim (8 * 32)
float q_vec[8] = {0.0f};

int q_offset = batch_idx * _num_heads * _k_head_dim + head_idx * _k_head_dim;

for (int i = tid.x; i < _k_head_dim; i += 32) {
  if (i < 256) {
    q_vec[i / 32] = queries[q_offset + i];
  }
}

// Running statistics for Softmax
float m_i = -INFINITY;
float l_i = 0.0f;
float acc_vec[8] = {0.0f};

// -------------------------------------------------------------------------
// Handle Implicit Sink Token (KV = 0, Not in Block Table)
// -------------------------------------------------------------------------
// The sink token is assumed to be an extra token (logical pos 0? or just extra)
// that is NOT stored in the paged KV blocks.
// Its Key is 0, Value is 0.
// Its Attention Score = (Q * K_sink) * scale + sinks_bias
//                     = (Q * 0) * scale + sinks[head_idx]
//                     = sinks[head_idx]

float sink_score = sinks[head_idx];

// Initialize Softmax stats with Sink Token if it's not masked out (-inf)
// We use a threshold slightly larger than -INFINITY to avoid precision issues
if (sink_score > -3.40282e+38f) { // roughly > -FLT_MAX
    m_i = sink_score;
    l_i = 1.0f;
    // acc_vec remains 0.0f because V_sink is 0.
}

int context_len = context_lengths[batch_idx];
int num_context_blocks = (context_len + _block_size - 1) / _block_size;

// Strides for Key
long k_layer_stride =
    (long)_num_total_blocks * _num_kv_heads * _block_size * _k_head_dim;
long k_block_stride = _num_kv_heads * _block_size * _k_head_dim;
long k_head_stride = _block_size * _k_head_dim;

long k_layer_offset = _layer_idx * k_layer_stride;

// Strides for Value
long v_layer_stride =
    (long)_num_total_blocks * _num_kv_heads * _block_size * _v_head_dim;
long v_block_stride = _num_kv_heads * _block_size * _v_head_dim;
long v_head_stride = _block_size * _v_head_dim;

long v_layer_offset = _layer_idx * v_layer_stride;

// Iterate over blocks
for (int b = 0; b < num_context_blocks; b++) {
  int block_num = block_tables[batch_idx * _max_blocks + b];

  // Calculate logic position range for this block
  int block_start_pos = b * _block_size;
  int block_end_pos = block_start_pos + _block_size; // exclusive

  // Handle Window Attention Skip
  // Current query position is at (context_len - 1)
  // Window range is [context_len - 1 - window_size, context_len - 1]

  // NOTE: Sink token is handled separately above, so we don't need to protect block 0
  // unless block 0 also contains other vital tokens (it usually does).

  if (_window_size > 0) {
      int window_start = context_len - 1 - _window_size;
      if (block_end_pos <= window_start) {
          // Entire block is out of window
          continue;
      }
  }

  long k_block_base =
      k_layer_offset + block_num * k_block_stride + kv_head_idx * k_head_stride;
  long v_block_base =
      v_layer_offset + block_num * v_block_stride + kv_head_idx * v_head_stride;

  int tokens_in_block = _block_size;
  if (b == num_context_blocks - 1) {
    tokens_in_block = context_len % _block_size;
    if (tokens_in_block == 0)
      tokens_in_block = _block_size;
  }

  for (int t = 0; t < tokens_in_block; t++) {
    int token_pos = block_start_pos + t;

    // Check if token is within window
    if (_window_size > 0) {
        int window_start = context_len - 1 - _window_size;
        if (token_pos < window_start) {
            continue;
        }
    }

    // Compute Dot Product Q * K[t]
    float score = 0.0f;
    for (int i = tid.x; i < _k_head_dim; i += 32) {
      // offset inside block: t * k_head_dim + i
      float k_val = key_cache[k_block_base + t * _k_head_dim + i];

      if (i < 256) {
        score += q_vec[i / 32] * k_val;
      }
    }

    // SIMD Reduction for score
    score = simd_sum(score);
    score *= _scale;

    // Softmax update
    float m_prev = m_i;
    m_i = max(m_prev, score);
    float alpha = exp(m_prev - m_i);
    float beta = exp(score - m_i);

    l_i = l_i * alpha + beta;

    // Accumulate V
    for (int i = tid.x; i < _v_head_dim; i += 32) {
      float v_val = value_cache[v_block_base + t * _v_head_dim + i];
      if (i < 256) {
        acc_vec[i / 32] = acc_vec[i / 32] * alpha + v_val * beta;
      }
    }
  }
}

// Finalize Output
for (int i = 0; i < 8; i++) {
  acc_vec[i] /= l_i;
}

int out_offset = batch_idx * _num_heads * _v_head_dim + head_idx * _v_head_dim;

for (int i = tid.x; i < _v_head_dim; i += 32) {
  if (i < 256) {
    output[out_offset + i] = ({{T}})acc_vec[i / 32];
  }
}


================================================================================
File: src/parallax/metal/paged_attention/reshape_and_cache.metal
Size: 2.27 kB
================================================================================

device {{T}} *key_cache_mut = (device {{T}} *)key_cache;
device {{T}} *value_cache_mut = (device {{T}} *)value_cache;
// reshape_and_cache logic
// Inputs are provided by MLX wrapper:
// key, value, key_cache, value_cache, slot_mapping, ...

// MLX provided variable for grid position
uint3 gid = thread_position_in_grid;

int kv_head_dim_idx = gid.x;
int token_idx = gid.y;

// Scalars are passed by value (int32), so no dereference needed
int n_kv_heads = num_kv_heads;
int k_dim = k_head_dim;
int v_dim = v_head_dim;
int max_dim = (k_dim > v_dim) ? k_dim : v_dim;

if (kv_head_dim_idx >= n_kv_heads * max_dim)
  return;

int head_idx = kv_head_dim_idx / max_dim;
int dim_idx = kv_head_dim_idx % max_dim;

int64_t slot = slot_mapping[token_idx];

// Handle padding tokens (slot == -1)
if (slot < 0) {
  return;
}

int b_size = block_size;
int64_t block_idx = slot / b_size;
int64_t block_offset = slot % b_size;

int l_idx = layer_idx;
int n_blocks = num_blocks;

// Handle Key
if (dim_idx < k_dim) {
    // Calculate source index
    // key shape: (num_tokens, num_kv_heads, k_head_dim)
    int64_t src_idx =
        (int64_t)token_idx * n_kv_heads * k_dim + head_idx * k_dim + dim_idx;

    // Calculate destination index
    int64_t head_stride = b_size * k_dim;
    int64_t block_stride = n_kv_heads * head_stride;
    int64_t layer_stride = n_blocks * block_stride;

    int64_t dest_idx = (int64_t)l_idx * layer_stride + block_idx * block_stride +
                       (int64_t)head_idx * head_stride + block_offset * k_dim +
                       dim_idx;

    key_cache_mut[dest_idx] = key[src_idx];
}

// Handle Value
if (dim_idx < v_dim) {
    // Calculate source index
    // value shape: (num_tokens, num_kv_heads, v_head_dim)
    int64_t src_idx =
        (int64_t)token_idx * n_kv_heads * v_dim + head_idx * v_dim + dim_idx;

    // Calculate destination index
    int64_t head_stride = b_size * v_dim;
    int64_t block_stride = n_kv_heads * head_stride;
    int64_t layer_stride = n_blocks * block_stride;

    int64_t dest_idx = (int64_t)l_idx * layer_stride + block_idx * block_stride +
                       (int64_t)head_idx * head_stride + block_offset * v_dim +
                       dim_idx;

    value_cache_mut[dest_idx] = value[src_idx];
}


================================================================================
File: src/parallax/models/deepseek_v2.py
Size: 6.05 kB
================================================================================

"""
hidden_dimefines the Qwen3 model.
"""

from typing import Optional, Tuple

import mlx.core as mx
from mlx_lm.models.base import scaled_dot_product_attention
from mlx_lm.models.deepseek_v2 import DeepseekV2Attention as MLXDeepseekV2Attention
from mlx_lm.models.deepseek_v2 import DeepseekV2DecoderLayer as MLXDeepseekV2Block
from mlx_lm.models.deepseek_v2 import ModelArgs

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


class ParallaxDeepSeekV2Attention(MLXDeepseekV2Attention):
    """A custom attention module for Parallax, extending the DeepseekV2 Attention class.

    We apply explicit KV cache handling and passing in `offset` directly from Request.
    This version returns the new K and V states for external caching.
    """

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        layer_idx: int = 0,
    ) -> mx.array:
        """
        Attention forward pass with explicit KV cache handling.

        Args:
            x: (batch, target_len, hidden_dim) - Input hidden states for the current query segment.
            mask: (batch, n_q_heads, target_len, source_len)
            cache: contains (key_cache, value_cache) global.
            block_tables: (batch, max_blocks) - PagedKV block tables.
            context_lengths: (batch,) - PagedKV sequence lengths.
            layer_idx: Layer index for PagedKV access.

        Returns:
            output_h: (batch, target_len, hidden_dim) - Output hidden states.
        """
        batch, target_len, _ = x.shape
        if self.q_lora_rank is None:
            q = self.q_proj(x)
        else:
            q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(x)))

        q = q.reshape(batch, target_len, self.num_heads, self.q_head_dim).transpose(0, 2, 1, 3)
        q_nope, q_pe = mx.split(q, [self.qk_nope_head_dim], axis=-1)
        compressed_kv = self.kv_a_proj_with_mqa(x)
        compressed_kv, k_pe = mx.split(compressed_kv, [self.kv_lora_rank], axis=-1)
        k_pe = k_pe.reshape(batch, target_len, 1, self.qk_rope_head_dim).transpose(0, 2, 1, 3)
        kv = self.kv_b_proj(self.kv_a_layernorm(compressed_kv))
        kv = kv.reshape(batch, target_len, self.num_heads, -1)

        k_nope, values = mx.split(kv, [self.qk_nope_head_dim], axis=-1)
        k_nope = k_nope.transpose(0, 2, 1, 3)

        # q_pe = self.rope(q_pe, offset=offset)
        # k_pe = self.rope(k_pe, offset=offset)
        key_cache_global, value_cache_global = cache
        q_pe_list = []
        k_pe_list = []
        for i in range(batch):
            current_pos = int(context_lengths[i]) - 1 if target_len == 1 else 0
            q_slice = q_pe[i : i + 1]
            k_slice = k_pe[i : i + 1]
            q_rot = self.rope(q_slice, offset=current_pos)
            k_rot = self.rope(k_slice, offset=current_pos)
            q_pe_list.append(q_rot)
            k_pe_list.append(k_rot)
        q_pe = mx.concatenate(q_pe_list, axis=0)
        k_pe = mx.concatenate(k_pe_list, axis=0)

        k_pe = mx.repeat(k_pe, self.num_heads, axis=1)
        queries = mx.concatenate([q_nope, q_pe], axis=-1)

        # Construct full keys
        keys = mx.concatenate([k_nope, k_pe], axis=-1)

        block_size = key_cache_global.shape[3]

        reshape_and_cache(
            keys.transpose(0, 2, 1, 3),
            values,
            key_cache_global,
            value_cache_global,
            block_tables,
            context_lengths,
            block_size,
            layer_idx=layer_idx,
            slot_mapping=slot_mapping,
        )

        if target_len == 1:
            output = paged_attention(
                queries,
                key_cache_global,
                value_cache_global,
                block_tables,
                context_lengths,
                block_size,
                self.scale,
                self.num_heads,  # num_kv_heads (MQA/MLA, here num_heads == num_kv_heads effectively after repeat?)
                layer_idx,
                v_head_dim=values.shape[-1],
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)
        else:
            output = scaled_dot_product_attention(
                queries,
                keys,
                values.transpose(0, 2, 1, 3),
                scale=self.scale,
                mask=mask,
                cache=None,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output)


class ParallaxDeepSeekV2Block(MLXDeepseekV2Block):
    """A custom transformer block for Parallax, extending the Qwen3 Block class.
    This version handles the KV cache explicitly and returns new K and V states.
    """

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args, layer_idx=layer_idx)
        self.self_attn = ParallaxDeepSeekV2Attention(args)
        self.layer_idx = layer_idx

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
    ):
        r = self.self_attn(
            self.input_layernorm(x),
            mask,
            cache,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
            layer_idx=self.layer_idx,
        )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "DeepseekV2ForCausalLM"


EntryClass = ParallaxDeepSeekV2Block


================================================================================
File: src/parallax/models/deepseek_v3.py
Size: 6.33 kB
================================================================================

"""
hidden_dimefines the Qwen3 model.
"""

from typing import Optional, Tuple

import mlx.core as mx
from mlx_lm.models.base import scaled_dot_product_attention
from mlx_lm.models.deepseek_v3 import DeepseekV3Attention as MLXDeepseekV3Attention
from mlx_lm.models.deepseek_v3 import DeepseekV3DecoderLayer as MLXDeepseekV3Block
from mlx_lm.models.deepseek_v3 import ModelArgs

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


class ParallaxDeepSeekV3Attention(MLXDeepseekV3Attention):
    """A custom attention module for Parallax, extending the DeepseekV3 Attention class.

    We apply explicit KV cache handling and passing in `offset` directly from Request.
    This version returns the new K and V states for external caching.
    """

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        offset: int = 0,
        lengths: Optional[mx.array] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        layer_idx: int = 0,
    ) -> mx.array:
        """
        Attention forward pass with explicit KV cache handling.

        Args:
            x: (batch, target_len, hidden_dim) - Input hidden states for the current query segment.
            mask: (batch, n_q_heads, target_len, source_len)
            cache: contains (key_cache, value_cache) global.
            block_tables: (batch, max_blocks) - PagedKV block tables.
            context_lengths: (batch,) - PagedKV sequence lengths.
            layer_idx: Layer index for PagedKV access.

        Returns:
            output_h: (batch, target_len, hidden_dim) - Output hidden states.
        """
        batch, target_len, _ = x.shape
        if self.q_lora_rank is None:
            q = self.q_proj(x)
        else:
            q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(x)))

        q = q.reshape(batch, target_len, self.num_heads, self.q_head_dim).transpose(0, 2, 1, 3)
        q_nope, q_pe = mx.split(q, [self.qk_nope_head_dim], axis=-1)
        compressed_kv = self.kv_a_proj_with_mqa(x)
        compressed_kv, k_pe = mx.split(compressed_kv, [self.kv_lora_rank], axis=-1)
        k_pe = k_pe.reshape(batch, target_len, 1, self.qk_rope_head_dim).transpose(0, 2, 1, 3)
        kv = self.kv_b_proj(self.kv_a_layernorm(compressed_kv))

        kv = kv.reshape(batch, target_len, self.num_heads, -1)
        k_nope, values = mx.split(kv, [self.qk_nope_head_dim], axis=-1)
        k_nope = k_nope.transpose(0, 2, 1, 3)

        # q_pe = self.rope(q_pe, offset=offset)
        # k_pe = self.rope(k_pe, offset=offset)
        key_cache_global, value_cache_global = cache

        q_pe_list = []
        k_pe_list = []
        for i in range(batch):
            current_pos = int(context_lengths[i]) - 1 if target_len == 1 else 0
            q_slice = q_pe[i : i + 1]
            k_slice = k_pe[i : i + 1]
            q_rot = self.rope(q_slice, offset=current_pos)
            k_rot = self.rope(k_slice, offset=current_pos)
            q_pe_list.append(q_rot)
            k_pe_list.append(k_rot)

        q_pe = mx.concatenate(q_pe_list, axis=0)
        k_pe = mx.concatenate(k_pe_list, axis=0)

        k_pe = mx.repeat(k_pe, self.num_heads, axis=1)
        queries = mx.concatenate([q_nope, q_pe], axis=-1)

        # Construct full keys
        keys = mx.concatenate([k_nope, k_pe], axis=-1)

        block_size = key_cache_global.shape[3]

        reshape_and_cache(
            keys.transpose(0, 2, 1, 3),
            values,
            key_cache_global,
            value_cache_global,
            block_tables,
            context_lengths,
            block_size,
            layer_idx=layer_idx,
            slot_mapping=slot_mapping,
        )

        if target_len == 1:
            # Decode phase: Use Paged Attention
            output = paged_attention(
                queries,
                key_cache_global,
                value_cache_global,
                block_tables,
                context_lengths,
                block_size,
                self.scale,
                self.num_heads,
                layer_idx,
                v_head_dim=values.shape[-1],
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)
        else:
            # Prefill phase: Use Standard Attention
            if mask is not None:
                mask = mx.array(mask, dtype=queries.dtype)

            output = scaled_dot_product_attention(
                queries,
                keys,
                values.transpose(0, 2, 1, 3),
                scale=self.scale,
                mask=mask,
                cache=None,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output)


class ParallaxDeepSeekV3Block(MLXDeepseekV3Block):
    """A custom transformer block for Parallax, extending the Qwen3 Block class.
    This version handles the KV cache explicitly and returns new K and V states.
    """

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args, layer_idx=layer_idx)
        self.self_attn = ParallaxDeepSeekV3Attention(args)
        self.layer_idx = layer_idx

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        offset: int = 0,
        lengths: Optional[mx.array] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
    ):
        r = self.self_attn(
            self.input_layernorm(x),
            mask,
            cache,
            offset=offset,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
            layer_idx=self.layer_idx,
        )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "DeepseekV3ForCausalLM"


EntryClass = ParallaxDeepSeekV3Block


================================================================================
File: src/parallax/models/glm4_moe.py
Size: 4.48 kB
================================================================================

from typing import Optional, Tuple

import mlx.core as mx
from mlx_lm.models.base import scaled_dot_product_attention
from mlx_lm.models.glm4_moe import Attention as MLXGLM4MoeAttention
from mlx_lm.models.glm4_moe import DecoderLayer as MLXGLM4MoeBlock
from mlx_lm.models.glm4_moe import ModelArgs

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


class ParallaxGLM4MoeAttention(MLXGLM4MoeAttention):
    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        layer_idx: int = 0,
    ) -> mx.array:
        batch, target_len, _ = x.shape

        queries_new, keys_new, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        queries_new = queries_new.reshape(batch, target_len, self.n_heads, -1)
        keys_new = keys_new.reshape(batch, target_len, self.n_kv_heads, -1)

        if self.use_qk_norm:
            queries_new = self.q_norm(queries_new)
            keys_new = self.k_norm(keys_new)

        queries_new = queries_new.transpose(0, 2, 1, 3)
        keys_new = keys_new.transpose(0, 2, 1, 3)
        values_new = values.reshape(batch, target_len, self.n_kv_heads, -1)

        key_cache_global, value_cache_global = cache

        queries_rotated_list = []
        keys_rotated_list = []

        for i in range(batch):
            current_pos = int(context_lengths[i]) - 1 if target_len == 1 else 0
            q_slice = queries_new[i : i + 1]
            k_slice = keys_new[i : i + 1]
            q_rot = self.rope(q_slice, offset=current_pos)
            k_rot = self.rope(k_slice, offset=current_pos)
            queries_rotated_list.append(q_rot)
            keys_rotated_list.append(k_rot)

        queries_rotated = mx.concatenate(queries_rotated_list, axis=0)
        keys_rotated = mx.concatenate(keys_rotated_list, axis=0)

        block_size = key_cache_global.shape[3]

        reshape_and_cache(
            keys_rotated.transpose(0, 2, 1, 3),
            values_new,
            key_cache_global,
            value_cache_global,
            block_tables,
            context_lengths,
            block_size,
            layer_idx,
            slot_mapping=slot_mapping,
        )

        # 3. Compute Attention
        if target_len == 1:
            # Decode Phase: Use Paged Attention Kernel
            output = paged_attention(
                queries_rotated,
                key_cache_global,
                value_cache_global,
                block_tables,
                context_lengths,
                block_size,
                self.scale,
                self.n_kv_heads,
                layer_idx,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)
        else:
            # Prefill Phase: Use Standard Self-Attention on local data
            output = scaled_dot_product_attention(
                queries_rotated,
                keys_rotated,
                values_new.transpose(0, 2, 1, 3),
                scale=self.scale,
                mask=mask,
                cache=None,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output)


class ParallaxGLM4MoeBlock(MLXGLM4MoeBlock):

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args, layer_idx)
        self.self_attn = ParallaxGLM4MoeAttention(args)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
    ):
        r = self.self_attn(
            self.input_layernorm(x),
            mask,
            cache,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
            layer_idx=self.layer_idx,
        )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "Glm4MoeForCausalLM"


EntryClass = ParallaxGLM4MoeBlock


================================================================================
File: src/parallax/models/gpt_oss.py
Size: 5.9 kB
================================================================================

"""
hidden_dimefines the Qwen3 model.
"""

from typing import Optional, Tuple

import mlx.core as mx
from mlx_lm.models.base import create_causal_mask, scaled_dot_product_attention
from mlx_lm.models.gpt_oss import AttentionBlock as MLXGPTOSSAttention
from mlx_lm.models.gpt_oss import ModelArgs
from mlx_lm.models.gpt_oss import TransformerBlock as MLXGPTOSSBlock

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


class ParallaxGPTOSSAttention(MLXGPTOSSAttention):
    """A custom attention module for Parallax, extending the Qwen3 Attention class.

    We apply explicit KV cache handling and passing in `offset` directly from Request.
    This version returns the new K and V states for external caching.
    """

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        layer_idx: int = 0,
        window_size: Optional[int] = None,
    ) -> mx.array:
        """
        Attention forward pass with PagedAttention integration.
        """
        batch, target_len, _ = x.shape

        queries_new = self.q_proj(x)
        keys_new = self.k_proj(x)
        values_new = self.v_proj(x)

        queries_new = queries_new.reshape(
            batch, target_len, self.num_attention_heads, -1
        ).transpose(0, 2, 1, 3)
        keys_new = keys_new.reshape(batch, target_len, self.num_key_value_heads, -1).transpose(
            0, 2, 1, 3
        )
        values_new = values_new.reshape(batch, target_len, self.num_key_value_heads, -1)

        key_cache_global, value_cache_global = cache

        queries_rotated_list = []
        keys_rotated_list = []
        for i in range(batch):
            current_pos = int(context_lengths[i]) - 1 if target_len == 1 else 0
            q_slice = queries_new[i : i + 1]
            k_slice = keys_new[i : i + 1]
            q_rot = self.rope(q_slice, offset=current_pos)
            k_rot = self.rope(k_slice, offset=current_pos)
            queries_rotated_list.append(q_rot)
            keys_rotated_list.append(k_rot)

        queries_rotated = mx.concatenate(queries_rotated_list, axis=0)
        keys_rotated = mx.concatenate(keys_rotated_list, axis=0)

        # Update Paged Cache
        block_size = key_cache_global.shape[3]

        reshape_and_cache(
            keys_rotated.transpose(0, 2, 1, 3),
            values_new,
            key_cache_global,
            value_cache_global,
            block_tables,
            context_lengths,
            block_size,
            layer_idx,
            slot_mapping=slot_mapping,
        )

        # Compute Attention
        if target_len == 1:
            output = paged_attention(
                queries_rotated,
                key_cache_global,
                value_cache_global,
                block_tables,
                context_lengths,
                block_size,
                self.sm_scale,
                self.num_key_value_heads,
                layer_idx,
                window_size=window_size,
                sinks=self.sinks,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)
        else:
            if window_size is not None:
                mask_prefill = create_causal_mask(target_len, offset=0, window_size=window_size)
                mask_prefill = (1 - mask_prefill) * -1e9
                mask = mask + mask_prefill

            mask = mask.astype(queries_rotated.dtype)
            output = scaled_dot_product_attention(
                queries_rotated,
                keys_rotated,
                values_new.transpose(0, 2, 1, 3),
                scale=self.sm_scale,
                mask=mask,
                cache=None,
                sinks=self.sinks,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output)


class ParallaxGPTOSSBlock(MLXGPTOSSBlock):
    """A custom transformer block for Parallax, extending the GptOss Block class.
    This version handles the KV cache explicitly and returns new K and V states.
    """

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args)
        self.self_attn = ParallaxGPTOSSAttention(args)
        self.sliding_window = args.sliding_window
        self.layer_idx = layer_idx
        if args.layer_types:
            self.layer_type = args.layer_types[layer_idx]
        else:
            self.layer_type = "sliding_attention" if layer_idx % 2 == 0 else "full_attention"

    def get_window_size(self):
        return self.sliding_window - 1

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
    ):
        # Determine window size for this layer
        if self.layer_type == "sliding_attention":
            window_size = self.get_window_size()
        else:
            window_size = None

        r = self.self_attn(
            self.input_layernorm(x),
            mask=mask,
            cache=cache,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
            layer_idx=self.layer_idx,
            window_size=window_size,
        )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "GptOssForCausalLM"


EntryClass = ParallaxGPTOSSBlock


================================================================================
File: src/parallax/models/llama.py
Size: 5.44 kB
================================================================================

"""
Defines the Llama4 model wrapper for Parallax.

This module adapts MLX llama attention to explicitly handle KV cache and
exposes the same block interface as Qwen implementations, so that
`ShardedModel` can drive it uniformly.
"""

from typing import Optional, Tuple

import mlx.core as mx
from mlx_lm.models.base import scaled_dot_product_attention
from mlx_lm.models.llama import Attention as MLXLlamaAttention
from mlx_lm.models.llama import ModelArgs
from mlx_lm.models.llama import TransformerBlock as MLXLlamaBlock

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


class ParallaxLlamaAttention(MLXLlamaAttention):
    """Custom attention for Llama, with explicit KV cache returns.

    We pass in `offset` for RoPE and return (keys_rotated, values) so that
    outer KV cache can be maintained by Parallax.
    """

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        layer_idx: int = 0,
    ) -> mx.array:
        """
        Attention forward pass with explicit KV cache handling.

        Args:
            x: (batch, target_len, hidden_dim) - Input hidden states for the current query segment.
            mask: (batch, n_q_heads, target_len, source_len)
            cache: contains (key_cache, value_cache) global.
            block_tables: (batch, max_blocks) - PagedKV block tables.
            context_lengths: (batch,) - PagedKV sequence lengths.
            layer_idx: Layer index for PagedKV access.

        Returns:
            output: (batch, target_len, hidden_dim) - Output hidden states.
        """
        batch, target_len, _ = x.shape

        queries_new = self.q_proj(x)
        keys_new = self.k_proj(x)
        values_new = self.v_proj(x)

        queries_new = queries_new.reshape(batch, target_len, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys_new = keys_new.reshape(batch, target_len, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values_new = values_new.reshape(batch, target_len, self.n_kv_heads, -1)

        key_cache_global, value_cache_global = cache

        queries_rotated_list = []
        keys_rotated_list = []

        for i in range(batch):
            current_pos = int(context_lengths[i]) - 1 if target_len == 1 else 0
            q_slice = queries_new[i : i + 1]
            k_slice = keys_new[i : i + 1]
            q_rot = self.rope(q_slice, offset=current_pos)
            k_rot = self.rope(k_slice, offset=current_pos)
            queries_rotated_list.append(q_rot)
            keys_rotated_list.append(k_rot)

        queries_rotated = mx.concatenate(queries_rotated_list, axis=0)
        keys_rotated = mx.concatenate(keys_rotated_list, axis=0)

        block_size = key_cache_global.shape[3]

        reshape_and_cache(
            keys_rotated.transpose(0, 2, 1, 3),
            values_new,
            key_cache_global,
            value_cache_global,
            block_tables,
            context_lengths,
            block_size,
            layer_idx,
            slot_mapping=slot_mapping,
        )

        # 3. Compute Attention
        if target_len == 1:
            # Decode Phase: Use Paged Attention Kernel
            output = paged_attention(
                queries_rotated,
                key_cache_global,
                value_cache_global,
                block_tables,
                context_lengths,
                block_size,
                self.scale,
                self.n_kv_heads,
                layer_idx,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)
        else:
            # Prefill Phase: Use Standard Self-Attention on local data
            output = scaled_dot_product_attention(
                queries_rotated,
                keys_rotated,
                values_new.transpose(0, 2, 1, 3),
                scale=self.scale,
                mask=mask,
                cache=None,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output)


class ParallaxLlamaBlock(MLXLlamaBlock):
    """Transformer block wrapper returning explicit KV cache updates."""

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args)
        self.self_attn = ParallaxLlamaAttention(args)
        self.layer_idx = layer_idx

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
    ):
        r = self.self_attn(
            self.input_layernorm(x),
            mask,
            cache,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
            layer_idx=self.layer_idx,
        )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "LlamaForCausalLM"


EntryClass = ParallaxLlamaBlock


================================================================================
File: src/parallax/models/minimax.py
Size: 9.4 kB
================================================================================

# Copyright Â© 2025 Apple Inc.

from dataclasses import dataclass
from typing import Any, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
from mlx_lm.models.base import BaseModelArgs, scaled_dot_product_attention
from mlx_lm.models.switch_layers import SwitchGLU

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    intermediate_size: int
    num_attention_heads: int
    num_key_value_heads: int
    max_position_embeddings: int
    num_experts_per_tok: int
    num_local_experts: int
    shared_intermediate_size: int
    num_hidden_layers: int
    rms_norm_eps: float
    rope_theta: float
    rotary_dim: int
    vocab_size: int
    tie_word_embeddings: bool = False
    scoring_func: str = "sigmoid"
    head_dim: Optional[int] = None
    use_qk_norm: bool = True


class MLXMiniMaxAttention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.hidden_dim = hidden_size = args.hidden_size

        self.num_attention_heads = args.num_attention_heads
        self.num_key_value_heads = args.num_key_value_heads
        self.head_dim = head_dim = args.head_dim or hidden_size // args.num_attention_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(args.hidden_size, self.num_attention_heads * head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * head_dim, bias=False)
        self.o_proj = nn.Linear(self.num_attention_heads * head_dim, args.hidden_size, bias=False)

        self.use_qk_norm = args.use_qk_norm if hasattr(args, "use_qk_norm") else False
        if self.use_qk_norm:
            self.q_norm = nn.RMSNorm(head_dim * self.num_attention_heads, eps=args.rms_norm_eps)
            self.k_norm = nn.RMSNorm(head_dim * self.num_key_value_heads, eps=args.rms_norm_eps)

        self.rope = nn.RoPE(args.rotary_dim, traditional=False, base=args.rope_theta)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        if self.use_qk_norm:
            queries = self.q_norm(queries)
            keys = self.k_norm(keys)

        queries = queries.reshape(B, L, self.num_attention_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.num_key_value_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.num_key_value_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)

        return self.o_proj(output)


class MLXMiniMaxSparseMoeBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_experts_per_tok = args.num_experts_per_tok

        self.gate = nn.Linear(args.hidden_size, args.num_local_experts, bias=False)
        self.switch_mlp = SwitchGLU(
            args.hidden_size, args.intermediate_size, args.num_local_experts
        )
        self.e_score_correction_bias = mx.zeros((args.num_local_experts,))

    def __call__(self, x: mx.array) -> mx.array:
        gates = self.gate(x.astype(mx.float32))

        scores = mx.sigmoid(gates)
        orig_scores = scores
        scores = scores + self.e_score_correction_bias

        k = self.num_experts_per_tok
        inds = mx.argpartition(-scores, kth=k - 1, axis=-1)[..., :k]
        scores = mx.take_along_axis(orig_scores, inds, axis=-1)

        scores = scores / (mx.sum(scores, axis=-1, keepdims=True) + 1e-20)
        scores = scores.astype(x.dtype)

        y = self.switch_mlp(x, inds)
        y = (y * scores[..., None]).sum(axis=-2)
        return y


class MLXMiniMaxBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.self_attn = MLXMiniMaxAttention(args)

        self.block_sparse_moe = MLXMiniMaxSparseMoeBlock(args)

        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = x + self.self_attn(self.input_layernorm(x), mask, cache)
        r = r + self.block_sparse_moe(self.post_attention_layernorm(r))
        return r


class ParallaxMiniMaxAttention(MLXMiniMaxAttention):

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        layer_idx: int = 0,
    ) -> mx.array:

        batch, target_len, _ = x.shape

        queries_new = self.q_proj(x)
        keys_new = self.k_proj(x)
        values = self.v_proj(x)

        if self.use_qk_norm:
            queries_new = self.q_norm(queries_new)
            keys_new = self.k_norm(keys_new)

        queries_new = queries_new.reshape(
            batch, target_len, self.num_attention_heads, -1
        ).transpose(0, 2, 1, 3)
        keys_new = keys_new.reshape(batch, target_len, self.num_key_value_heads, -1).transpose(
            0, 2, 1, 3
        )
        values_new = values.reshape(batch, target_len, self.num_key_value_heads, -1)

        key_cache_global, value_cache_global = cache

        queries_rotated_list = []
        keys_rotated_list = []

        for i in range(batch):
            current_pos = int(context_lengths[i]) - 1 if target_len == 1 else 0
            q_slice = queries_new[i : i + 1]
            k_slice = keys_new[i : i + 1]
            q_rot = self.rope(q_slice, offset=current_pos)
            k_rot = self.rope(k_slice, offset=current_pos)
            queries_rotated_list.append(q_rot)
            keys_rotated_list.append(k_rot)

        queries_rotated = mx.concatenate(queries_rotated_list, axis=0)
        keys_rotated = mx.concatenate(keys_rotated_list, axis=0)

        block_size = key_cache_global.shape[3]

        reshape_and_cache(
            keys_rotated.transpose(0, 2, 1, 3),
            values_new,
            key_cache_global,
            value_cache_global,
            block_tables,
            context_lengths,
            block_size,
            layer_idx,
            slot_mapping=slot_mapping,
        )

        # 3. Compute Attention
        if target_len == 1:
            # Decode Phase: Use Paged Attention Kernel
            output = paged_attention(
                queries_rotated,
                key_cache_global,
                value_cache_global,
                block_tables,
                context_lengths,
                block_size,
                self.scale,
                self.num_key_value_heads,
                layer_idx,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)
        else:
            # Prefill Phase: Use Standard Self-Attention on local data
            output = scaled_dot_product_attention(
                queries_rotated,
                keys_rotated,
                values_new.transpose(0, 2, 1, 3),
                scale=self.scale,
                mask=mask,
                cache=None,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output)


class ParallaxMiniMaxBlock(MLXMiniMaxBlock):
    """A custom transformer block for Parallax, extending the MiniMax Block class.
    This version handles the KV cache explicitly and returns new K and V states.
    """

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args)
        self.self_attn = ParallaxMiniMaxAttention(args)
        self.layer_idx = layer_idx

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
    ):
        r = self.self_attn(
            self.input_layernorm(x),
            mask,
            cache,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
            layer_idx=self.layer_idx,
        )
        h = x + r
        r = self.block_sparse_moe(self.post_attention_layernorm(h))
        out = h + r
        return out

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "MiniMaxM2ForCausalLM"


EntryClass = ParallaxMiniMaxBlock


================================================================================
File: src/parallax/models/qwen2.py
Size: 5.39 kB
================================================================================

"""
hidden_dimefines the Qwen3 model.
"""

from typing import Optional, Tuple

import mlx.core as mx
from mlx_lm.models.base import scaled_dot_product_attention
from mlx_lm.models.qwen2 import Attention as MLXQwen2Attention
from mlx_lm.models.qwen2 import ModelArgs
from mlx_lm.models.qwen2 import TransformerBlock as MLXQwen2Block

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


class ParallaxQwen2Attention(MLXQwen2Attention):
    """A custom attention module for Parallax, extending the Qwen3 Attention class.

    We apply explicit KV cache handling and passing in `offset` directly from Request.
    This version returns the new K and V states for external caching.
    """

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        layer_idx: int = 0,
    ) -> mx.array:
        """
        Attention forward pass with explicit KV cache handling.

        Args:
            x: (batch, target_len, hidden_dim) - Input hidden states for the current query segment.
            mask: (batch, n_q_heads, target_len, source_len)
            cache: contains (key_cache, value_cache) global.
            block_tables: (batch, max_blocks) - PagedKV block tables.
            context_lengths: (batch,) - PagedKV sequence lengths.
            layer_idx: Layer index for PagedKV access.

        Returns:
            output: (batch, target_len, hidden_dim) - Output hidden states.
        """
        batch, target_len, _ = x.shape

        queries_new = self.q_proj(x)
        keys_new = self.k_proj(x)
        values_new = self.v_proj(x)

        queries_new = queries_new.reshape(batch, target_len, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys_new = keys_new.reshape(batch, target_len, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values_new = values_new.reshape(batch, target_len, self.n_kv_heads, -1)

        key_cache_global, value_cache_global = cache

        queries_rotated_list = []
        keys_rotated_list = []

        for i in range(batch):
            current_pos = int(context_lengths[i]) - 1 if target_len == 1 else 0
            q_slice = queries_new[i : i + 1]
            k_slice = keys_new[i : i + 1]
            q_rot = self.rope(q_slice, offset=current_pos)
            k_rot = self.rope(k_slice, offset=current_pos)
            queries_rotated_list.append(q_rot)
            keys_rotated_list.append(k_rot)

        queries_rotated = mx.concatenate(queries_rotated_list, axis=0)
        keys_rotated = mx.concatenate(keys_rotated_list, axis=0)

        block_size = key_cache_global.shape[3]

        reshape_and_cache(
            keys_rotated.transpose(0, 2, 1, 3),
            values_new,
            key_cache_global,
            value_cache_global,
            block_tables,
            context_lengths,
            block_size,
            layer_idx,
            slot_mapping=slot_mapping,
        )

        # 3. Compute Attention
        if target_len == 1:
            # Decode Phase: Use Paged Attention Kernel
            output = paged_attention(
                queries_rotated,
                key_cache_global,
                value_cache_global,
                block_tables,
                context_lengths,
                block_size,
                self.scale,
                self.n_kv_heads,
                layer_idx,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)
        else:
            # Prefill Phase: Use Standard Self-Attention on local data
            output = scaled_dot_product_attention(
                queries_rotated,
                keys_rotated,
                values_new.transpose(0, 2, 1, 3),
                scale=self.scale,
                mask=mask,
                cache=None,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output)


class ParallaxQwen2Block(MLXQwen2Block):
    """A custom transformer block for Parallax, extending the Qwen3 Block class.
    This version handles the KV cache explicitly and returns new K and V states.
    """

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args)
        self.self_attn = ParallaxQwen2Attention(args)
        self.layer_idx = layer_idx

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
    ):
        r = self.self_attn(
            self.input_layernorm(x),
            mask,
            cache,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
            layer_idx=self.layer_idx,
        )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "Qwen2ForCausalLM"


EntryClass = ParallaxQwen2Block


================================================================================
File: src/parallax/models/qwen3.py
Size: 5.62 kB
================================================================================

"""
hidden_dimefines the Qwen3 model.
"""

from typing import Optional, Tuple

import mlx.core as mx
from mlx_lm.models.base import scaled_dot_product_attention
from mlx_lm.models.qwen3 import Attention as MLXQwen3Attention
from mlx_lm.models.qwen3 import ModelArgs
from mlx_lm.models.qwen3 import TransformerBlock as MLXQwen3Block

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


class ParallaxQwen3Attention(MLXQwen3Attention):
    """A custom attention module for Parallax, extending the Qwen3 Attention class.

    We apply explicit KV cache handling and passing in `offset` directly from Request.
    This version returns the new K and V states for external caching.
    """

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        layer_idx: int = 0,
    ) -> mx.array:
        """
        Attention forward pass with explicit KV cache handling.

        Args:
            x: (batch, target_len, hidden_dim) - Input hidden states for the current query segment.
            mask: (batch, n_q_heads, target_len, source_len)
            cache: contains (key_cache, value_cache) global.
            block_tables: (batch, max_blocks) - PagedKV block tables.
            context_lengths: (batch,) - PagedKV sequence lengths.
            layer_idx: Layer index for PagedKV access.

        Returns:
            output: (batch, target_len, hidden_dim) - Output hidden states.
        """
        batch, target_len, _ = x.shape

        queries_new = self.q_proj(x)
        keys_new = self.k_proj(x)
        values_new = self.v_proj(x)

        queries_new = self.q_norm(
            queries_new.reshape(batch, target_len, self.n_heads, -1)
        ).transpose(0, 2, 1, 3)
        keys_new = self.k_norm(keys_new.reshape(batch, target_len, self.n_kv_heads, -1)).transpose(
            0, 2, 1, 3
        )
        values_new = values_new.reshape(batch, target_len, self.n_kv_heads, -1)

        key_cache_global, value_cache_global = cache

        queries_rotated_list = []
        keys_rotated_list = []

        for i in range(batch):
            # TODO: for chunked Prefill, we need to pass in the offset for each chunk.
            # Currently, we assume the offset is 0 for all chunks.
            current_pos = int(context_lengths[i]) - 1 if target_len == 1 else 0
            q_slice = queries_new[i : i + 1]
            k_slice = keys_new[i : i + 1]
            q_rot = self.rope(q_slice, offset=current_pos)
            k_rot = self.rope(k_slice, offset=current_pos)
            queries_rotated_list.append(q_rot)
            keys_rotated_list.append(k_rot)

        queries_rotated = mx.concatenate(queries_rotated_list, axis=0)
        keys_rotated = mx.concatenate(keys_rotated_list, axis=0)

        block_size = key_cache_global.shape[3]

        reshape_and_cache(
            keys_rotated.transpose(0, 2, 1, 3),
            values_new,
            key_cache_global,
            value_cache_global,
            block_tables,
            context_lengths,
            block_size,
            layer_idx,
            slot_mapping=slot_mapping,
        )

        # 3. Compute Attention
        if target_len == 1:
            # Decode Phase: Use Paged Attention Kernel
            output = paged_attention(
                queries_rotated,
                key_cache_global,
                value_cache_global,
                block_tables,
                context_lengths,
                block_size,
                self.scale,
                self.n_kv_heads,
                layer_idx,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)
        else:
            # Prefill Phase: Use Standard Self-Attention on local data
            output = scaled_dot_product_attention(
                queries_rotated,
                keys_rotated,
                values_new.transpose(0, 2, 1, 3),
                scale=self.scale,
                mask=mask,
                cache=None,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output)


class ParallaxQwen3Block(MLXQwen3Block):
    """A custom transformer block for Parallax, extending the Qwen3 Block class.
    This version handles the KV cache explicitly and returns new K and V states.
    """

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args)
        self.self_attn = ParallaxQwen3Attention(args)
        self.layer_idx = layer_idx

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
    ):
        r = self.self_attn(
            self.input_layernorm(x),
            mask,
            cache,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
            layer_idx=self.layer_idx,
        )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "Qwen3ForCausalLM"


EntryClass = ParallaxQwen3Block


================================================================================
File: src/parallax/models/qwen3_moe.py
Size: 5.53 kB
================================================================================

"""
hidden_dimefines the Qwen3 model.
"""

from typing import Optional, Tuple

import mlx.core as mx
from mlx_lm.models.base import scaled_dot_product_attention
from mlx_lm.models.qwen3_moe import Attention as MLXQwen3MoeAttention
from mlx_lm.models.qwen3_moe import ModelArgs
from mlx_lm.models.qwen3_moe import Qwen3MoeDecoderLayer as MLXQwen3MoeBlock

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


class ParallaxQwen3MoeAttention(MLXQwen3MoeAttention):
    """A custom attention module for Parallax, extending the Qwen3 Attention class.

    We apply explicit KV cache handling and passing in `offset` directly from Request.
    This version returns the new K and V states for external caching.
    """

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        layer_idx: int = 0,
    ) -> mx.array:
        """
        Attention forward pass with explicit KV cache handling.

        Args:
            x: (batch, target_len, hidden_dim) - Input hidden states for the current query segment.
            mask: (batch, n_q_heads, target_len, source_len)
            cache: contains (key_cache, value_cache) global.
            block_tables: (batch, max_blocks) - PagedKV block tables.
            context_lengths: (batch,) - PagedKV sequence lengths.
            layer_idx: Layer index for PagedKV access.

        Returns:
            output: (batch, target_len, hidden_dim) - Output hidden states.
        """
        batch, target_len, _ = x.shape

        queries_new = self.q_proj(x)
        keys_new = self.k_proj(x)
        values_new = self.v_proj(x)

        queries_new = self.q_norm(
            queries_new.reshape(batch, target_len, self.n_heads, -1)
        ).transpose(0, 2, 1, 3)
        keys_new = self.k_norm(keys_new.reshape(batch, target_len, self.n_kv_heads, -1)).transpose(
            0, 2, 1, 3
        )
        values_new = values_new.reshape(batch, target_len, self.n_kv_heads, -1)

        key_cache_global, value_cache_global = cache

        queries_rotated_list = []
        keys_rotated_list = []

        for i in range(batch):
            current_pos = int(context_lengths[i]) - 1 if target_len == 1 else 0
            q_slice = queries_new[i : i + 1]
            k_slice = keys_new[i : i + 1]
            q_rot = self.rope(q_slice, offset=current_pos)
            k_rot = self.rope(k_slice, offset=current_pos)
            queries_rotated_list.append(q_rot)
            keys_rotated_list.append(k_rot)

        queries_rotated = mx.concatenate(queries_rotated_list, axis=0)
        keys_rotated = mx.concatenate(keys_rotated_list, axis=0)

        block_size = key_cache_global.shape[3]

        reshape_and_cache(
            keys_rotated.transpose(0, 2, 1, 3),
            values_new,
            key_cache_global,
            value_cache_global,
            block_tables,
            context_lengths,
            block_size,
            layer_idx,
            slot_mapping=slot_mapping,
        )

        # 3. Compute Attention
        if target_len == 1:
            # Decode Phase: Use Paged Attention Kernel
            output = paged_attention(
                queries_rotated,
                key_cache_global,
                value_cache_global,
                block_tables,
                context_lengths,
                block_size,
                self.scale,
                self.n_kv_heads,
                layer_idx,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)
        else:
            # Prefill Phase: Use Standard Self-Attention on local data
            output = scaled_dot_product_attention(
                queries_rotated,
                keys_rotated,
                values_new.transpose(0, 2, 1, 3),
                scale=self.scale,
                mask=mask,
                cache=None,
            )
            output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output)


class ParallaxQwen3MoeBlock(MLXQwen3MoeBlock):
    """A custom transformer block for Parallax, extending the Qwen3 Block class.
    This version handles the KV cache explicitly and returns new K and V states.
    """

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args, layer_idx)
        self.self_attn = ParallaxQwen3MoeAttention(args, layer_idx)
        self.layer_idx = layer_idx

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
    ):
        r = self.self_attn(
            self.input_layernorm(x),
            mask,
            cache,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
            layer_idx=self.layer_idx,
        )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "Qwen3MoeForCausalLM"


EntryClass = ParallaxQwen3MoeBlock


================================================================================
File: src/parallax/models/qwen3_next.py
Size: 8.94 kB
================================================================================

"""
hidden_dimefines the Qwen3 model.
"""

from typing import Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
from mlx_lm.models.base import scaled_dot_product_attention
from mlx_lm.models.gated_delta import gated_delta_update
from mlx_lm.models.qwen3_next import ModelArgs
from mlx_lm.models.qwen3_next import Qwen3NextAttention as MLXQwen3NextAttention
from mlx_lm.models.qwen3_next import Qwen3NextDecoderLayer as MLXQwen3NextBlock
from mlx_lm.models.qwen3_next import Qwen3NextGatedDeltaNet as MLXQwen3NextGatedDeltaNet


class ParallaxQwen3NextAttention(MLXQwen3NextAttention):
    """A custom attention module for Parallax, extending the Qwen3 Attention class.

    We apply explicit KV cache handling and passing in `offset` directly from Request.
    This version returns the new K and V states for external caching.
    """

    def __init__(self, args: ModelArgs):
        super().__init__(args)
        self.hidden_size = args.hidden_size
        self.num_v_heads = args.linear_num_value_heads
        self.num_k_heads = args.linear_num_key_heads
        self.head_k_dim = args.linear_key_head_dim
        self.head_v_dim = args.linear_value_head_dim
        self.conv_kernel_size = args.linear_conv_kernel_dim
        self.key_dim = self.head_k_dim * self.num_k_heads
        self.value_dim = self.head_v_dim * self.num_v_heads
        self.conv_dim = self.key_dim * 2 + self.value_dim

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        offset: int = 0,
        state_cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> Tuple[mx.array, Tuple[mx.array, mx.array]]:
        """
        Attention forward pass with explicit KV cache handling.

        Args:
            x: (batch, target_len, hidden_dim) - Input hidden states for the current query segment.
            mask: (batch, n_q_heads, target_len, source_len)
            cache: Optional tuple (past_k, past_v).
                   shape: (batch, n_kv_heads, S_past_padded, head_dim)
            offset: source_len_padded (scalar, used for RoPE calculation).

        Returns:
            output_h: (batch, target_len, hidden_dim) - Output hidden states.
            new_k: (batch, n_kv_heads, target_len, head_dim) - New keys for this segment.
            new_v: (batch, n_kv_heads, target_len, head_dim) - New values for this segment.
        """
        batch, target_len, _ = x.shape
        # print("inputs shape:", x.shape)
        # print(f"x.value --- IGNORE --- {x}")

        queries_new = self.q_proj(x)
        keys_new = self.k_proj(x)
        values_new = self.v_proj(x)

        queries_new, gate = mx.split(
            queries_new.reshape(batch, target_len, self.num_attention_heads, -1), 2, axis=-1
        )
        gate = gate.reshape(batch, target_len, -1)
        queries_new = self.q_norm(queries_new).transpose(0, 2, 1, 3)
        keys_new = self.k_norm(
            keys_new.reshape(batch, target_len, self.num_key_value_heads, -1)
        ).transpose(0, 2, 1, 3)
        values_new = values_new.reshape(batch, target_len, self.num_key_value_heads, -1).transpose(
            0, 2, 1, 3
        )

        queries_rotated = self.rope(queries_new, offset=offset)
        keys_rotated = self.rope(keys_new, offset=offset)

        if cache is not None:
            past_k, past_v = cache
            if past_k is not None and past_v is not None:
                if past_k.shape[2] != offset:
                    raise ValueError(
                        f"ParallaxAttention: Expected past_k sequence length {past_k.shape[2]} "
                        f"to match RoPE offset {offset} (S_past_padded)."
                    )
                final_keys_for_attn = mx.concatenate([past_k, keys_rotated], axis=2)
                final_values_for_attn = mx.concatenate([past_v, values_new], axis=2)
            else:
                raise ValueError("cache was provided but one of k/v was None.")
        else:
            final_keys_for_attn = keys_rotated
            final_values_for_attn = values_new

        output = scaled_dot_product_attention(
            queries_rotated,
            final_keys_for_attn,
            final_values_for_attn,
            scale=self.scale,
            mask=mask,
            cache=None,
        )

        output = output.transpose(0, 2, 1, 3).reshape(batch, target_len, -1)

        return self.o_proj(output * mx.sigmoid(gate)), (
            keys_rotated,
            values_new,
            (
                state_cache[0]
                if (state_cache is not None)
                else mx.zeros((batch, self.conv_kernel_size - 1, self.conv_dim), dtype=x.dtype)
            ),
            (
                state_cache[1]
                if (state_cache is not None)
                else mx.zeros(
                    (batch, self.num_v_heads, self.head_k_dim, self.head_v_dim), dtype=x.dtype
                )
            ),
        )


class ParallaxQwen3NextGatedDeltaNet(MLXQwen3NextGatedDeltaNet):
    def __init__(self, args: ModelArgs):
        super().__init__(args)
        self.num_key_value_heads = args.num_key_value_heads
        self.head_dim = args.head_dim

    def __call__(
        self,
        inputs,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        state_cache: Optional[Tuple[mx.array, mx.array]] = None,
    ):
        B, S, _ = inputs.shape
        # print(f"inputs.value --- IGNORE --- {inputs}")
        q, k, v, z, b, a = self.fix_query_key_value_ordering(
            self.in_proj_qkvz(inputs), self.in_proj_ba(inputs)
        )

        if state_cache is not None and state_cache[0] is not None:
            conv_state = state_cache[0]
        else:
            conv_state = mx.zeros(
                (B, self.conv_kernel_size - 1, self.conv_dim),
                dtype=inputs.dtype,
            )

        mixed_qkv = mx.concatenate(
            [q.reshape(B, S, -1), k.reshape(B, S, -1), v.reshape(B, S, -1)], axis=-1
        )
        conv_input = mx.concatenate([conv_state, mixed_qkv], axis=1)

        state0 = conv_input[:, -(self.conv_kernel_size - 1) :]
        conv_out = nn.silu(self.conv1d(conv_input))

        q, k, v = [
            t.reshape(B, S, h, d)
            for t, h, d in zip(
                mx.split(conv_out, [self.key_dim, 2 * self.key_dim], -1),
                [self.num_k_heads, self.num_k_heads, self.num_v_heads],
                [self.head_k_dim, self.head_k_dim, self.head_v_dim],
            )
        ]
        if state_cache is not None:
            state1 = state_cache[1]
        else:
            state1 = None

        inv_scale = k.shape[-1] ** -0.5
        q = (inv_scale**2) * mx.fast.rms_norm(q, None, 1e-6)
        k = inv_scale * mx.fast.rms_norm(k, None, 1e-6)

        out, state1 = gated_delta_update(q, k, v, a, b, self.A_log, self.dt_bias, state1)

        out = self.norm(out, z)
        return self.out_proj(out.reshape(B, S, -1)), (
            (
                cache[0][..., :S, :]
                if cache is not None
                else mx.zeros((B, self.num_key_value_heads, S, self.head_dim), dtype=inputs.dtype)
            ),
            (
                cache[1][..., :S, :]
                if cache is not None
                else mx.zeros((B, self.num_key_value_heads, S, self.head_dim), dtype=inputs.dtype)
            ),
            state0,
            state1,
        )


class ParallaxQwen3NextBlock(MLXQwen3NextBlock):
    """A custom transformer block for Parallax, extending the Qwen3 Block class.
    This version handles the KV cache explicitly and returns new K and V states.
    """

    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__(args, layer_idx)
        self.layer_idx = layer_idx
        if self.is_linear:
            self.linear_attn = ParallaxQwen3NextGatedDeltaNet(args)
        else:
            self.self_attn = ParallaxQwen3NextAttention(args)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        offset: int = 0,
        lengths: Optional[mx.array] = None,
        state_cache: Optional[Tuple[mx.array, mx.array]] = None,
    ):
        if self.is_linear:
            r, (k_cache, v_cache, state0, state1) = self.linear_attn(
                self.input_layernorm(x), cache, state_cache
            )
        else:
            r, (k_cache, v_cache, state0, state1) = self.self_attn(
                self.input_layernorm(x), mask, cache, offset, state_cache
            )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out, (k_cache, v_cache, state0, state1)

    @classmethod
    def get_architecture(cls):
        """Get the architecture name for the block."""
        return "Qwen3NextForCausalLM"


EntryClass = ParallaxQwen3NextBlock


================================================================================
File: src/parallax/p2p/message_util.py
Size: 7.74 kB
================================================================================

ï»¿"""
Utility functions for message serialization and deserialization.

This module contains utility functions for serializing and deserializing messages
between the P2P server and the executor.
"""

import io
from typing import Any, List, Optional

import mlx.core as mx

from parallax.p2p.proto import forward_pb2
from parallax.server.request import IntermediateRequest, Request, RequestStatus
from parallax.server.sampling.sampling_params import SamplingParams


def request_to_proto(
    requests: List[IntermediateRequest],
    device: Optional[str] = "mlx",
) -> forward_pb2.ForwardRequest:
    """
    Convert a list of IntermediateRequest objects to a ForwardRequest protobuf message.
    IntermediateRequest contains request_id, current_position, status, and hidden_states.
    """
    forward_request = forward_pb2.ForwardRequest()
    assert len(requests) > 0, "No requests to convert"
    assert all(
        request.status == requests[0].status for request in requests
    ), "All requests must have the same status"
    if requests[0].status == RequestStatus.PREFILLING:
        forward_request.forward_mode = forward_pb2.ForwardMode.EXTEND
    elif requests[0].status == RequestStatus.DECODING:
        forward_request.forward_mode = forward_pb2.ForwardMode.DECODE
    else:
        raise ValueError(f"Invalid status: {requests[0].status}")

    for request in requests:
        proto_req = forward_pb2.Req()
        proto_req.rid = request.request_id
        proto_req.output_length = request.current_position - len(request.input_ids)
        proto_req.input_ids.extend(request.input_ids)
        proto_req.routing_table.extend(request.routing_table)
        proto_req.sampling_params.CopyFrom(sampling_params_to_proto(request.sampling_params))
        proto_req.lora_path = request.lora_path if request.lora_path is not None else ""

        if request.hidden_states is not None:
            proto_req.hidden_states = tensor_to_bytes(request.hidden_states, device=device)

        if request.next_token_id is not None:
            proto_req.next_token_id = request.next_token_id

        forward_request.reqs.append(proto_req)

    return forward_request


def proto_to_request(
    proto_request: forward_pb2.ForwardRequest,
    device: Optional[str] = "mlx",
) -> List[IntermediateRequest]:
    """
    Convert a ForwardRequest protobuf message to a IntermediateRequest object.
    """

    requests = []

    for proto_req in proto_request.reqs:
        current_position = len(proto_req.input_ids) + proto_req.output_length

        next_token_id = proto_req.next_token_id

        hidden_states = None
        if proto_req.hidden_states:
            hidden_states = bytes_to_tensor(proto_req.hidden_states, device)

        status = None
        if hidden_states is None:
            status = RequestStatus.FINISHED_EOS
        elif proto_request.forward_mode == forward_pb2.ForwardMode.EXTEND:
            status = RequestStatus.PREFILLING
        elif proto_request.forward_mode == forward_pb2.ForwardMode.DECODE:
            status = RequestStatus.DECODING
        else:
            raise ValueError(f"Invalid forward mode: {proto_request.forward_mode}")

        sampling_params = proto_to_sampling_params(proto_req.sampling_params)

        request = IntermediateRequest(
            request_id=proto_req.rid,
            current_position=current_position,
            status=status,
            input_ids=list(proto_req.input_ids),
            hidden_states=hidden_states,
            routing_table=list(proto_req.routing_table),
            next_token_id=next_token_id,
            sampling_params=sampling_params,
            lora_path=proto_req.lora_path if proto_req.lora_path != "" else None,
        )

        requests.append(request)

    return requests


def abort_request_to_proto(reqs: List[Request]) -> forward_pb2.AbortRequest:
    """Converts aborted/finished requests to a AbortRequest"""
    proto = forward_pb2.AbortRequest()
    for req in reqs:
        req_proto = forward_pb2.Req()
        req_proto.rid = req.request_id
        if req.routing_table is not None:
            req_proto.routing_table.extend(req.routing_table)
        proto.reqs.append(req_proto)
    return proto


def proto_to_abort_request(proto_request: forward_pb2.AbortRequest) -> List[IntermediateRequest]:
    """
    Converts a AbortRequest a list of IntermediateRequest objects.
    Only request_id and routing table are useful information.
    """
    status = RequestStatus.FINISHED_EOS
    requests = []
    for proto_req in proto_request.reqs:
        request = IntermediateRequest(
            request_id=proto_req.rid,
            current_position=0,
            status=status,
            routing_table=proto_req.routing_table,
        )

        requests.append(request)

    return requests


def proto_to_sampling_params(proto: forward_pb2.SamplingParams) -> SamplingParams:
    """Convert protobuf message to SamplingParams."""
    if proto is None:
        return SamplingParams()
    sampling_params = SamplingParams(
        max_new_tokens=proto.max_new_tokens,
        min_new_tokens=proto.min_new_tokens,
        temperature=proto.temperature,
        top_p=proto.top_p,
        min_p=proto.min_p,
        top_k=proto.top_k,
        stop_strs=list(proto.stop_strs),
        stop_token_ids=list(proto.stop_token_ids),
        ignore_eos=proto.ignore_eos,
        repetition_penalty=proto.repetition_penalty,
        presence_penalty=proto.presence_penalty,
        frequency_penalty=proto.frequency_penalty,
        json_schema=proto.json_schema,
    )
    return sampling_params


def sampling_params_to_proto(params: SamplingParams) -> forward_pb2.SamplingParams:
    """Convert SamplingParams to protobuf message."""
    proto = forward_pb2.SamplingParams()

    proto.max_new_tokens = params.max_new_tokens
    proto.min_new_tokens = params.min_new_tokens
    proto.temperature = params.temperature
    proto.top_p = params.top_p
    proto.min_p = params.min_p
    proto.top_k = params.top_k
    if params.stop_strs is not None:
        proto.stop_strs.extend(params.stop_strs)
    if params.stop_token_ids is not None:
        proto.stop_token_ids.extend(params.stop_token_ids)
    proto.ignore_eos = params.ignore_eos
    proto.repetition_penalty = params.repetition_penalty
    proto.presence_penalty = params.presence_penalty
    proto.frequency_penalty = params.frequency_penalty
    if params.json_schema is not None:
        proto.json_schema = params.json_schema
    return proto


def tensor_to_bytes(tensor: Any, device: Optional[str] = "mlx") -> bytes:
    """Convert tensor to protobuf Tensor using safetensor serialization."""
    if device == "cuda":
        from safetensors.torch import save

        # Convert tensor to CPU
        if tensor.device.type != "cpu":
            cpu_tensor = tensor.cpu()
        else:
            cpu_tensor = tensor
        # Store buffer using safetensor (dtype and size are automatically preserved)
        serialized_data = save({"tensor": cpu_tensor.contiguous()})
        return serialized_data
    else:
        assert tensor.size > 0, "Tensor must have size > 0"
        buffer = io.BytesIO()
        mx.save_safetensors(buffer, {"tensor": tensor})
        return buffer.getvalue()


def bytes_to_tensor(
    tensor: bytes,
    device: Optional[str] = "mlx",
) -> Any:
    """Convert bytes (safetensor format) to tensor."""
    if device == "cuda":
        from safetensors.torch import load

        tensor_dict = load(tensor)
        tensor = tensor_dict["tensor"].to(device)
    else:
        buffer = io.BytesIO(tensor)
        tensors_dict = mx.load(buffer, format="safetensors")
        tensor = tensors_dict["tensor"]
    return tensor


================================================================================
File: src/parallax/p2p/proto/forward.proto
Size: 1.02 kB
================================================================================

ï»¿// python -m grpc_tools.protoc -I. --python_out=.  src/parallax/p2p/proto/forward.proto

syntax = "proto3";
package gradient;

enum ForwardMode {
  EXTEND = 0;
  DECODE = 1;
  MIXED = 2;
}

message ForwardRequest {
  ForwardMode forward_mode = 1;
  repeated Req reqs = 2;
}

message ForwardResponse {
}


message AbortRequest {
  repeated Req reqs = 1;
}

message AbortResponse {
}

message Req {
  string rid = 1;
  int32 output_length = 2;
  repeated string routing_table = 3;

  repeated int32 input_ids = 4;
  SamplingParams sampling_params = 5;

  int32 next_token_id = 6;
  bytes hidden_states = 7;
  string lora_path = 8;
}

message SamplingParams {
  int32 max_new_tokens = 1;
  int32 min_new_tokens = 2;
  float temperature = 3;
  float top_p = 4;
  float min_p = 5;
  int32 top_k = 6;
  repeated int32 stop_token_ids = 7;
  bool ignore_eos = 8;
  repeated string stop_strs = 9;
  float repetition_penalty = 10;
  float presence_penalty = 11;
  float frequency_penalty = 12;
  string json_schema = 13;
}


================================================================================
File: src/parallax/p2p/proto/forward_pb2.py
Size: 3.24 kB
================================================================================

# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: src/parallax/p2p/proto/forward.proto
# Protobuf Python Version: 6.31.1
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC,
    6,
    31,
    1,
    '',
    'src/parallax/p2p/proto/forward.proto'
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n$src/parallax/p2p/proto/forward.proto\x12\x08gradient\"Z\n\x0e\x46orwardRequest\x12+\n\x0c\x66orward_mode\x18\x01 \x01(\x0e\x32\x15.gradient.ForwardMode\x12\x1b\n\x04reqs\x18\x02 \x03(\x0b\x32\r.gradient.Req\"\x11\n\x0f\x46orwardResponse\"+\n\x0c\x41\x62ortRequest\x12\x1b\n\x04reqs\x18\x01 \x03(\x0b\x32\r.gradient.Req\"\x0f\n\rAbortResponse\"\xc7\x01\n\x03Req\x12\x0b\n\x03rid\x18\x01 \x01(\t\x12\x15\n\routput_length\x18\x02 \x01(\x05\x12\x15\n\rrouting_table\x18\x03 \x03(\t\x12\x11\n\tinput_ids\x18\x04 \x03(\x05\x12\x31\n\x0fsampling_params\x18\x05 \x01(\x0b\x32\x18.gradient.SamplingParams\x12\x15\n\rnext_token_id\x18\x06 \x01(\x05\x12\x15\n\rhidden_states\x18\x07 \x01(\x0c\x12\x11\n\tlora_path\x18\x08 \x01(\t\"\xa7\x02\n\x0eSamplingParams\x12\x16\n\x0emax_new_tokens\x18\x01 \x01(\x05\x12\x16\n\x0emin_new_tokens\x18\x02 \x01(\x05\x12\x13\n\x0btemperature\x18\x03 \x01(\x02\x12\r\n\x05top_p\x18\x04 \x01(\x02\x12\r\n\x05min_p\x18\x05 \x01(\x02\x12\r\n\x05top_k\x18\x06 \x01(\x05\x12\x16\n\x0estop_token_ids\x18\x07 \x03(\x05\x12\x12\n\nignore_eos\x18\x08 \x01(\x08\x12\x11\n\tstop_strs\x18\t \x03(\t\x12\x1a\n\x12repetition_penalty\x18\n \x01(\x02\x12\x18\n\x10presence_penalty\x18\x0b \x01(\x02\x12\x19\n\x11\x66requency_penalty\x18\x0c \x01(\x02\x12\x13\n\x0bjson_schema\x18\r \x01(\t*0\n\x0b\x46orwardMode\x12\n\n\x06\x45XTEND\x10\x00\x12\n\n\x06\x44\x45\x43ODE\x10\x01\x12\t\n\x05MIXED\x10\x02\x62\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'src.parallax.p2p.proto.forward_pb2', _globals)
if not _descriptor._USE_C_DESCRIPTORS:
  DESCRIPTOR._loaded_options = None
  _globals['_FORWARDMODE']._serialized_start=723
  _globals['_FORWARDMODE']._serialized_end=771
  _globals['_FORWARDREQUEST']._serialized_start=50
  _globals['_FORWARDREQUEST']._serialized_end=140
  _globals['_FORWARDRESPONSE']._serialized_start=142
  _globals['_FORWARDRESPONSE']._serialized_end=159
  _globals['_ABORTREQUEST']._serialized_start=161
  _globals['_ABORTREQUEST']._serialized_end=204
  _globals['_ABORTRESPONSE']._serialized_start=206
  _globals['_ABORTRESPONSE']._serialized_end=221
  _globals['_REQ']._serialized_start=224
  _globals['_REQ']._serialized_end=423
  _globals['_SAMPLINGPARAMS']._serialized_start=426
  _globals['_SAMPLINGPARAMS']._serialized_end=721
# @@protoc_insertion_point(module_scope)


================================================================================
File: src/parallax/p2p/server.py
Size: 38.98 kB
================================================================================

"""
P2P server for Parallax.

This module contains the P2P server for Parallax.

It is used to handle the communication between the peers, and communicate with the executor by zmq.

"""

import dataclasses
import enum
import json
import multiprocessing
import threading
import time
from typing import List, Optional

import dijkstar
import httpx
import zmq
from lattica import ConnectionHandler, Lattica, rpc_method, rpc_stream, rpc_stream_iter

from backend.server.rpc_connection_handler import RPCConnectionHandler
from parallax.p2p.proto import forward_pb2
from parallax.p2p.utils import AsyncWorker
from parallax.server.server_info import detect_node_hardware
from parallax.utils.shared_state import SharedState
from parallax.utils.utils import get_zmq_socket
from parallax_utils.logging_config import get_logger, set_log_level

logger = get_logger(__name__)

# Global HTTP client for reuse
_http_client = None


async def get_http_client():
    """Get or create a shared HTTP client"""
    global _http_client
    if _http_client is None or _http_client.is_closed:
        _http_client = httpx.AsyncClient(
            timeout=httpx.Timeout(3),  # 3 second timeout
            limits=httpx.Limits(max_connections=100, max_keepalive_connections=100),
        )
    return _http_client


class ServerState(enum.Enum):
    """Server state enum."""

    JOINING = "joining"
    INITIALIZING = "initializing"
    READY = "ready"
    OFFLINE = "offline"
    ERROR = "error"


@dataclasses.dataclass
class ServerInfo:
    """Server info data class."""

    state: ServerState
    throughput: Optional[float] = None
    max_batch_size: Optional[int] = None
    max_sequence_len: Optional[int] = None
    error_message: Optional[str] = None


def send_notify(notify_url, block_start_index, block_end_index, request, status):
    payload = [
        {
            "session_id": req.rid,
            "step_id": req.output_length + (block_start_index == 0 and status == "started"),
            "block_idx": block_start_index,
            "total_blocks": block_end_index - block_start_index,
            "status": status,
        }
        for req in request.reqs
    ]

    logger.info(f"Send {status} notification, batch size: {len(payload)}")

    if notify_url is not None:

        async def send_async(notify_url, payload):
            try:
                client = await get_http_client()
                await client.post(notify_url, json=payload)
            except Exception as e:
                logger.exception(f"Error in send_async: {e}")

        if not hasattr(send_notify, "async_worker"):
            send_notify.async_worker = AsyncWorker()
        send_notify.async_worker.run_coroutine(send_async(notify_url, payload), return_future=True)


class TransformerConnectionHandler(ConnectionHandler):
    """
    Handles RPC requests from clients, forwarding them to the appropriate TransformerBackend.
    Inherits from hivemind's ConnectionHandler.
    """

    def __init__(
        self,
        lattica: Lattica,
        recv_from_peer_addr: str,
        send_to_peer_addr: str,
        block_start_index: int,
        block_end_index: int,
        http_port: Optional[int] = None,
        notify_url: Optional[str] = None,
    ):
        # Initialize the base class
        super().__init__(lattica)
        self.recv_from_peer_addr = recv_from_peer_addr
        self.send_to_peer_addr = send_to_peer_addr
        self.block_start_index = block_start_index
        self.block_end_index = block_end_index
        self.http_port = http_port
        self.notify_url = notify_url
        self._recv_from_peer = None
        self._recv_from_peer_lock = threading.Lock()

    @property
    def recv_from_peer(self):
        if self._recv_from_peer is None:
            self._recv_from_peer = get_zmq_socket(
                zmq.Context(2), zmq.PUSH, self.recv_from_peer_addr, True
            )
        return self._recv_from_peer

    @rpc_stream
    def rpc_pp_forward(
        self,
        request: forward_pb2.ForwardRequest,
    ) -> forward_pb2.ForwardResponse:
        """Handle forward pass request with explicit proxy tensors support"""
        try:
            send_notify(
                self.notify_url, self.block_start_index, self.block_end_index, request, "started"
            )
            with self._recv_from_peer_lock:
                self.recv_from_peer.send_multipart([b"forward", request.SerializeToString()])
        except Exception as e:
            logger.exception(f"Error in rpc_pp_forward: {e}")
        return forward_pb2.ForwardResponse()

    @rpc_method
    def rpc_abort(
        self,
        request: forward_pb2.AbortRequest,
    ) -> forward_pb2.AbortResponse:
        try:
            with self._recv_from_peer_lock:
                self.recv_from_peer.send_multipart([b"abort", request.SerializeToString()])
        except Exception as e:
            logger.exception(f"Error in rpc_abort: {e}")
        return forward_pb2.AbortResponse()

    @rpc_stream_iter
    def chat_completion(
        self,
        request,
    ):
        """Handle chat completion request"""
        logger.debug(f"Chat completion request: {request}, type: {type(request)}")
        try:
            with httpx.Client(timeout=10 * 60, proxy=None, trust_env=False) as client:
                if request.get("stream", False):
                    with client.stream(
                        "POST",
                        f"http://localhost:{self.http_port}/v1/chat/completions",
                        json=request,
                    ) as response:
                        for chunk in response.iter_bytes():
                            if chunk:
                                yield chunk
                else:
                    response = client.post(
                        f"http://localhost:{self.http_port}/v1/chat/completions", json=request
                    ).json()
                    yield json.dumps(response).encode()
        except Exception as e:
            logger.exception(f"Error in chat completion: {e}")
            yield b"internal server error"


class GradientServer:
    """
    Main server class for Parallax.

    This class handles communication between peers and communicates with the executor by zmq.
    """

    def __init__(
        self,
        recv_from_peer_addr: str,
        send_to_peer_addr: str,
        initial_peers: List[str] = [],
        scheduler_addr: Optional[str] = None,
        relay_servers: List[str] = [],
        block_start_index: int = 0,
        block_end_index: int = 1,
        hidden_layers: int = 128,
        tp_size: int = 1,
        dht_prefix: str = "gradient",
        host_maddrs: List[str] = [],
        http_port: Optional[int] = None,
        announce_maddrs: List[str] = [],
        notify_url: str = None,
        model_name: Optional[str] = None,
        max_batch_size: Optional[int] = None,
        max_sequence_length: Optional[int] = None,
        param_mem_ratio: float = 0.65,
        kvcache_mem_ratio: float = 0.25,
    ):
        self.recv_from_peer_addr = recv_from_peer_addr
        self.send_to_peer_addr = send_to_peer_addr
        self.initial_peers = initial_peers
        self.scheduler_addr = scheduler_addr
        self.relay_servers = relay_servers
        self.block_start_index = block_start_index
        self.block_end_index = block_end_index
        self.hidden_layers = hidden_layers
        self.tp_size = tp_size
        self.dht_prefix = dht_prefix
        self.host_maddrs = host_maddrs
        self.announce_maddrs = announce_maddrs
        self.http_port = http_port
        self.notify_url = notify_url
        self.model_name = model_name
        self.max_batch_size = max_batch_size
        self.max_sequence_length = max_sequence_length
        self.param_mem_ratio = param_mem_ratio
        self.kvcache_mem_ratio = kvcache_mem_ratio
        self.prefix_id = f"{dht_prefix}_announce"
        self.lattica = None
        self.routing_table = None
        self.routing_table_update_interval = 10
        self.server_info = ServerInfo(state=ServerState.JOINING)
        self.stubs = {}
        self.rtts = {}
        self.rtt_last_update = 0
        self.rtt_update_interval = 60
        self.status = ServerState.JOINING
        self.manual_layer_assignment = block_end_index is not None and block_start_index is not None

        self.scheduler_stub = None
        self.scheduler_peer_id = None
        self.routing_table_updater = None
        self.announcer = None
        self.connection_handler = None
        self.stop_event = threading.Event()
        logger.debug(f"manual_layer_assignment: {self.manual_layer_assignment}")
        self._layer_allocation_changed = False
        self._shared_state = None  # Will be set if running in subprocess mode

    def _sync_to_shared_state(self):
        """Sync current layer allocation and status to shared state if available"""
        if hasattr(self, "_shared_state") and self._shared_state is not None:
            self._shared_state.update(
                block_start_index=self.block_start_index,
                block_end_index=self.block_end_index,
                model_name=self.model_name,
                tp_size=self.tp_size,
                status=self.status.value,
                _layer_allocation_changed=self._layer_allocation_changed,
            )

    def build_lattica(self):
        self.lattica = Lattica.builder().with_listen_addrs(self.host_maddrs)

        if self.scheduler_addr is not None and self.scheduler_addr != "auto":
            if self.scheduler_addr.startswith("/"):
                logger.info(f"Using scheduler addr: {self.scheduler_addr}")
                self.lattica.with_bootstraps([self.scheduler_addr])
            self.scheduler_peer_id = self.scheduler_addr.split("/")[-1]

        if len(self.relay_servers) > 0:
            logger.info(f"Using relay servers: {self.relay_servers}")
            self.lattica.with_relay_servers(self.relay_servers).with_dcutr(True)
            if self.scheduler_peer_id is not None:
                logger.info(f"Using protocol: /{self.scheduler_peer_id}")
                self.lattica.with_protocol("/" + self.scheduler_peer_id)

        if len(self.announce_maddrs) > 0:
            logger.info(f"Using announce maddrs: {self.announce_maddrs}")
            self.lattica.with_external_addrs(self.announce_maddrs)

        if len(self.initial_peers) > 0:
            logger.info(f"Using initial peers: {self.initial_peers}")
            self.lattica.with_bootstraps(self.initial_peers)

        self.lattica.build()

        if len(self.relay_servers) > 0:
            try:
                is_symmetric_nat = self.lattica.is_symmetric_nat()
                if is_symmetric_nat is None:
                    logger.warning("Failed to get is symmetric NAT, skip")
                elif is_symmetric_nat:
                    logger.error(
                        "Your network NAT type is symmetric, relay does not work on this type of NAT, see https://en.wikipedia.org/wiki/Network_address_translation"
                    )
                    exit(1)
            except Exception as e:
                logger.exception(f"Error in is symmetric NAT: {e}")

        if self.scheduler_addr == "auto":
            self.scheduler_peer_id = None
            for _ in range(20):
                try:
                    time.sleep(3)
                    self.scheduler_peer_id = self.lattica.get("scheduler_peer_id")
                    if self.scheduler_peer_id is not None:
                        self.scheduler_peer_id = self.scheduler_peer_id.value
                        logger.info(f"Found scheduler peer id: {self.scheduler_peer_id}")
                        break
                    logger.info(
                        f"Discovering scheduler peer id, {_ + 1} times, you can specify scheduler peer id by -s"
                    )
                except Exception as e:
                    logger.warning(f"Failed to get scheduler addr: {e}, waiting for 3 seconds.")
            if self.scheduler_peer_id is None:
                logger.error("Failed to get scheduler peer id")
                return False

        return True

    def run(self):
        if self.build_lattica():
            logger.info("Lattica built successfully")
        else:
            logger.error("Failed to build lattica")
            exit(1)

        if self.scheduler_addr is not None:  # central scheduler mode
            try:
                self.scheduler_stub = RPCConnectionHandler(self.lattica, None, None).get_stub(
                    self.scheduler_peer_id
                )
                node_info = self.get_node_info()
                if node_info == {}:
                    logger.error("Failed to get node info, try again after 10 seconds")
                    self.lattica.close()
                    self.lattica = None
                    time.sleep(10)
                    return self.run()

                if self.manual_layer_assignment:
                    node_info["manual_layer_assignment"] = True

                response = self.scheduler_stub.node_join(node_info)
                response = response.result(timeout=300)
                if response == {}:
                    logger.error("Failed to join scheduler")
                    exit(1)

                logger.info(f"Join scheduler response: {response}")

                if not self.manual_layer_assignment:
                    self.block_start_index = response.get("start_layer")
                    self.block_end_index = response.get("end_layer")
                self.model_name = response.get("model_name")
                self.tp_size = response.get("tp_size")

                # Sync to shared state if available
                self._sync_to_shared_state()

            except Exception as e:
                logger.exception(f"Error in join scheduler: {e}")
                exit(1)
        else:  # no scheduler mode
            self.start_routing_table_updater()  # thread

        self.connection_handler = TransformerConnectionHandler(
            lattica=self.lattica,
            recv_from_peer_addr=self.recv_from_peer_addr,
            send_to_peer_addr=self.send_to_peer_addr,
            block_start_index=self.block_start_index,
            block_end_index=self.block_end_index,
            http_port=self.http_port,
            notify_url=self.notify_url,
        )  # thread

        self.start_node_announcer()  # thread
        self.start_node_sender()  # main loop

    def find_servers(self):
        """Find available servers in the DHT network"""
        # Find all announced blocks
        server_blocks = []
        block_servers = self.lattica.get(self.prefix_id)
        if block_servers is None:
            return []
        for peer_id, value in block_servers.value.items():
            server_blocks.append(
                {
                    "peer_id": peer_id,
                    "block_start_index": value.value["block_start_index"],
                    "block_end_index": value.value["block_end_index"],
                }
            )

        return server_blocks

    def get_stub(self, peer_id):
        if peer_id not in self.stubs:
            self.stubs[peer_id] = self.connection_handler.get_stub(peer_id)
        return self.stubs[peer_id]

    def start_routing_table_updater(self):
        def _updater_thread():
            while True and not self.stop_event.is_set():
                try:
                    graph = dijkstar.Graph()
                    servers = self.find_servers()
                    for server in servers:
                        start_index = server["block_start_index"]
                        end_index = server["block_end_index"]
                        peer_id = server["peer_id"]
                        graph.add_edge(start_index, end_index, (1, peer_id))
                    try:
                        path = dijkstar.find_path(
                            graph,
                            self.block_end_index,
                            self.hidden_layers,
                            cost_func=lambda u, v, e, prev_path: e[0],
                        )
                        routing_table = [self.lattica.peer_id()] + [edge[1] for edge in path.edges]
                        if self.routing_table != routing_table:
                            self.routing_table = routing_table
                            logger.info(f"Set routing table: {routing_table}")
                    except dijkstar.NoPathError:
                        self.routing_table = None
                        logger.warning(
                            f"No path found from 0 to {self.hidden_layers}, find servers {servers}"
                        )
                except Exception as e:
                    logger.exception(f"Error in routing table updater: {e}")

                time.sleep(self.routing_table_update_interval)

        if self.block_start_index == 0:
            self.routing_table_updater = threading.Thread(target=_updater_thread, daemon=True)
            self.routing_table_updater.start()

    def start_node_sender(self):
        send_to_peer = get_zmq_socket(zmq.Context(2), zmq.PULL, self.send_to_peer_addr, True)

        def group_requests_by_next_peer(requests: List[forward_pb2.Req]):
            grouped_requests = {}
            for req in requests:
                assert len(req.routing_table) > 0, "Request routing table is not set"
                try:
                    self_index = list(req.routing_table).index(self.lattica.peer_id())
                except ValueError as exc:
                    raise RuntimeError("Can not find self in the routing table") from exc

                next_peer_id = req.routing_table[(self_index + 1) % len(req.routing_table)]
                if next_peer_id not in grouped_requests:
                    grouped_requests[next_peer_id] = []
                grouped_requests[next_peer_id].append(req)
            if len(grouped_requests) > 1:
                logger.warning(
                    f"Grouped requests by next peer: {len(grouped_requests)}, {grouped_requests.keys()}"
                )
            return grouped_requests

        while True and not self.stop_event.is_set():
            try:
                if (
                    self.scheduler_addr is None
                    and self.block_start_index == 0
                    and self.routing_table is None
                ):
                    logger.info("Routing table is not ready in head rank, waiting for it to be set")
                    time.sleep(self.routing_table_update_interval)
                    continue

                message_type, message_body = send_to_peer.recv_multipart()[:2]

                if message_type == b"forward":
                    forward_request = forward_pb2.ForwardRequest()
                    forward_request.ParseFromString(message_body)
                    if len(forward_request.reqs) == 0:
                        raise RuntimeError("No requests in the forward request")

                    requests = []
                    for req in forward_request.reqs:
                        # set routing table if not scheduler mode
                        if len(req.routing_table) == 0 and self.scheduler_addr is None:
                            assert (
                                self.block_start_index == 0
                            ), "Request routing table is not set for non-head rank"

                            req.routing_table.extend(self.routing_table)
                            logger.info(
                                f"Set routing table {self.routing_table} for request {req.rid}"
                            )

                        if len(req.routing_table) > 0:
                            requests.append(req)
                        else:
                            logger.error(f"Request {req.rid} has no routing table, drop it")

                    grouped_requests = group_requests_by_next_peer(requests)

                    for next_peer_id, requests in grouped_requests.items():
                        stub = self.get_stub(next_peer_id)
                        start = time.time()
                        logger.info(f"Start forwarding data to {next_peer_id}")
                        new_forward_request = forward_pb2.ForwardRequest()
                        new_forward_request.forward_mode = forward_request.forward_mode
                        new_forward_request.reqs.extend(requests)
                        response = stub.rpc_pp_forward(new_forward_request)
                        response.result()
                        send_notify(
                            self.notify_url,
                            self.block_start_index,
                            self.block_end_index,
                            new_forward_request,
                            "completed",
                        )

                        logger.info(
                            f"Forwarding data to {next_peer_id}, "
                            f"total size: {len(message_body) / (1024 * 1024):.3f} MB, "
                            f"cost time: {(time.time() - start) * 1000:.3f} ms, "
                            f"speed: {len(message_body) / (time.time() - start) / (1024 * 1024):.3f} MB/s"
                        )

                elif message_type == b"abort":
                    abort_request = forward_pb2.AbortRequest()
                    abort_request.ParseFromString(message_body)
                    if len(abort_request.reqs) == 0:
                        raise RuntimeError("No requests in the abort request")

                    grouped_requests = {}
                    for req in abort_request.reqs:
                        # set routing table if not scheduler mode
                        if len(req.routing_table) == 0 and self.scheduler_addr is None:
                            assert (
                                self.block_start_index == 0
                            ), "Request routing table is not set for non-head rank"

                            req.routing_table.extend(self.routing_table)
                            logger.info(
                                f"Set routing table {self.routing_table} for request {req.rid}"
                            )

                        if len(req.routing_table) > 0:
                            # broadcast to all other nodes
                            for peer_id in req.routing_table:
                                if peer_id not in grouped_requests:
                                    grouped_requests[peer_id] = []
                                grouped_requests[peer_id].append(req)
                        else:
                            logger.error(f"Abort Request {req.rid} has no routing table, drop it")

                    for peer_id, requests in grouped_requests.items():
                        if peer_id != self.lattica.peer_id():
                            stub = self.get_stub(peer_id)
                            logger.info(
                                f"Send abort request: {[r.rid for r in requests]} to: {peer_id}"
                            )
                            new_abort_request = forward_pb2.AbortRequest()
                            new_abort_request.reqs.extend(requests)
                            stub.rpc_abort(new_abort_request)
                else:
                    logger.error(f"Unknown message type: {message_type}")

            except Exception as e:
                logger.exception(f"Error in handle_request: {e}")
                time.sleep(1)

    def start_node_announcer(self):
        """Start a thread that regularly announces this module's presence on DHT"""

        def _announcer_thread():
            try:
                while not self.stop_event.is_set():
                    # Announce the range ID
                    try:
                        if self.scheduler_peer_id is not None:
                            response_future = self.scheduler_stub.node_update(
                                self.get_node_info(is_update=True)
                            )
                            # Get the response result
                            response = (
                                response_future.result(timeout=30)
                                if hasattr(response_future, "result")
                                else response_future
                            )

                            # Print layer allocation information
                            if response and isinstance(response, dict):
                                start_layer = response.get("start_layer")
                                end_layer = response.get("end_layer")
                                model_name = response.get("model_name")
                                if start_layer is not None and end_layer is not None:
                                    logger.debug(
                                        f"Heartbeat: Node {self.lattica.peer_id()}... "
                                        f"Model: {model_name}, Layers: [{start_layer}, {end_layer})"
                                    )
                                    # Check if layer allocation changed
                                    if (
                                        start_layer != self.block_start_index
                                        or end_layer != self.block_end_index
                                        or model_name != self.model_name
                                    ):
                                        logger.warning(
                                            f"Layer allocation changed! "
                                            f"Current: [{self.block_start_index}, {self.block_end_index}) -> "
                                            f"New: [{start_layer}, {end_layer}) "
                                            f"Model: {self.model_name} -> {model_name}"
                                        )
                                        # Update layer allocation
                                        self.block_start_index = start_layer
                                        self.block_end_index = end_layer
                                        if model_name:
                                            self.model_name = model_name
                                        # Set flag to trigger executor reload
                                        self._layer_allocation_changed = True
                                        # Set status to INITIALIZING to prevent scheduler from sending requests
                                        # during rebalancing
                                        self.status = ServerState.INITIALIZING

                                        # Sync to shared state if available
                                        self._sync_to_shared_state()

                                        logger.info(
                                            "Layer allocation updated. Executor will reload on next check. "
                                            "Status set to INITIALIZING to prevent new requests."
                                        )
                                else:
                                    logger.debug(
                                        f"Heartbeat: Missing layer info - start_layer={start_layer}, "
                                        f"end_layer={end_layer}, response={response}"
                                    )
                            else:
                                logger.warning(
                                    f"Heartbeat: No layer allocation received yet, response: {response}"
                                )
                                self.status = ServerState.JOINING
                                self.model_name = None
                                if self._shared_state is not None:
                                    self._shared_state.set_status(self.status.value)
                                    self._shared_state.set("model_name", None)
                                logger.debug(
                                    "Status set to JOINING and model_name to None because no valid layer allocation received yet."
                                )
                        else:
                            self.lattica.store(
                                key=self.prefix_id,
                                subkey=self.lattica.peer_id(),
                                value={
                                    "block_start_index": self.block_start_index,
                                    "block_end_index": self.block_end_index,
                                },
                                expiration_time=time.time() + 60,  # Valid for 60 seconds
                            )
                    except Exception as e:
                        logger.warning(
                            f"Failed to announce {self.prefix_id}_{self.lattica.peer_id()}: {e}",
                            exc_info=True,
                        )

                    time.sleep(10)
            except Exception as e:
                logger.exception(f"Module announcer thread error: {e}")

        # Start announcer thread
        self.announcer = threading.Thread(target=_announcer_thread, daemon=True)
        self.announcer.start()
        logger.info(
            f"Started node announcer thread (daemon={self.announcer.daemon}, alive={self.announcer.is_alive()})"
        )

    def _get_status(self) -> str:
        """Get current status, checking shared_state if available (subprocess mode)"""
        # When running in subprocess mode, check shared_state status
        if hasattr(self, "_shared_state") and self._shared_state is not None:
            shared_status = self._shared_state.get_status()
            if shared_status is not None:
                return shared_status
        # When running in same process, use local status
        return self.status.value

    def get_node_info(self, is_update: bool = False):
        # update rtt to nodes
        if time.time() - self.rtt_last_update > self.rtt_update_interval:
            self.rtts = {}
            all_peers = []
            for _ in range(1 if is_update else 10):
                all_peers = self.lattica.get_all_peers()
                if len(all_peers) > 0 and self.scheduler_peer_id in all_peers:
                    break
                logger.warning(
                    "No peers found or scheduler peer id not found, waiting for 1 second."
                )
                time.sleep(1)

            if len(all_peers) == 0 or self.scheduler_peer_id not in all_peers:
                logger.warning(
                    "No peers found or scheduler peer id not found, return empty node info."
                )
                return {}

            for peer_id in all_peers:
                rtt = None
                for _ in range(1 if is_update else 30):
                    try:
                        rtt = self.lattica.get_peer_rtt(peer_id) * 1000
                    except Exception as e:
                        logger.warning(f"Failed to get rtt to {peer_id}: {e}")
                    if rtt is not None:
                        break
                    logger.warning(f"Failed to get rtt to {peer_id}, waiting for 1 second.")
                    time.sleep(1)

                self.rtts[peer_id] = rtt if rtt is not None else 100
            self.rtt_last_update = time.time()

        info = {
            "node_id": self.lattica.peer_id(),
            "hardware": detect_node_hardware(self.lattica.peer_id()),
            "kvcache_mem_ratio": self.kvcache_mem_ratio,
            "param_mem_ratio": self.param_mem_ratio,
            "max_concurrent_requests": self.max_batch_size,
            "max_sequence_length": (
                1024 if self.max_sequence_length is None else self.max_sequence_length
            ),
            "rtt_to_nodes": self.rtts,
            "status": self._get_status(),
            "is_active": self._get_status() == ServerState.READY.value,
        }

        # For manual layer assignment, always include start_layer and end_layer
        if self.manual_layer_assignment:
            info["start_layer"] = self.block_start_index
            info["end_layer"] = self.block_end_index
            logger.info(
                f"Manual assignment: sending start_layer={self.block_start_index}, "
                f"end_layer={self.block_end_index}"
            )

        if is_update:
            metrics = {}
            if hasattr(self, "_shared_state") and self._shared_state is not None:
                metrics = self._shared_state.get_metrics()

            info["current_requests"] = metrics.get("current_requests", 0)
            if metrics.get("layer_latency_ms") is not None:
                info["layer_latency_ms"] = metrics.get("layer_latency_ms")
            # In update mode, always include current allocation
            if not self.manual_layer_assignment:
                info["start_layer"] = self.block_start_index
                info["end_layer"] = self.block_end_index

        return info

    def shutdown(self):
        self.stop_event.set()

        self.status = ServerState.OFFLINE
        # Sync final status to shared state
        self._sync_to_shared_state()
        if self.scheduler_addr is not None:
            logger.info(f"Leave scheduler: {self.lattica.peer_id()}")
            self.scheduler_stub.node_leave(self.get_node_info(is_update=True))

        if self.announcer is not None:
            self.announcer.join()
        if self.routing_table_updater is not None:
            self.routing_table_updater.join()
        if self.lattica is not None:
            self.lattica.close()


def _run_p2p_server_process(
    initial_peers: List[str],
    scheduler_addr: Optional[str],
    relay_servers: List[str],
    pp_start_layer: int,
    pp_end_layer: int,
    hidden_layers: int,
    tp_size: int,
    tcp_port: int,
    udp_port: int,
    dht_prefix: str,
    announce_maddrs: List[str],
    http_port: Optional[int],
    notify_url: str,
    recv_from_peer_addr: str,
    send_to_peer_addr: str,
    model_name: Optional[str],
    max_batch_size: Optional[int] = None,
    max_sequence_length: Optional[int] = None,
    param_mem_ratio: float = 0.65,
    kvcache_mem_ratio: float = 0.25,
    shared_state: Optional[dict] = None,
    log_level: str = "INFO",
):
    """Run P2P server in subprocess"""
    # Set log level in subprocess (spawn mode doesn't inherit log configuration)
    set_log_level(log_level)
    server = None
    try:
        server = GradientServer(
            recv_from_peer_addr=recv_from_peer_addr,
            send_to_peer_addr=send_to_peer_addr,
            initial_peers=initial_peers,
            scheduler_addr=scheduler_addr,
            relay_servers=relay_servers,
            block_start_index=pp_start_layer,
            block_end_index=pp_end_layer,
            hidden_layers=hidden_layers,
            tp_size=tp_size,
            dht_prefix=dht_prefix,
            host_maddrs=[
                f"/ip4/0.0.0.0/tcp/{tcp_port}",
                f"/ip4/0.0.0.0/udp/{udp_port}/quic-v1",
            ],
            announce_maddrs=announce_maddrs,
            http_port=http_port,
            notify_url=notify_url,
            model_name=model_name,
            max_batch_size=max_batch_size,
            max_sequence_length=max_sequence_length,
            param_mem_ratio=param_mem_ratio,
            kvcache_mem_ratio=kvcache_mem_ratio,
        )
        # Attach shared state to server for syncing layer allocation
        if shared_state is not None:
            shared_state = SharedState(shared_state)  # Auto-converts dict to SharedState
            server._shared_state = shared_state
            # Initialize shared state with current values
            shared_state.update(
                block_start_index=server.block_start_index,
                block_end_index=server.block_end_index,
                model_name=server.model_name,
                tp_size=server.tp_size,
                status=server.status.value,
            )

        server.run()
    except KeyboardInterrupt:
        logger.debug("P2P server received interrupt signal, shutting down...")
    except Exception as e:
        logger.exception(f"P2P server error: {e}")
    finally:
        if server is not None:
            server.shutdown()


def launch_p2p_server_process(
    initial_peers: List[str],
    scheduler_addr: Optional[str],
    relay_servers: List[str],
    pp_start_layer: int,
    pp_end_layer: int,
    hidden_layers: int,
    tp_size: int,
    tcp_port: int,
    udp_port: int,
    dht_prefix: str,
    announce_maddrs: List[str],
    http_port: Optional[int],
    notify_url: str,
    recv_from_peer_addr: str,
    send_to_peer_addr: str,
    model_name: Optional[str],
    max_batch_size: Optional[int] = None,
    max_sequence_length: Optional[int] = None,
    param_mem_ratio: float = 0.65,
    kvcache_mem_ratio: float = 0.25,
    shared_state: Optional[dict] = None,
    log_level: str = "INFO",
) -> multiprocessing.Process:
    """Launch P2P server as a subprocess and return the process object

    Args:
        shared_state: Optional shared dictionary for inter-process communication.
                     If provided, layer allocation info will be synced to this dict.
        log_level: Log level for the subprocess (default: INFO).
    """
    process = multiprocessing.Process(
        target=_run_p2p_server_process,
        args=(
            initial_peers,
            scheduler_addr,
            relay_servers,
            pp_start_layer,
            pp_end_layer,
            hidden_layers,
            tp_size,
            tcp_port,
            udp_port,
            dht_prefix,
            announce_maddrs,
            http_port,
            notify_url,
            recv_from_peer_addr,
            send_to_peer_addr,
            model_name,
            max_batch_size,
            max_sequence_length,
            param_mem_ratio,
            kvcache_mem_ratio,
            shared_state,
            log_level,
        ),
    )
    process.start()
    return process


def stop_p2p_server(p2p_server_process: Optional[multiprocessing.Process]):
    """Stop P2P server subprocess"""
    if p2p_server_process is not None and p2p_server_process.is_alive():
        logger.debug("Terminating P2P server subprocess...")
        try:
            p2p_server_process.terminate()
            p2p_server_process.join(timeout=5)
            if p2p_server_process.is_alive():
                logger.warning("P2P server process did not terminate gracefully, killing...")
                p2p_server_process.kill()
                p2p_server_process.join()
        except Exception as e:
            logger.error(f"Failed to terminate P2P server subprocess: {e}")


================================================================================
File: src/parallax/p2p/utils.py
Size: 1.8 kB
================================================================================

ï»¿"""
Utility functions for P2P server.

This module contains utility functions for the P2P server.
"""

import asyncio
import os
from concurrent.futures import Future
from threading import Thread
from typing import Awaitable

import uvloop


def switch_to_uvloop() -> asyncio.AbstractEventLoop:
    """stop any running event loops; install uvloop; then create, set and return a new event loop"""
    try:
        # if we're in jupyter, get rid of its built-in event loop
        asyncio.get_event_loop().stop()
    except RuntimeError:
        pass  # this allows running DHT from background threads with no event loop
    uvloop.install()
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    return loop


class AsyncWorker:
    """
    Async worker class for Parallax.

    This class is used to run coroutines in a separate thread.
    """

    def __init__(self) -> None:
        self._event_thread = None
        self._event_loop_fut = None
        self._pid = None

    def _run_event_loop(self):
        try:
            loop = switch_to_uvloop()
            self._event_loop_fut.set_result(loop)
        except Exception as e:
            self._event_loop_fut.set_exception(e)
        loop.run_forever()

    def run_coroutine(self, coro: Awaitable, return_future: bool = False):
        """Run a coroutine in a separate thread."""
        if self._event_thread is None or self._pid != os.getpid():
            self._pid = os.getpid()
            self._event_loop_fut = Future()
            self._event_thread = Thread(target=self._run_event_loop, daemon=True)
            self._event_thread.start()

        loop = self._event_loop_fut.result()
        future = asyncio.run_coroutine_threadsafe(coro, loop)
        return future if return_future else future.result()


================================================================================
File: src/parallax/server/__init__.py
Size: 0 B
================================================================================



================================================================================
File: src/parallax/server/executor/base_executor.py
Size: 30.7 kB
================================================================================

"""
High-level executor for managing model shards, scheduler, and cache pool on each Peer.

Executor handles
1. Loading model shards from the repository;
2. Instantiate scheduler, kv cache manager;
3. Handles tokenization / detokenization if needed;
4. Keep listening to RPC to get requests, feed these to scheduler's request pool;
5. Get batched requests from the scheduler,
    - prepare the MLX tensor input
    - rebuild KV cache
    - feed to model runner;
    For now we process prefill and decode requests separately.
    Later when we have Ragged Paged Flash Attention kernel, we can process both in one batch.
6. Run model forward, our model will returned updated caches,
    kv cache manager will handle updating caches per layer;
7. Get the hidden-states from the model execution.
"""

import time
from abc import abstractmethod
from http import HTTPStatus
from typing import Any, Dict, List, Optional, Tuple

import zmq
from jinja2 import TemplateError
from mlx_lm.server import convert_chat, process_message_content

from parallax.p2p.message_util import (
    abort_request_to_proto,
    proto_to_abort_request,
    proto_to_request,
    request_to_proto,
)
from parallax.p2p.proto import forward_pb2
from parallax.p2p.server import ServerState
from parallax.server.request import (
    InitialRequest,
    IntermediateRequest,
    Request,
    RequestStatus,
)
from parallax.server.sampling.sampling_params import SamplingParams
from parallax.server.scheduler import Scheduler
from parallax.utils.shared_state import SharedState
from parallax.utils.utils import get_current_device, get_device_dtype, get_zmq_socket
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


class BaseExecutor:
    """High-level executor for managing model shards, scheduler, and cache pool on each Peer."""

    def __init__(
        self,
        # Model Configs
        start_layer: int,
        end_layer: int,
        dtype: str = "float16",
        # Device override
        device: Optional[str] = None,
        # Scheduler Configs
        max_batch_size: Optional[int] = 8,
        max_sequence_length: Optional[int] = None,
        # Controlling perfill / decode ratio
        max_num_tokens_per_batch: int = 1024,
        prefill_priority: int = 0,
        micro_batch_ratio: int = 2,
        scheduler_wait_ms: int = 500,
        request_timeout_s: Optional[int] = 600,
        # Metrics Configs
        layer_latency_update_every: int = 4096,
        # Communication Configs
        # P2P Communication Configs
        send_to_peer_addr: Optional[str] = None,
        recv_from_peer_addr: Optional[str] = None,
        # IPC Communication Configs
        executor_input_ipc_addr: Optional[str] = None,
        executor_output_ipc_addr: Optional[str] = None,
        # Tensor Parallel Configs
        tp_rank: Optional[int] = 0,
        tp_size: Optional[int] = 1,
        # Optional shared state for layer reallocation detection (when running in subprocess)
        shared_state: Optional[dict] = None,
    ):
        # Backend
        if device is not None:
            self.device = device
        else:
            self.device = get_current_device()
        logger.debug(f"Executor initializing on device: {self.device}")

        # for window attention need to calculate causal mask size
        self.finished_batch = []
        self.start_layer = start_layer
        self.end_layer = end_layer
        self._should_stop = False  # Flag to gracefully stop the executor
        # Reference to shared state for layer reallocation detection (when in subprocess mode)
        if shared_state is not None:
            self.shared_state = SharedState(shared_state)  # Auto-converts dict to SharedState
        else:
            self.shared_state = None

        self.is_first_peer = start_layer == 0
        self.is_last_peer = end_layer == self.config.get("num_hidden_layers")
        self.tp_size = tp_size
        self.tp_rank = tp_rank

        # Metrics throttling for per-layer latency updates
        self.layer_latency_update_every = int(max(1, layer_latency_update_every))
        self._decode_steps_since_metric = self.layer_latency_update_every

        # TODO: Duplicate code to MLXExecutor.
        self.num_shard_layers = end_layer - start_layer
        self.dtype = get_device_dtype(dtype, self.device)
        logger.debug(
            f"Executor dtype set to {dtype} (resolved={self.dtype}); shard_layers={self.num_shard_layers}"
        )

        if self.tokenizer.pad_token_id is None:
            self.pad_token_id = self.tokenizer.eos_token_id
        else:
            self.pad_token_id = self.tokenizer.pad_token_id

        self.eos_token_id = self.config.get("eos_token_id", None)

        # Scheduler: derive final max_batch_size with KV constraints
        # Remove this for now as it's not working on gpu devices
        # max_batch_size = compute_max_batch_size(
        #     requested_max_batch_size=max_batch_size,
        #     max_sequence_len=max_sequence_length,
        #     device=self.device,
        #     kv_cache_memory_fraction=kv_cache_memory_fraction,
        #     num_shard_layers=self.num_shard_layers,
        #     num_key_value_heads=self.num_key_value_heads,
        #     head_dim=self.head_dim,
        #     dtype=self.dtype,
        # )

        self.scheduler = Scheduler(
            max_batch_size=max_batch_size,
            max_num_tokens_per_batch=max_num_tokens_per_batch,
            prefill_priority=prefill_priority,
            scheduler_wait_ms=scheduler_wait_ms,
            micro_batch_ratio=micro_batch_ratio,
            is_first_peer=self.is_first_peer,
            tokenizer=self.tokenizer,
            eos_token_id=self.eos_token_id,
            kv_cache_manager=self.kv_cache_manager if self.device == "mlx" else None,
            request_timeout_s=request_timeout_s,
            shared_state=self.shared_state,
        )
        logger.debug(
            f"Scheduler initialized (max_batch_size={max_batch_size}, max_tokens={max_num_tokens_per_batch}, wait_ms={scheduler_wait_ms})"
        )

        # Communication Related
        if self.tp_rank == 0:
            self.zmq_context = zmq.Context()
            if recv_from_peer_addr:
                self.recv_from_peer_socket = get_zmq_socket(
                    self.zmq_context, zmq.PULL, recv_from_peer_addr, bind=False
                )
            if send_to_peer_addr:
                self.send_to_peer_socket = get_zmq_socket(
                    self.zmq_context, zmq.PUSH, send_to_peer_addr, bind=False
                )
            if executor_input_ipc_addr:
                self.recv_from_ipc_socket = get_zmq_socket(
                    self.zmq_context, zmq.PULL, executor_input_ipc_addr, bind=False
                )
            if executor_output_ipc_addr:
                self.send_to_ipc_socket = get_zmq_socket(
                    self.zmq_context, zmq.PUSH, executor_output_ipc_addr, bind=False
                )
        if self.shared_state is not None:
            self.shared_state.set_status(ServerState.READY.value)

    @abstractmethod
    def handle_input_requests(self, requests: List[Request]):
        """Update requests states and status in scheduler and cache manager."""

    @abstractmethod
    def process_batch(self, prepared_inputs: Dict[str, Any], return_decoded_tokens: bool = True):
        """
        Process a batch of requests.

        Args:
            prepared_inputs: A dictionary containing the prepared inputs for the ShardedModel.
            return_decoded_tokens: Whether to return decoded tokens.

        Returns:
            A tensor of shape (B, L, D) containing the hidden states for the next peer.
            or (B,) containing the decoded tokens.
        """

    @abstractmethod
    def _prepare_prefill_batch(self, batched_requests: List[Request]) -> Dict[str, Any]:
        """Prepares inputs for ShardedModel from a batch of prefill requests."""

    @abstractmethod
    def _prepare_decode_batch(self, batched_requests: List[Request]) -> Dict[str, Any]:
        """Prepares inputs for ShardedModel from a batch of decode requests."""

    @abstractmethod
    def _gen_token_id_from_hidden(self, hidden_states) -> Tuple[int, Any]:
        """
        Inplace modifies hidden_states.
        Returns token_id, hidden_states
        """

    @abstractmethod
    def _release_request(self, rid: str):
        """Release request in backend frameworks"""

    def recv_requests_from_http(self) -> List[Request]:
        """Receives requests from http frontend"""
        if self.tp_rank != 0:
            return []

        recv_reqs = []
        while True:
            try:
                raw_request = self.recv_from_ipc_socket.recv_pyobj(zmq.NOBLOCK)

                # Check if this is an abort request
                if isinstance(raw_request, dict) and raw_request.get("type") == "abort":
                    logger.debug(
                        f"Received abort request from HTTP for request ID: {raw_request.get('rid')}"
                    )
                    self.scheduler.cancel_request(raw_request.get("rid"))
                else:
                    # Normal request processing - do tokenization and form InitialRequest
                    req = self._handle_raw_request(raw_request)
                    recv_reqs.append(req)
            except zmq.ZMQError:
                break
            except Exception as e:
                logger.exception(f"Error receiving http request: {e}")
                self._notify_http_request_error(raw_request, e)
        if len(recv_reqs) > 0:
            logger.debug(f"Received {len(recv_reqs)} HTTP requests")
        return recv_reqs

    def recv_requests_from_peer(self) -> List[Request]:
        """Receives requests from the RPC server."""
        if self.tp_rank == 0:
            recv_reqs = []
            while True:
                try:
                    recv_req = self.recv_from_peer_socket.recv_multipart(zmq.NOBLOCK)
                    assert len(recv_req) == 2, f"Received invalid request: {recv_req}"
                    if recv_req[0] == b"forward":
                        # Create a new ForwardRequest instance and parse from bytes
                        forward_request = forward_pb2.ForwardRequest()
                        forward_request.ParseFromString(recv_req[1])
                        recv_req = proto_to_request(forward_request, self.device)

                        # Convert hidden_states dtype if necessary
                        if recv_req is not None and len(recv_req) > 0:
                            for req in recv_req:
                                if req.hidden_states is not None:
                                    if req.hidden_states.dtype != self.dtype:
                                        logger.debug(
                                            f"Converting hidden_states dtype from {req.hidden_states.dtype} to {self.dtype} for request {req.request_id}"
                                        )
                                        if self.device == "cuda":
                                            req.hidden_states = req.hidden_states.to(self.dtype)
                                        elif self.device == "mlx":
                                            req.hidden_states = req.hidden_states.astype(self.dtype)
                                        else:
                                            raise ValueError(
                                                f"Unsupported device type: {self.device}"
                                            )

                        # Move current position for first peer
                        if self.is_first_peer:
                            for req in recv_req:
                                req.current_position += 1
                        recv_reqs.extend(recv_req)
                    elif recv_req[0] == b"abort":
                        abort_request = forward_pb2.AbortRequest()
                        abort_request.ParseFromString(recv_req[1])
                        recv_req = proto_to_abort_request(abort_request)
                        recv_reqs.extend(recv_req)
                    else:
                        raise ValueError(f"Unknown request type: {recv_req[0]}")
                    # First peer is responsible for tokenization
                    # if self.is_first_peer and isinstance(recv_req, InitialRequest):
                    #     recv_req.input_ids = self.tokenizer.encode(recv_req.prompt)
                    #     recv_req.prompt_len = len(recv_req.input_ids)
                    #     recv_req.max_total_length = min(
                    #         recv_req.max_total_length, recv_req.prompt_len + recv_req.max_new_tokens
                    #     )

                except zmq.ZMQError:
                    break
                except Exception as e:
                    logger.exception(f"Error receiving or deserializing request: {e}")
        else:
            recv_reqs = []

        return recv_reqs

    def prepare_batch_inputs(self, batched_requests: List[Request]) -> Optional[Dict[str, Any]]:
        """Prepares inputs for ShardedModel from a batch of requests.
        Args:
            batched_requests: A list of requests to prepare inputs for.

        Returns:
            A dictionary containing the prepared inputs for the ShardedModel.
            The dictionary contains "prefill_batch" and "decode_batch",
            with the prepared inputs for the corresponding request type.

            For now we process prefill and decode requests separately.
            Later when we have Ragged Paged Flash Attention kernel,
            we can process both in one batch.
        """
        if len(batched_requests) == 0:
            return None

        prefill_reqs: List[Request] = []
        decode_reqs: List[Request] = []
        for req in batched_requests:
            if req.is_prefill:
                prefill_reqs.append(req)
            elif req.is_decoding:
                decode_reqs.append(req)
        prefill_batch = self._prepare_prefill_batch(prefill_reqs)
        decode_batch = self._prepare_decode_batch(decode_reqs)
        if prefill_batch is None and decode_batch is None:
            return None
        if prefill_batch is not None:
            logger.debug(f"Prepared prefill batch with {len(prefill_batch['requests'])} requests.")
        if decode_batch is not None:
            logger.debug(f"Prepared decode batch with {len(decode_batch['requests'])} requests.")
        return {
            "prefill_batch": prefill_batch,
            "decode_batch": decode_batch,
        }

    def prepare_next_batch_requests(
        self, requests: List[Request], hidden_states: Any, context_lengths: Any
    ) -> List[Request]:
        """Prepares a batch of requests for the next stage of the pipeline."""
        if self.tp_rank == 0:
            batched_requests = []
            pre_length = 0
            for i, src_request in enumerate(requests):
                if self.is_last_peer:
                    # Last peer gets a 1D array of token IDs
                    hidden_state_for_req = hidden_states[i : i + 1]
                else:
                    # Other peers get a 3D array of hidden states
                    if src_request.is_prefill:
                        true_length = int(context_lengths[i])
                        if hidden_states.ndim == 3:
                            hidden_state_for_req = hidden_states[i, :true_length, :]
                        else:
                            hidden_state_for_req = hidden_states[
                                pre_length : pre_length + true_length, :
                            ]
                        pre_length += true_length
                    else:
                        if hidden_states.ndim == 3:
                            hidden_state_for_req = hidden_states[i, :, :]
                        else:
                            hidden_state_for_req = hidden_states[pre_length : pre_length + 1, :]
                        pre_length += 1

                next_req = self._prepare_next_single_request(src_request, hidden_state_for_req)
                batched_requests.append(next_req)
        else:
            batched_requests = None

        return batched_requests

    def release_and_evict_request(self, rid: str):
        """Release per-request resources and evict from scheduler. Best-effort, never raises."""
        # Release resources
        self._release_request(rid)

        # Evict from scheduler
        try:
            self.scheduler.evict_request(rid)
        except Exception:
            pass

    def run_loop(self):
        """The main loop of the executor."""
        logger.debug(
            f"Executor for layers [{self.start_layer}, {self.end_layer}) starting run loop..."
        )
        self._should_stop = False
        while not self._should_stop:
            received_requests = []

            # Receive requests from http frontend
            if self.is_first_peer:
                received_requests = self.recv_requests_from_http()

            # Receive requests from peer
            received_requests.extend(self.recv_requests_from_peer())

            self.handle_input_requests(received_requests)

            # Send finished batch to next peer
            if len(self.finished_batch) > 0 and self.is_first_peer and self.tp_rank == 0:
                self.send_to_peer_socket.send_multipart(
                    [b"abort", abort_request_to_proto(self.finished_batch).SerializeToString()]
                )
                self.finished_batch = []

            # Check for layer reallocation signal (before batch processing)
            layer_changed = False
            if self.shared_state is not None:
                layer_changed = self.shared_state.get_layer_allocation_changed()

            if layer_changed:
                logger.info(
                    "Layer reallocation detected. Stopping executor to reload with new layers."
                )
                self._should_stop = True
                break

            # 5. Admit requests into running set up to capacity, then form batch
            self.scheduler.admit_requests()
            # 5.1 Check for request timeouts and abort timed out requests
            try:
                timed_out_reqs = self.scheduler.get_timed_out_requests()
                if timed_out_reqs:
                    for req in timed_out_reqs:
                        rid = req.request_id
                        logger.warning(
                            f"Request {rid} exceeded timeout ({req.timeout_s}s). Aborting and releasing resources."
                        )
                        self.release_and_evict_request(rid)

                        # Notify downstream peers to abort if this peer is the first peer in a pipeline
                        if self.is_first_peer and not self.is_last_peer:
                            self.finished_batch.append(req)
            except Exception:
                # Non-fatal; continue serving
                pass
            batch_to_process = self.scheduler.form_batch()
            if not batch_to_process:
                continue
            logger.debug(f"Formed batch with {len(batch_to_process)} requests.")

            # 6. Process the batch
            try:
                prepared_inputs_dict = self.prepare_batch_inputs(batch_to_process)

                # We will process prefill and decode batches separately for now
                for batch_type in ["prefill_batch", "decode_batch"]:
                    if prepared_inputs_dict and prepared_inputs_dict.get(batch_type):
                        prepared_inputs = prepared_inputs_dict[batch_type]

                        start_time = time.time()
                        output = self.process_batch(
                            prepared_inputs, return_decoded_tokens=self.is_last_peer
                        )
                        # Update metrics with per-layer latency sample (throttled by decode steps)
                        if batch_type == "decode_batch":
                            try:
                                self._decode_steps_since_metric += len(prepared_inputs["requests"])
                                if (
                                    self._decode_steps_since_metric
                                    >= self.layer_latency_update_every
                                ):
                                    elapsed_ms = (time.time() - start_time) * 1000.0
                                    assert self.num_shard_layers > 0
                                    per_layer_ms = elapsed_ms / float(self.num_shard_layers)
                                    if self.shared_state is not None:
                                        self.shared_state.update_metrics(
                                            layer_latency_ms_sample=per_layer_ms
                                        )
                                    self._decode_steps_since_metric = 0
                            except Exception:
                                pass
                        # 7. Prepare requests for the next stage in the pipeline
                        next_batch = self.prepare_next_batch_requests(
                            requests=prepared_inputs["requests"],
                            hidden_states=output,
                            context_lengths=prepared_inputs.get("context_lengths"),
                        )

                        # 8. Dispatch to the appropriate destination
                        if self.tp_rank == 0:
                            if self.is_last_peer and self.is_first_peer:
                                # Single node: handle locally
                                self.handle_input_requests(next_batch)
                            else:
                                # Send output to next peer
                                self.send_to_peer_socket.send_multipart(
                                    [
                                        b"forward",
                                        request_to_proto(
                                            next_batch, self.device
                                        ).SerializeToString(),
                                    ]
                                )
                                logger.debug(
                                    f"Processed batch of type {batch_type} with {len(next_batch)} requests "
                                    f"in {(time.time() - start_time) * 1000:.3f} ms"
                                )

            except Exception as e:
                logger.exception(f"Error processing batch: {e}")
                # Naive error handling: release and evict all requests in the batch
                for req in batch_to_process:
                    self.release_and_evict_request(req.request_id)

    def run_loop_in_background(self):
        """Run the executor loop in the background."""

    def shutdown(self):
        """Shuts down the executor."""
        logger.debug("Executor shutting down...")
        self._should_stop = True
        import time

        time.sleep(0.1)  # Give run_loop a moment to exit gracefully

        try:
            all_requests = [req for _, _, _, req in self.scheduler._request_queue] + list(
                self.scheduler._running_requests.values()
            )
            for req in all_requests:
                try:
                    self.scheduler.evict_request(req.request_id, RequestStatus.CANCELLED)
                except Exception:
                    pass
        except Exception:
            pass

        try:
            if self.tp_rank == 0:
                self.recv_from_peer_socket.close()
                self.send_to_peer_socket.close()
                self.recv_from_ipc_socket.close()
                self.send_to_ipc_socket.close()
                self.zmq_context.term()
        except Exception as e:
            logger.debug(f"Error closing sockets (may already be closed): {e}")

        logger.debug("Executor shutdown complete.")

    def _handle_raw_request(self, raw_request: Dict):
        assert "messages" in raw_request, "Request did not contain messages"

        rid = raw_request["rid"]
        if self.tokenizer.chat_template:
            messages = raw_request["messages"]
            process_message_content(messages)
            chat_template_kwargs = raw_request.get("chat_template_kwargs", {})
            # check extra_body for backward compatibility
            if "extra_body" in raw_request and "chat_template_kwargs" in raw_request["extra_body"]:
                chat_template_kwargs.update(raw_request["extra_body"]["chat_template_kwargs"])

            prompt = self.tokenizer.apply_chat_template(
                messages,
                raw_request.get("tools") or None,
                tokenize=True,
                add_generation_prompt=True,
                **chat_template_kwargs,
            )
        else:
            prompt = convert_chat(raw_request["messages"], raw_request.get("role_mapping"))
            prompt = self.tokenizer.encode(prompt)

        max_new_tokens = raw_request.get("max_tokens")
        if max_new_tokens is None:
            max_new_tokens = 2048
        max_total_length = len(prompt) + max_new_tokens

        lora_path = raw_request.get("lora_path")

        raw_sampling_params = raw_request.get("sampling_params")
        if raw_sampling_params is None:
            sampling_params = SamplingParams()
        else:
            # TODO: Support more sampling params
            sampling_params = SamplingParams()
            if "temperature" in raw_sampling_params:
                sampling_params.temperature = raw_sampling_params["temperature"]
            if "top_k" in raw_sampling_params:
                sampling_params.top_k = raw_sampling_params["top_k"]
            if "top_p" in raw_sampling_params:
                sampling_params.top_p = raw_sampling_params["top_p"]
            if "ignore_eos" in raw_sampling_params:
                sampling_params.ignore_eos = raw_sampling_params["ignore_eos"]

        req = InitialRequest(
            request_id=rid,
            output_ids=None,
            input_ids=prompt,
            sampling_params=sampling_params,
            max_new_tokens=max_new_tokens,
            max_total_length=max_total_length,
            lora_path=lora_path,
        )
        if "routing_table" in raw_request:
            req.routing_table = raw_request["routing_table"]
        return req

    def _notify_http_request_error(self, raw_request: Optional[Dict], error: Exception):
        """Best-effort notification to HTTP server when request parsing fails."""
        if not hasattr(self, "send_to_ipc_socket") or self.send_to_ipc_socket is None:
            return
        if not isinstance(raw_request, dict):
            return
        rid = raw_request.get("rid")
        if rid is None:
            return

        is_template_error = isinstance(error, TemplateError)
        status = (
            HTTPStatus.BAD_REQUEST
            if isinstance(error, ValueError) or is_template_error
            else HTTPStatus.INTERNAL_SERVER_ERROR
        )
        payload = {
            "type": "error",
            "rid": rid,
            "error": str(error),
            "error_type": error.__class__.__name__,
            "status_code": status.value,
        }
        try:
            self.send_to_ipc_socket.send_pyobj(payload)
        except Exception:  # pragma: no cover - best effort notification
            logger.debug("Failed to send error notification to HTTP handler", exc_info=True)

    def _prepare_next_single_request(self, request: Request, hidden_states: Any) -> Request:
        """Handle request state changes both inter and intra peers.

        This function prepares the request object to be sent to the *next* peer in the
        pipeline, or back to the first peer if this is the last peer.

        Args:
            request: The request that was just processed by this peer.
            hidden_states: The output hidden_states/output_ids from the model for this request.

        Returns:
            A new Request object ready to be sent to the next destination.
        """
        # This peer is the last peer or a single node.
        if self.is_last_peer and self.is_first_peer:
            assert isinstance(
                request, (InitialRequest, IntermediateRequest)
            ), "Invalid request type for decoding."

            next_token_id, hidden_states = self._gen_token_id_from_hidden(hidden_states)
            return IntermediateRequest(
                request_id=request.request_id,
                status=RequestStatus.DECODING,
                current_position=request.total_length + 1,
                input_ids=request.input_ids,
                hidden_states=hidden_states,
                next_token_id=next_token_id,
                routing_table=request.routing_table,
                lora_path=request.lora_path,
            )
        if self.is_last_peer:
            # Last peer decodes a token and sends it back to the first peer.
            # The token is wrapped in an IntermediateRequest.
            assert isinstance(
                request, IntermediateRequest
            ), "Last peer must receive an IntermediateRequest."

            next_token_id, hidden_states = self._gen_token_id_from_hidden(hidden_states)
            return IntermediateRequest(
                request_id=request.request_id,
                status=RequestStatus.DECODING,  # Last peer always changes status to DECODING
                current_position=request.total_length,
                input_ids=request.input_ids,
                hidden_states=hidden_states,
                next_token_id=next_token_id,
                routing_table=request.routing_table,
                lora_path=request.lora_path,
            )
        # This peer is the first or an intermediate peer.
        if self.is_first_peer:
            assert isinstance(request, InitialRequest), "First peer must process an InitialRequest."
            if request.is_finished:
                hidden_states = None
            return IntermediateRequest.from_initial_request(
                request, hidden_states=hidden_states, lora_path=request.lora_path
            )
        assert isinstance(
            request, IntermediateRequest
        ), "Intermediate peer must process an IntermediateRequest."
        return IntermediateRequest.from_intermediate_request(
            request, hidden_states, lora_path=request.lora_path
        )


================================================================================
File: src/parallax/server/executor/factory.py
Size: 4.05 kB
================================================================================

"""
Creates executor from factory for different backends.
"""

import argparse
from typing import Optional

from parallax.utils.utils import get_current_device
from parallax_utils.logging_config import get_logger, set_log_level

logger = get_logger(__name__)


def create_executor_config(args: argparse.Namespace, shared_state=None):
    """Create executor configuration from command line arguments."""

    config = {
        "model_repo": args.model_path,
        "start_layer": args.start_layer,
        "end_layer": args.end_layer,
        "dtype": args.dtype,
        "max_sequence_length": args.max_sequence_length if "max_sequence_length" in args else None,
        "max_batch_size": args.max_batch_size if "max_batch_size" in args else None,
        "kv_block_size": args.kv_block_size,
        "kv_cache_memory_fraction": args.kv_cache_memory_fraction,
        "enable_prefix_cache": args.enable_prefix_cache,
        "max_num_tokens_per_batch": args.max_num_tokens_per_batch,
        "prefill_priority": args.prefill_priority,
        "micro_batch_ratio": args.micro_batch_ratio,
        "scheduler_wait_ms": args.scheduler_wait_ms,
        "send_to_peer_addr": args.send_to_peer_addr if "send_to_peer_addr" in args else None,
        "recv_from_peer_addr": args.recv_from_peer_addr if "recv_from_peer_addr" in args else None,
        "executor_input_ipc_addr": args.executor_input_ipc,
        "executor_output_ipc_addr": args.executor_output_ipc,
        "attention_backend": args.attention_backend,
        "moe_runner_backend": args.moe_runner_backend,
        "tp_rank": args.tp_rank,
        "tp_size": args.tp_size,
        "nccl_port": args.nccl_port,
        "shared_state": shared_state,
        "use_hfcache": args.use_hfcache,
        "enable_lora": args.enable_lora,
        "max_lora_rank": args.max_lora_rank,
        "lora_target_modules": args.lora_target_modules,
        "lora_paths": args.lora_paths,
        "max_loras_per_batch": args.max_loras_per_batch,
        "max_loaded_loras": args.max_loaded_loras,
        "lora_eviction_policy": args.lora_eviction_policy,
        "lora_backend": args.lora_backend,
        "max_lora_chunk_size": args.max_lora_chunk_size,
    }
    return config


def create_from_args(
    args,
    shared_state: Optional[dict] = None,
    device: Optional[str] = None,
):
    """
    Creat executor for different backend.
    Lazy import here since CUDA modules cannot be import withough hardware support.
    """
    config = create_executor_config(args, shared_state)
    if device is None:
        device = get_current_device()
    if device == "cuda":
        if args.gpu_backend == "sglang":
            from parallax.server.executor.sglang_executor import SGLExecutor

            executor = SGLExecutor(**config)
        elif args.gpu_backend == "vllm":
            from parallax.server.executor.vllm_executor import VLLMExecutor

            executor = VLLMExecutor(**config)
        else:
            raise ValueError(f"Unsupported GPU backend type: {args.gpu_backend}")
    elif device == "mlx":
        from parallax.server.executor.mlx_executor import MLXExecutor

        executor = MLXExecutor(**config)
    else:
        raise ValueError(f"Unsupported device type: {device}")
    return executor


def run_executor_process(args, shared_state=None):
    """Run executor as a subprocess"""
    set_log_level(args.log_level)
    executor = None
    try:
        executor = create_from_args(args, shared_state)
        executor.run_loop()
    except KeyboardInterrupt:
        logger.debug("Executor received interrupt signal, shutting down...")
    except Exception as e:
        logger.exception(e)
    finally:
        if executor is not None:
            executor.shutdown()


def stop_executor_process(executor_process):
    """Kill a subprocess"""
    logger.debug("Terminating executor subprocess...")
    try:
        executor_process.kill()
        executor_process.join()
    except Exception as e:
        logger.error(f"Failed to terminate executor subprocess: {e}")


================================================================================
File: src/parallax/server/executor/mlx_executor.py
Size: 21.81 kB
================================================================================

"""
MLX-LM backend implementation of high level executor
"""

import time
from typing import Any, Dict, List, Optional, Tuple

import mlx.core as mx

from parallax.server.executor.base_executor import BaseExecutor
from parallax.server.paged_kv_cache import PagedKVCacheManager
from parallax.server.request import (
    InitialRequest,
    IntermediateRequest,
    Request,
    RequestStatus,
)
from parallax.server.sampling.sampler import SamplingBatchInfo
from parallax.server.shard_loader import MLXModelLoader
from parallax.utils.utils import (
    combine_padding_and_causal_masks,
    create_causal_mask,
    get_device_dtype,
    pad_inputs,
)
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


class MLXExecutor(BaseExecutor):
    def __init__(
        self,
        # Model Configs
        model_repo: str,
        start_layer: int,
        end_layer: int,
        dtype: str = "float16",
        # Device override
        device: Optional[str] = None,
        use_hfcache: bool = False,
        # Scheduler Configs
        max_batch_size: Optional[int] = 8,
        max_sequence_length: Optional[int] = None,
        max_tokens_in_kv_pool: Optional[int] = None,
        # Controlling perfill / decode ratio
        max_num_tokens_per_batch: int = 1024,
        prefill_priority: int = 0,
        micro_batch_ratio: int = 2,
        scheduler_wait_ms: int = 500,
        request_timeout_s: Optional[int] = 600,
        # Metrics Configs
        layer_latency_update_every: int = 4096,
        # KV Cache Configs
        kv_block_size: int = 64,
        kv_cache_memory_fraction: float = 0.8,
        enable_prefix_cache: Optional[bool] = False,
        # Communication Configs
        # P2P Communication Configs
        send_to_peer_addr: Optional[str] = None,
        recv_from_peer_addr: Optional[str] = None,
        # IPC Communication Configs
        executor_input_ipc_addr: Optional[str] = None,
        executor_output_ipc_addr: Optional[str] = None,
        # GPU Specialized Configs
        attention_backend: Optional[str] = "flashinfer",
        moe_runner_backend: Optional[str] = "auto",
        enable_lora: Optional[bool] = False,
        max_lora_rank: Optional[int] = None,
        lora_target_modules: Optional[List[str]] = None,
        lora_paths: Optional[List[str]] = None,
        max_loras_per_batch: Optional[int] = None,
        max_loaded_loras: Optional[int] = None,
        lora_eviction_policy: Optional[str] = "lru",
        lora_backend: Optional[str] = "triton",
        max_lora_chunk_size: Optional[int] = 128,
        # Tensor Parallel Configs
        tp_rank: Optional[int] = 0,
        tp_size: Optional[int] = 1,
        nccl_port: Optional[int] = 4000,
        # Optional shared state for layer reallocation detection (when running in subprocess)
        shared_state: Optional[dict] = None,
    ):
        logger.debug(
            f"Initializing MLX sharded model loader for repo={model_repo}, layers=[{start_layer}, {end_layer})"
        )
        self.shard_loader = MLXModelLoader(
            model_repo,
            start_layer=start_layer,
            end_layer=end_layer,
            use_hfcache=use_hfcache,
        )
        t0 = time.time()
        self.model_shard, self.config, self.tokenizer = self.shard_loader.load()

        adapters = lora_paths[0] if lora_paths else None
        if adapters:
            logger.debug(f"mlx adapters is: {adapters}")
            self.model_shard = self.shard_loader.load_lora(self.model_shard, adapters)

        logger.debug(
            f"MLX sharded model loaded in {(time.time() - t0) * 1000:.1f} ms; num_layers={self.config.get('num_hidden_layers')}"
        )

        # TODO: Duplicate code to BaseExecutor since num_shard_layers and dtype are needed for initializing kv cache
        self.num_shard_layers = end_layer - start_layer
        self.dtype = get_device_dtype(dtype, device)
        logger.debug(
            f"Executor dtype set to {dtype} (resolved={self.dtype}); shard_layers={self.num_shard_layers}"
        )

        # Calculate feature dimensions for kv cache
        num_key_value_heads = self.config.get("num_key_value_heads")
        head_dim = self.config.get("head_dim") or self.config.get("hidden_size") // self.config.get(
            "num_attention_heads"
        )
        qk_nope_head_dim = self.config.get("qk_nope_head_dim", None)
        qk_rope_head_dim = self.config.get("qk_rope_head_dim", None)
        if qk_nope_head_dim is not None and qk_rope_head_dim is not None:
            logger.debug(
                f"qk_nope_head_dim={qk_nope_head_dim}, qk_rope_head_dim={qk_rope_head_dim}"
            )
            head_dim = qk_nope_head_dim + qk_rope_head_dim

        v_head_dim = self.config.get("v_head_dim", None)
        linear_key_head_dim = self.config.get("linear_key_head_dim", None)
        linear_value_head_dim = self.config.get("linear_value_head_dim", None)
        linear_conv_kernel_dim = self.config.get("linear_conv_kernel_dim", None)
        linear_num_key_heads = self.config.get("linear_num_key_heads", None)
        linear_num_value_heads = self.config.get("linear_num_value_heads", None)
        key_dim, value_dim, conv_dim = None, None, None
        if linear_key_head_dim is not None and linear_num_key_heads is not None:
            key_dim = linear_key_head_dim * linear_num_key_heads
        if linear_value_head_dim is not None and linear_num_value_heads is not None:
            value_dim = linear_value_head_dim * linear_num_value_heads
        if key_dim is not None and value_dim is not None:
            conv_dim = key_dim * 2 + value_dim
        self.using_state_cache = linear_conv_kernel_dim is not None and conv_dim is not None

        logger.debug(
            "Initializing PagedKVCacheManager (mlx) with block_size=%d, layers=%d",
            kv_block_size,
            self.num_shard_layers,
        )
        self.kv_cache_manager = PagedKVCacheManager(
            num_layers=self.num_shard_layers,
            num_kv_heads=num_key_value_heads,
            head_dim=head_dim,
            dtype=self.dtype,
            block_size=kv_block_size,
            cache_memory_fraction=kv_cache_memory_fraction,
            head_dim_v=v_head_dim,
        )
        super().__init__(
            start_layer=start_layer,
            end_layer=end_layer,
            dtype=dtype,
            device=device,
            max_batch_size=max_batch_size,
            max_sequence_length=max_sequence_length,
            max_num_tokens_per_batch=max_num_tokens_per_batch,
            prefill_priority=prefill_priority,
            micro_batch_ratio=micro_batch_ratio,
            scheduler_wait_ms=scheduler_wait_ms,
            request_timeout_s=request_timeout_s,
            layer_latency_update_every=layer_latency_update_every,
            send_to_peer_addr=send_to_peer_addr,
            recv_from_peer_addr=recv_from_peer_addr,
            executor_input_ipc_addr=executor_input_ipc_addr,
            executor_output_ipc_addr=executor_output_ipc_addr,
            tp_rank=tp_rank,
            tp_size=tp_size,
            shared_state=shared_state,
        )

        try:
            mx.set_wired_limit(mx.metal.device_info()["max_recommended_working_set_size"])
        except Exception:
            logger.warning(f"Using mlx without metal backend.")

        # Prefix Cache Manager
        self.enable_prefix_cache = enable_prefix_cache
        # self.prefix_cache = RadixCache(
        #     num_kv_heads=num_key_value_heads,
        #     head_dim=head_dim,
        #     head_dim_v=v_head_dim,
        #     num_layers=self.num_shard_layers,
        #     dtype=self.dtype,
        #     page_size=1,
        # )

        logger.debug(
            f"KVCacheManager ready; wired_limit set; prefix_cache={'on' if self.enable_prefix_cache else 'off'}"
        )

    def handle_input_requests(self, requests: List[Request]):
        """Update requests states and status in scheduler and cache manager."""
        if not requests:
            return
        if self.is_first_peer:
            # First peer can receive InitialRequests from the client RPC,
            # or IntermediateRequests from the last peer.
            for req in requests:
                if isinstance(req, InitialRequest):
                    self.scheduler.enque_request(req)
                elif isinstance(req, IntermediateRequest):
                    original_req = self.scheduler.get_running_request(req.request_id)
                    if original_req is None:
                        logger.warning(
                            f"Received IntermediateRequest {req.request_id}. "
                            "But no corresponding request found in scheduler. "
                            "It might have been cancelled or finished."
                        )
                        continue
                    if not self.kv_cache_manager.has_request(req.request_id):
                        logger.warning(
                            f"Received IntermediateRequest {req.request_id}. "
                            "But no corresponding request found in cache manager. "
                            "It might have been cancelled or finished."
                        )
                        continue

                    assert req.next_token_id is not None
                    original_req.commit_new_token(req.next_token_id)
                    if len(req.routing_table) > 0:
                        original_req.routing_table = req.routing_table

                    # Check for termination.
                    if self.scheduler.check_and_update_request_status(original_req):
                        self.kv_cache_manager.release_request(original_req.request_id)
                        logger.debug(
                            f"Released resources for finished request {req.request_id}, "
                            f"memory usage: {mx.get_active_memory() / 1024**3 :.3f} GB"
                        )
                        if not self.is_last_peer:
                            self.finished_batch.append(req)
                    else:
                        self.scheduler.enque_request(original_req)

                    # detokenize and send to http server
                    if self.tp_rank == 0:
                        req_dict = {
                            "prompt_tokens": len(req.input_ids),
                            "next_token_id": req.next_token_id,
                            "rid": req.request_id,
                        }
                        if original_req.status == RequestStatus.FINISHED_EOS:
                            req_dict["eos"] = True
                        if original_req.status == RequestStatus.FINISHED_MAX_LENGTH:
                            req_dict["length"] = True
                        if hasattr(self, "send_to_ipc_socket"):
                            self.send_to_ipc_socket.send_pyobj(req_dict)
                else:
                    raise TypeError(f"First peer received unexpected request type: {type(req)}")

        else:
            # Intermediate and Last peers receive IntermediateRequests from the previous peer.
            for req in requests:
                assert isinstance(
                    req, IntermediateRequest
                ), "Non-first peers must receive IntermediateRequests."
                if req.is_finished or req.hidden_states is None:
                    if self.enable_prefix_cache:
                        keys, values = self.kv_cache_manager.gather_kv_cache(req.request_id)
                        self.prefix_cache.cache_finished_request(req, keys, values)
                        self.prefix_cache.evict_request(req.request_id)

                    self.kv_cache_manager.release_request(req.request_id)
                    logger.debug(
                        f"Released resources for finished request {req.request_id}, "
                        f"kv cache manager has {self.kv_cache_manager.tokens_in_cache} tokens, "
                        f"memory usage: {mx.get_active_memory() / 1024**3 :.3f} GB"
                    )
                    self.scheduler.evict_request(req.request_id)
                    if not self.is_last_peer:
                        self.finished_batch.append(req)
                else:
                    # This is an active request, add it to the scheduler queue to be processed.
                    self.scheduler.enque_request(req)

    def process_batch(self, prepared_inputs: Dict[str, Any], return_decoded_tokens: bool = True):
        """Process a batch of requests in MLX."""
        # Run model and get updated cache
        # Note: Paged Attention writes KV cache in-place within the model (via reshape_and_cache).
        # The returned 'hidden_states' is what we need.
        # The returned cache tuple (_, _) is ignored/unused here.
        hidden_states = self.model_shard(
            h_or_tokens=prepared_inputs["h_or_tokens"],
            cache=prepared_inputs["cache"],
            mask=prepared_inputs.get("mask"),
            block_tables=prepared_inputs.get("block_tables"),
            context_lengths=prepared_inputs.get("context_lengths"),
            slot_mapping=prepared_inputs.get("slot_mapping"),
        )

        logger.debug(
            f"Processing batch with {len(prepared_inputs['requests'])} requests, "
            f"request status: {prepared_inputs['requests'][0].status}, "
            f"hidden_states shape: {hidden_states.shape}"
        )

        lengths = mx.zeros((len(prepared_inputs["requests"]),), dtype=mx.int32)
        requests = prepared_inputs["requests"]
        for i, req in enumerate(requests):
            if req.is_prefill:
                lengths[i] = prepared_inputs.get("context_lengths")[i]
            elif req.is_decoding:
                lengths[i] = 1
            else:
                continue

        # Note: With PagedAttention, we don't need to explicitly update requests with new K/V
        # because they are written in-place to the global cache.
        # self.kv_cache_manager.update_requests(...) is REMOVED.

        # Update prefix cache (TODO: Adapt to PagedKV)
        if self.enable_prefix_cache:
            pass
            # for _, req in enumerate(requests):
            #    if req.is_prefill:
            #        keys, values = self.kv_cache_manager.gather_kv_cache(req.request_id)
            #        self.prefix_cache.cache_unfinished_request(req, keys, values)

        # Process last peer: need additional sampling + detokenization
        if return_decoded_tokens:
            sampling_info = SamplingBatchInfo.from_reqs(requests)
            return mx.array(
                self.model_shard.logits_to_tokens(hidden_states, lengths, sampling_info)
            )

        return hidden_states

    def _release_request(self, rid: str):
        """Release per-request resources in MLX."""
        try:
            if hasattr(self, "kv_cache_manager") and self.kv_cache_manager is not None:
                self.kv_cache_manager.release_request(rid)
        except Exception:
            pass

    def _gen_token_id_from_hidden(self, hidden_states) -> Tuple[int, Any]:
        """
        Inplace modifies hidden_states.
        Returns token_id, hidden_states
        """
        assert hidden_states.dtype == mx.uint32, "Single node must generate an output_id."
        next_token_id = int(hidden_states[0])
        hidden_states = hidden_states.astype(mx.int32)
        return next_token_id, hidden_states

    def _prepare_prefill_batch(self, batched_requests: List[Request]) -> Dict[str, Any]:
        """Prepares inputs for ShardedModel from a batch of prefill requests."""
        batch_size = len(batched_requests)
        if batch_size == 0:
            return None

        h_or_tokens_list = []
        block_tables_list = []
        context_lengths_list = []

        # TODO: Adapt Prefix Cache to PagedKV

        for req in batched_requests:
            assert req.is_prefill, f"Request {req.request_id} is not a prefill request."
            if self.is_first_peer:
                h_or_tokens_list.append(req.input_ids)
            else:
                h_or_tokens_list.append(req.hidden_states)

            # Allocate Paged KV blocks
            # For first peer and intermediate peers, we allocate based on prompt length
            success = self.kv_cache_manager.allocate_request(req.request_id, req.total_length)
            if not success:
                raise RuntimeError(f"OOM during prefill allocation for {req.request_id}")

            block_table = self.kv_cache_manager.get_block_table(req.request_id)
            block_tables_list.append(block_table)
            # For prefill, context length after this step will be total_length
            context_lengths_list.append(req.total_length)

        if self.is_first_peer:
            padded_inputs, padding_mask = pad_inputs(
                self.pad_token_id, h_or_tokens_list, self.dtype
            )
        else:
            padded_inputs, padding_mask = pad_inputs(0, h_or_tokens_list, self.dtype)

        # Generate slot_mapping (Batch * MaxLen) for prefill
        max_len = padded_inputs.shape[1]
        slot_mapping_flat = []

        for i, req in enumerate(batched_requests):
            block_table = block_tables_list[i]
            length = req.total_length

            for seq_idx in range(max_len):
                if seq_idx < length:
                    # Valid token
                    block_idx = seq_idx // self.kv_cache_manager.block_size
                    block_offset = seq_idx % self.kv_cache_manager.block_size
                    physical_block = block_table[block_idx]
                    slot = physical_block * self.kv_cache_manager.block_size + block_offset
                    slot_mapping_flat.append(slot)
                else:
                    # Padding token
                    # Map to -1. The kernel should ignore this.
                    slot_mapping_flat.append(-1)

        slot_mapping_tensor = mx.array(slot_mapping_flat, dtype=mx.int64)

        # Pad block tables
        max_blocks = max(len(bt) for bt in block_tables_list)
        padded_block_tables = []
        for bt in block_tables_list:
            padded_block_tables.append(bt + [0] * (max_blocks - len(bt)))

        block_tables_tensor = mx.array(padded_block_tables, dtype=mx.int32)
        context_lengths_tensor = mx.array(context_lengths_list, dtype=mx.int32)

        # Create mask for standard attention (used during Prefill computation)
        causal_mask = create_causal_mask(padded_inputs.shape[1], padded_inputs.shape[1], self.dtype)
        mask = combine_padding_and_causal_masks(padding_mask, causal_mask, self.dtype)

        ret = {
            "h_or_tokens": padded_inputs,
            "cache": self.kv_cache_manager.get_cache(),
            "mask": mask,
            "requests": batched_requests,
            "block_tables": block_tables_tensor,
            "context_lengths": context_lengths_tensor,
            "slot_mapping": slot_mapping_tensor,
            "state_cache": None,
        }
        logger.debug(f"Prepared MLX prefill batch (size={batch_size})")
        return ret

    def _prepare_decode_batch(self, batched_requests: List[Request]) -> Optional[Dict[str, Any]]:
        """Prepares inputs for ShardedModel from a batch of decode requests."""
        batch_size = len(batched_requests)
        if batch_size == 0:
            return None

        h_or_tokens_list = []
        block_tables_list = []
        context_lengths_list = []

        for req in batched_requests:
            assert req.is_decoding, f"Request {req.request_id} is not a decode request."

            if self.is_first_peer:
                # First peer input is the last generated token
                h_or_tokens_list.append([req.output_ids[-1]])
            else:
                h_or_tokens_list.append(req.hidden_states)

            # TODO: Prefix cache update

            # Allocate slot for new token
            success = self.kv_cache_manager.append_slot(req.request_id)
            if not success:
                raise RuntimeError(f"OOM during decode for {req.request_id}")

            block_table = self.kv_cache_manager.get_block_table(req.request_id)
            block_tables_list.append(block_table)
            context_lengths_list.append(self.kv_cache_manager.get_context_length(req.request_id))

        if isinstance(h_or_tokens_list[0], list):
            # First peer case: h_or_tokens_list is list of list of ints [[token_id], ...]
            padded_inputs = mx.array(h_or_tokens_list, dtype=mx.int32)  # (Batch, 1)
        else:
            # Intermediate peer case: h_or_tokens_ list is list of mx.arrays (1, D)
            padded_inputs = mx.concatenate(h_or_tokens_list, axis=0)  # (Batch, D)
            padded_inputs = padded_inputs.reshape(batch_size, 1, -1)  # (Batch, 1, D)

        # Pad block tables
        max_blocks = max(len(bt) for bt in block_tables_list)
        padded_block_tables = []
        for bt in block_tables_list:
            padded_block_tables.append(bt + [0] * (max_blocks - len(bt)))

        block_tables_tensor = mx.array(padded_block_tables, dtype=mx.int32)
        context_lengths_tensor = mx.array(context_lengths_list, dtype=mx.int32)

        ret = {
            "h_or_tokens": padded_inputs,
            "cache": self.kv_cache_manager.get_cache(),
            "mask": None,
            "requests": batched_requests,
            "block_tables": block_tables_tensor,
            "context_lengths": context_lengths_tensor,
            "slot_mapping": None,
            "state_cache": None,
        }
        logger.debug(f"Prepared MLX decode batch (size={batch_size})")
        return ret


================================================================================
File: src/parallax/server/executor/sglang_executor.py
Size: 22.35 kB
================================================================================

"""
SGLang backend implementation of high level executor
"""

from typing import Any, Dict, List, Optional, Tuple

import torch
from sglang.srt.lora.lora_registry import LoRARef
from sglang.srt.managers.schedule_batch import ScheduleBatch
from sglang.srt.model_executor.forward_batch_info import PPProxyTensors
from sglang.srt.utils import broadcast_pyobj
from sglang.srt.utils.common import SUPPORTED_LORA_TARGET_MODULES

from parallax.server.executor.base_executor import BaseExecutor
from parallax.server.request import (
    InitialRequest,
    IntermediateRequest,
    Request,
    RequestStatus,
)
from parallax.sglang.batch_info import (
    form_sgl_batch_decode,
    form_sgl_batch_prefill,
    release_sglang_request,
)
from parallax.sglang.model_runner import initialize_sgl_model_runner
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


class SGLExecutor(BaseExecutor):
    def __init__(
        self,
        # Model Configs
        model_repo: str,
        start_layer: int,
        end_layer: int,
        dtype: str = "float16",
        # Device override
        device: Optional[str] = None,
        use_hfcache: bool = False,
        # Scheduler Configs
        max_batch_size: Optional[int] = 8,
        max_sequence_length: Optional[int] = None,
        max_tokens_in_kv_pool: Optional[int] = None,
        # Controlling perfill / decode ratio
        max_num_tokens_per_batch: int = 1024,
        prefill_priority: int = 0,
        micro_batch_ratio: int = 2,
        scheduler_wait_ms: int = 500,
        request_timeout_s: Optional[int] = 600,
        # Metrics Configs
        layer_latency_update_every: int = 4096,
        # KV Cache Configs
        kv_block_size: int = 64,
        kv_cache_memory_fraction: float = 0.8,
        enable_prefix_cache: Optional[bool] = False,
        # Communication Configs
        # P2P Communication Configs
        send_to_peer_addr: Optional[str] = None,
        recv_from_peer_addr: Optional[str] = None,
        # IPC Communication Configs
        executor_input_ipc_addr: Optional[str] = None,
        executor_output_ipc_addr: Optional[str] = None,
        # GPU Specialized Configs
        attention_backend: Optional[str] = "flashinfer",
        moe_runner_backend: Optional[str] = "auto",
        enable_lora: Optional[bool] = False,
        max_lora_rank: Optional[int] = None,
        lora_target_modules: Optional[List[str]] = None,
        lora_paths: Optional[List[str]] = None,
        max_loras_per_batch: Optional[int] = None,
        max_loaded_loras: Optional[int] = None,
        lora_eviction_policy: Optional[str] = "lru",
        lora_backend: Optional[str] = "triton",
        max_lora_chunk_size: Optional[int] = 128,
        # Tensor Parallel Configs
        tp_rank: Optional[int] = 0,
        tp_size: Optional[int] = 1,
        nccl_port: Optional[int] = 4000,
        # Optional shared state for layer reallocation detection (when running in subprocess)
        shared_state: Optional[dict] = None,
    ):

        self.enable_lora = True if lora_paths is not None else enable_lora
        self.lora_paths = lora_paths
        self.max_lora_rank = max_lora_rank
        self.lora_target_modules = lora_target_modules
        self.max_loras_per_batch = 1 if max_loras_per_batch is None else max_loras_per_batch
        self.max_loaded_loras = max_loaded_loras
        self.lora_eviction_policy = lora_eviction_policy
        self.lora_backend = lora_backend
        self.max_lora_chunk_size = max_lora_chunk_size

        if self.lora_paths is not None and len(self.lora_paths) > 0:
            self.check_lora_server_args()

        # output lora paths
        if self.lora_paths is not None:
            logger.info(f"LoRA paths provided: {[str(lora_path) for lora_path in self.lora_paths]}")

        model_runner_params = {
            "model_repo": model_repo,
            "start_layer": start_layer,
            "end_layer": end_layer,
            "kv_cache_memory_fraction": kv_cache_memory_fraction,
            "attention_backend": attention_backend,
            "kv_block_size": kv_block_size,
            "max_num_tokens_per_batch": max_num_tokens_per_batch,
            "dtype": dtype,
            "moe_runner_backend": moe_runner_backend,
            "tp_rank": tp_rank,
            "tp_size": tp_size,
            "nccl_port": nccl_port,
            "using_hfcache": use_hfcache,
            "enable_lora": self.enable_lora,
            "max_lora_rank": self.max_lora_rank,
            "lora_target_modules": self.lora_target_modules,
            "lora_paths": self.lora_paths,
            "max_loras_per_batch": self.max_loras_per_batch,
            "max_loaded_loras": self.max_loaded_loras,
            "lora_eviction_policy": self.lora_eviction_policy,
            "lora_backend": self.lora_backend,
            "max_lora_chunk_size": self.max_lora_chunk_size,
        }
        logger.debug(
            f"Initializing SGLang model runner for repo={model_repo}, layers=[{start_layer}, {end_layer})"
        )
        self.model_runner, self.config, self.tokenizer = initialize_sgl_model_runner(
            **model_runner_params
        )
        logger.debug(
            f"SGLang model runner initialized. num_layers={self.config.get('num_hidden_layers')}"
        )
        super().__init__(
            start_layer=start_layer,
            end_layer=end_layer,
            dtype=dtype,
            device=device,
            max_batch_size=max_batch_size,
            max_sequence_length=max_sequence_length,
            max_num_tokens_per_batch=max_num_tokens_per_batch,
            prefill_priority=prefill_priority,
            micro_batch_ratio=micro_batch_ratio,
            scheduler_wait_ms=scheduler_wait_ms,
            request_timeout_s=request_timeout_s,
            layer_latency_update_every=layer_latency_update_every,
            send_to_peer_addr=send_to_peer_addr,
            recv_from_peer_addr=recv_from_peer_addr,
            executor_input_ipc_addr=executor_input_ipc_addr,
            executor_output_ipc_addr=executor_output_ipc_addr,
            tp_rank=tp_rank,
            tp_size=tp_size,
            shared_state=shared_state,
        )
        self.cur_batch = None
        self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)
        self.tp_group = self.model_runner.tp_group
        self.tp_cpu_group = self.tp_group.cpu_group

    def check_lora_server_args(self):
        assert self.max_loras_per_batch > 0, "max_loras_per_batch must be positive"

        # Enable LoRA if any LoRA paths are provided for backward compatibility.
        if self.lora_paths:
            if self.enable_lora is None:
                self.enable_lora = True
                logger.warning("--enable-lora is set to True because --lora-paths is provided.")
            elif self.enable_lora is False:
                logger.warning(
                    "--enable-lora is set to False, any provided lora_paths will be ignored."
                )

        if self.enable_lora:
            # Parse lora_paths
            if isinstance(self.lora_paths, list):
                lora_paths = self.lora_paths
                self.lora_paths = []
                for lora_path in lora_paths:
                    if isinstance(lora_path, str):
                        if "=" in lora_path:
                            name, path = lora_path.split("=", 1)
                            lora_ref = LoRARef(lora_name=name, lora_path=path, pinned=False)
                        else:
                            lora_ref = LoRARef(
                                lora_name=lora_path, lora_path=lora_path, pinned=False
                            )
                    elif isinstance(lora_path, dict):
                        assert (
                            "lora_name" in lora_path and "lora_path" in lora_path
                        ), f"When providing LoRA paths as a list of dict, each dict should contain 'lora_name' and 'lora_path' keys. Got: {lora_path}"
                        lora_ref = LoRARef(
                            lora_name=lora_path["lora_name"],
                            lora_path=lora_path["lora_path"],
                            pinned=lora_path.get("pinned", False),
                        )
                    else:
                        raise ValueError(
                            f"Invalid type for item in --lora-paths list: {type(lora_path)}. "
                            "Expected a string or a dictionary."
                        )
                    self.lora_paths.append(lora_ref)
            elif isinstance(self.lora_paths, dict):
                self.lora_paths = [
                    LoRARef(lora_name=k, lora_path=v, pinned=False)
                    for k, v in self.lora_paths.items()
                ]
            elif self.lora_paths is None:
                self.lora_paths = []
            else:
                raise ValueError(
                    f"Invalid type for --lora-paths: {type(self.lora_paths)}. "
                    "Expected a list or a dictionary."
                )

            # Expand target modules
            if self.lora_target_modules:
                self.lora_target_modules = set(self.lora_target_modules)
                if "all" in self.lora_target_modules:
                    assert (
                        len(self.lora_target_modules) == 1
                    ), "If 'all' is specified in --lora-target-modules, it should be the only module specified."
                    self.lora_target_modules = set(SUPPORTED_LORA_TARGET_MODULES)

            # Ensure sufficient information is provided for LoRA initialization.
            assert self.lora_paths or (
                self.max_lora_rank and self.lora_target_modules
            ), "When no initial --lora-paths is provided, you need to specify both --max-lora-rank and --lora-target-modules for LoRA initialization."

            # Validate max_loaded_loras
            if self.max_loaded_loras is not None:
                assert self.max_loaded_loras >= self.max_loras_per_batch, (
                    "max_loaded_loras should be greater than or equal to max_loras_per_batch. "
                    f"max_loaded_loras={self.max_loaded_loras}, max_loras_per_batch={self.max_loras_per_batch}"
                )
                assert len(self.lora_paths) <= self.max_loaded_loras, (
                    "The number of LoRA paths should not exceed max_loaded_loras. "
                    f"max_loaded_loras={self.max_loaded_loras}, lora_paths={len(self.lora_paths)}"
                )

            if self.max_lora_chunk_size is not None:
                assert (
                    16 <= self.max_lora_chunk_size <= 128
                    and (self.max_lora_chunk_size & (self.max_lora_chunk_size - 1)) == 0
                ), "--max-lora-chunk-size must be a power of 2 between 16 and 128."

    def handle_input_requests(self, requests: List[Request]):
        """Update requests states and status in scheduler and cache manager."""
        if self.tp_rank == 0 and not requests:
            return
        if self.tp_size > 1:
            requests = self._tensor_parallel_broadcast_byobj(requests)
        if len(requests) > 0:
            logger.debug(f"Handling {len(requests)} requests.")

        if self.is_first_peer:
            # First peer can receive InitialRequests from the client RPC,
            # or IntermediateRequests from the last peer.
            for req in requests:
                if isinstance(req, InitialRequest):
                    self.scheduler.enque_request(req)
                elif isinstance(req, IntermediateRequest):
                    original_req = self.scheduler.get_running_request(req.request_id)
                    if original_req is None:
                        logger.warning(
                            f"Received IntermediateRequest {req.request_id}. "
                            "But no corresponding request found in scheduler (CUDA). "
                            "It might have been cancelled or finished."
                        )
                        continue

                    assert req.next_token_id is not None
                    original_req.commit_new_token(req.next_token_id)
                    logger.debug(
                        f"[FirstPeer-CUDA] Committed token {req.next_token_id} for {req.request_id}, "
                        f"output_ids now has {len(original_req.output_ids)} tokens"
                    )
                    if len(req.routing_table) > 0:
                        original_req.routing_table = req.routing_table

                    # Check for termination.
                    if self.scheduler.check_and_update_request_status(original_req):
                        logger.debug(f"Releasing resources for finished request {req.request_id}")
                        self.release_and_evict_request(req.request_id)
                        if not self.is_last_peer:
                            self.finished_batch.append(req)
                    else:
                        self.scheduler.enque_request(original_req)

                    # detokenize and send to http server
                    if self.tp_rank == 0:
                        req_dict = {
                            "prompt_tokens": len(req.input_ids),
                            "next_token_id": req.next_token_id,
                            "rid": req.request_id,
                        }
                        if original_req.status == RequestStatus.FINISHED_EOS:
                            req_dict["eos"] = True
                        if original_req.status == RequestStatus.FINISHED_MAX_LENGTH:
                            req_dict["length"] = True
                        if hasattr(self, "send_to_ipc_socket"):
                            self.send_to_ipc_socket.send_pyobj(req_dict)
                else:
                    raise TypeError(f"First peer received unexpected request type: {type(req)}")
        else:
            # Intermediate and Last peers receive IntermediateRequests from the previous peer.
            for req in requests:
                assert isinstance(
                    req, IntermediateRequest
                ), "Non-first peers must receive IntermediateRequests."
                if req.is_finished or req.hidden_states is None:
                    self.release_and_evict_request(req.request_id)
                    if not self.is_last_peer:
                        self.finished_batch.append(req)
                else:
                    # This is an active request, add it to the scheduler queue to be processed.
                    self.scheduler.enque_request(req)

    def process_batch(self, prepared_inputs: Dict[str, Any], return_decoded_tokens: bool = True):
        """Process a batch of requests in SGLang."""
        assert "forward_batch" in prepared_inputs, "forward_batch should be in cuda prepared inputs"
        assert (
            "pp_proxy_tensors" in prepared_inputs
        ), "pp_proxy_tensors should be in cuda prepared inputs"

        forward_batch = prepared_inputs["forward_batch"]
        pp_proxy_tensors = prepared_inputs["pp_proxy_tensors"]

        # Execute model with SGLang
        logits_output, _ = self.model_runner.forward(
            forward_batch=forward_batch,
            pp_proxy_tensors=pp_proxy_tensors,
        )

        # Merge prefill batch into running batch
        if self.cur_batch:
            if self.cur_batch.forward_mode.is_extend():
                # Merge the new batch into the running batch
                if not self.cur_batch.is_empty():
                    if self.running_batch.is_empty():
                        self.running_batch = self.cur_batch
                    else:
                        # Merge running_batch with prefill batch
                        self.running_batch.merge_batch(self.cur_batch)
            self.cur_batch = None

        # Return appropriate output based on peer position
        if return_decoded_tokens:
            # Last peer: sample and return token IDs
            next_token_ids = self.model_runner.sample(logits_output, forward_batch)
            return next_token_ids
        else:
            # Intermediate peer: return hidden states for next peer
            # Note: SGLang stores hidden_states + residual separately
            final_hidden_states = (
                logits_output.tensors["hidden_states"] + logits_output.tensors["residual"]
            )
            return final_hidden_states

    def _release_request(self, rid: str):
        """Release per-request resources in SGLang."""
        try:
            release_sglang_request(self.running_batch, rid)
        except Exception:
            pass

    def _gen_token_id_from_hidden(self, hidden_states) -> Tuple[int, Any]:
        """
        Inplace modifies hidden_states.
        Returns token_id, hidden_states
        """
        assert hidden_states.dtype in (
            torch.int64,
            torch.int32,
        ), "Single node must generate an output_id."
        next_token_id = int(hidden_states[0])
        return next_token_id, hidden_states

    def _tensor_parallel_broadcast_byobj(self, broadcast_obj):
        """Wrapper for broadcast pyobject in TP group"""

        broadcast_result = broadcast_pyobj(
            broadcast_obj,
            self.tp_group.rank,
            self.tp_cpu_group,
            src=self.tp_group.ranks[0],
        )
        return broadcast_result

    def _prepare_prefill_batch(self, batched_requests: List[Request]) -> Dict[str, Any]:
        """
        Prepares inputs for CUDA backends from a batch of prefill requests.
        """

        batch_size = len(batched_requests)
        if batch_size == 0:
            return None

        # Prepare PP proxy tensors
        pp_proxy_tensors = None
        if not self.is_first_peer:
            hidden_states = torch.cat(
                [
                    (
                        req.hidden_states
                        if req.hidden_states.ndim == 2
                        else req.hidden_states.unsqueeze(0)
                    )
                    for req in batched_requests
                ],
                dim=0,
            )

            # Create residual tensor with same shape
            residual = torch.zeros(
                hidden_states.shape, dtype=hidden_states.dtype, device=hidden_states.device
            )

            pp_proxy_tensors = PPProxyTensors(
                {
                    "hidden_states": hidden_states,
                    "residual": residual,
                }
            )
            logger.debug(f"PP Proxy: hidden_states shape: {hidden_states.shape}")

        # Prepare lengths (common for both backends)
        lengths = []
        for req in batched_requests:
            if req.lora_path is not None and self.lora_paths is not None:
                for lora_ref in self.lora_paths:
                    if lora_ref.lora_path == req.lora_path:
                        req.lora_id = lora_ref.lora_id
                        break
            else:
                req.lora_id = (
                    self.lora_paths[0].lora_id
                    if self.lora_paths and len(self.lora_paths) > 0
                    else None
                )
            lengths.append(req.total_length)
        lengths_tensor = torch.tensor(lengths, device=self.device)

        schedule_batch, forward_batch = form_sgl_batch_prefill(
            batched_requests,
            self.model_runner,
        )
        self.cur_batch = schedule_batch

        ret = {
            "forward_batch": forward_batch,
            "pp_proxy_tensors": pp_proxy_tensors,
            "context_lengths": lengths_tensor,
            "requests": batched_requests,
        }
        logger.debug(f"Prepared CUDA prefill batch (sglang, size={batch_size})")
        return ret

    def _prepare_decode_batch(self, batched_requests: List[Request]) -> Optional[Dict[str, Any]]:
        """
        Prepares inputs for CUDA backends from a batch of decode requests.
        """

        batch_size = len(batched_requests)
        if batch_size == 0:
            return None

        # Prepare PP proxy tensors
        pp_proxy_tensors = None
        if not self.is_first_peer:
            hidden_states = torch.cat(
                [
                    (
                        req.hidden_states
                        if req.hidden_states.ndim == 2
                        else req.hidden_states.unsqueeze(0)
                    )
                    for req in batched_requests
                ],
                dim=0,
            )

            # Create residual tensor with same shape
            residual = torch.zeros(
                hidden_states.shape, dtype=hidden_states.dtype, device=hidden_states.device
            )

            pp_proxy_tensors = PPProxyTensors(
                {
                    "hidden_states": hidden_states,
                    "residual": residual,
                }
            )
            logger.debug(f"PP Proxy: hidden_states shape: {hidden_states.shape}")

        # Prepare lengths (common for both backends)
        lengths = []
        for req in batched_requests:
            if req.lora_path is not None and self.lora_paths is not None:
                for lora_ref in self.lora_paths:
                    if lora_ref.lora_path == req.lora_path:
                        req.lora_id = lora_ref.lora_id
                        break
            else:
                req.lora_id = (
                    self.lora_paths[0].lora_id
                    if self.lora_paths and len(self.lora_paths) > 0
                    else None
                )
            lengths.append(req.total_length)
        lengths_tensor = torch.tensor(lengths, device=self.device)

        forward_batch = form_sgl_batch_decode(
            batched_requests,
            self.model_runner,
            self.running_batch,
            self.is_first_peer,
        )

        ret = {
            "forward_batch": forward_batch,
            "pp_proxy_tensors": pp_proxy_tensors,
            "context_lengths": lengths_tensor,
            "requests": batched_requests,
        }
        logger.debug(f"Prepared CUDA decode batch (sglang, size={batch_size})")
        return ret


================================================================================
File: src/parallax/server/executor/vllm_executor.py
Size: 16.47 kB
================================================================================

"""
vLLM backend implementation of high level executor
"""

from typing import Any, Dict, List, Optional, Tuple

import torch
from vllm.sequence import IntermediateTensors

from parallax.server.executor.base_executor import BaseExecutor
from parallax.server.request import (
    InitialRequest,
    IntermediateRequest,
    Request,
    RequestStatus,
)
from parallax.vllm.batch_info import (
    compute_expected_intermediate_tokens,
    form_vllm_batch_decode,
    form_vllm_batch_prefill,
    release_vllm_request,
    resize_intermediate_tensors,
)
from parallax.vllm.model_runner import initialize_vllm_model_runner
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


class VLLMExecutor(BaseExecutor):
    def __init__(
        self,
        # Model Configs
        model_repo: str,
        start_layer: int,
        end_layer: int,
        dtype: str = "float16",
        # Device override
        device: Optional[str] = None,
        use_hfcache: bool = False,
        # Scheduler Configs
        max_batch_size: Optional[int] = 8,
        max_sequence_length: Optional[int] = None,
        max_tokens_in_kv_pool: Optional[int] = None,
        # Controlling perfill / decode ratio
        max_num_tokens_per_batch: int = 1024,
        prefill_priority: int = 0,
        micro_batch_ratio: int = 2,
        scheduler_wait_ms: int = 500,
        request_timeout_s: Optional[int] = 600,
        # Metrics Configs
        layer_latency_update_every: int = 4096,
        # KV Cache Configs
        kv_block_size: int = 64,
        kv_cache_memory_fraction: float = 0.8,
        enable_prefix_cache: Optional[bool] = False,
        # Communication Configs
        # P2P Communication Configs
        send_to_peer_addr: Optional[str] = None,
        recv_from_peer_addr: Optional[str] = None,
        # IPC Communication Configs
        executor_input_ipc_addr: Optional[str] = None,
        executor_output_ipc_addr: Optional[str] = None,
        # GPU Specialized Configs
        attention_backend: Optional[str] = "flashinfer",
        moe_runner_backend: Optional[str] = "auto",
        enable_lora: Optional[bool] = False,
        max_lora_rank: Optional[int] = None,
        lora_target_modules: Optional[List[str]] = None,
        lora_paths: Optional[List[str]] = None,
        max_loras_per_batch: Optional[int] = None,
        max_loaded_loras: Optional[int] = None,
        lora_eviction_policy: Optional[str] = "lru",
        lora_backend: Optional[str] = "triton",
        max_lora_chunk_size: Optional[int] = 128,
        # Tensor Parallel Configs
        tp_rank: Optional[int] = 0,
        tp_size: Optional[int] = 1,
        nccl_port: Optional[int] = 4000,
        # Optional shared state for layer reallocation detection (when running in subprocess)
        shared_state: Optional[dict] = None,
    ):
        model_runner_params = {
            "model_repo": model_repo,
            "start_layer": start_layer,
            "end_layer": end_layer,
            "kv_cache_memory_fraction": kv_cache_memory_fraction,
            "attention_backend": attention_backend,
            "kv_block_size": kv_block_size,
            "max_num_tokens_per_batch": max_num_tokens_per_batch,
            "dtype": dtype,
            "moe_runner_backend": moe_runner_backend,
            "tp_rank": tp_rank,
            "tp_size": tp_size,
            "nccl_port": nccl_port,
            "using_hfcache": use_hfcache,
            "enable_lora": enable_lora,
            "max_lora_rank": max_lora_rank,
            "lora_target_modules": lora_target_modules,
            "lora_paths": lora_paths,
            "max_loras_per_batch": max_loras_per_batch,
            "max_loaded_loras": max_loaded_loras,
            "lora_eviction_policy": lora_eviction_policy,
            "lora_backend": lora_backend,
            "max_lora_chunk_size": max_lora_chunk_size,
        }
        logger.debug(
            f"Initializing vLLM model runner for repo={model_repo}, layers=[{start_layer}, {end_layer})"
        )
        self.model_runner, self.config, self.tokenizer = initialize_vllm_model_runner(
            **model_runner_params
        )
        super().__init__(
            start_layer=start_layer,
            end_layer=end_layer,
            dtype=dtype,
            device=device,
            max_batch_size=max_batch_size,
            max_sequence_length=max_sequence_length,
            max_num_tokens_per_batch=max_num_tokens_per_batch,
            prefill_priority=prefill_priority,
            micro_batch_ratio=micro_batch_ratio,
            scheduler_wait_ms=scheduler_wait_ms,
            request_timeout_s=request_timeout_s,
            layer_latency_update_every=layer_latency_update_every,
            send_to_peer_addr=send_to_peer_addr,
            recv_from_peer_addr=recv_from_peer_addr,
            executor_input_ipc_addr=executor_input_ipc_addr,
            executor_output_ipc_addr=executor_output_ipc_addr,
            tp_rank=tp_rank,
            tp_size=tp_size,
            shared_state=shared_state,
        )

    def handle_input_requests(self, requests: List[Request]):
        """Update requests states and status in scheduler and cache manager."""
        if not requests:
            return
        if self.is_first_peer:
            # First peer can receive InitialRequests from the client RPC,
            # or IntermediateRequests from the last peer.
            for req in requests:
                if isinstance(req, InitialRequest):
                    self.scheduler.enque_request(req)
                elif isinstance(req, IntermediateRequest):
                    original_req = self.scheduler.get_running_request(req.request_id)
                    if original_req is None:
                        logger.warning(
                            f"Received IntermediateRequest {req.request_id}. "
                            "But no corresponding request found in scheduler (CUDA). "
                            "It might have been cancelled or finished."
                        )
                        continue

                    assert req.next_token_id is not None
                    original_req.commit_new_token(req.next_token_id)
                    logger.debug(
                        f"[FirstPeer-CUDA] Committed token {req.next_token_id} for {req.request_id}, "
                        f"output_ids now has {len(original_req.output_ids)} tokens"
                    )
                    if len(req.routing_table) > 0:
                        original_req.routing_table = req.routing_table

                    # Check for termination.
                    if self.scheduler.check_and_update_request_status(original_req):
                        logger.debug(f"Releasing resources for finished request {req.request_id}")
                        self.release_and_evict_request(req.request_id)
                        if not self.is_last_peer:
                            self.finished_batch.append(req)
                    else:
                        self.scheduler.enque_request(original_req)

                    # detokenize and send to http server
                    if self.tp_rank == 0:
                        req_dict = {
                            "prompt_tokens": len(req.input_ids),
                            "next_token_id": req.next_token_id,
                            "rid": req.request_id,
                        }
                        if original_req.status == RequestStatus.FINISHED_EOS:
                            req_dict["eos"] = True
                        if original_req.status == RequestStatus.FINISHED_MAX_LENGTH:
                            req_dict["length"] = True
                        if hasattr(self, "send_to_ipc_socket"):
                            self.send_to_ipc_socket.send_pyobj(req_dict)
                else:
                    raise TypeError(f"First peer received unexpected request type: {type(req)}")
        else:
            # Intermediate and Last peers receive IntermediateRequests from the previous peer.
            for req in requests:
                assert isinstance(
                    req, IntermediateRequest
                ), "Non-first peers must receive IntermediateRequests."
                if req.is_finished or req.hidden_states is None:
                    self.release_and_evict_request(req.request_id)
                    if not self.is_last_peer:
                        self.finished_batch.append(req)
                else:
                    # This is an active request, add it to the scheduler queue to be processed.
                    self.scheduler.enque_request(req)

    def process_batch(self, prepared_inputs: Dict[str, Any], return_decoded_tokens: bool = True):
        """Process a batch of requests in vLLM."""
        assert (
            "scheduler_output" in prepared_inputs
        ), "scheduler_output should be provided for vLLM backend"
        assert (
            "pp_proxy_tensors" in prepared_inputs
        ), "pp_proxy_tensors should be in cuda prepared inputs"
        scheduler_output = prepared_inputs["scheduler_output"]
        pp_proxy_tensors = prepared_inputs["pp_proxy_tensors"]
        # For vLLM, pp_proxy_tensors is already an IntermediateTensors object
        intermediate_tensors = pp_proxy_tensors if pp_proxy_tensors is not None else None
        if intermediate_tensors is not None:
            logger.debug(f"vLLM: Using intermediate_tensors for PP (non-first peer)")

        # Import IntermediateTensors for type checking

        # Execute model with vLLM
        output = self.model_runner.execute_model(
            scheduler_output=scheduler_output,
            intermediate_tensors=intermediate_tensors,
        )

        # Return appropriate output based on peer position
        if return_decoded_tokens:
            sampled_token_ids = output.sampled_token_ids
            if isinstance(sampled_token_ids, list) and len(sampled_token_ids) > 0:
                # Convert to tensor: pad sequences to same length
                max_len = max(len(seq) for seq in sampled_token_ids)
                padded_tokens = []
                for seq in sampled_token_ids:
                    padded_seq = seq + [-1] * (max_len - len(seq))  # Pad with -1
                    padded_tokens.append(padded_seq)
                return torch.tensor(padded_tokens, dtype=torch.int64)
            else:
                return torch.tensor(sampled_token_ids, dtype=torch.int64)
        else:
            # Intermediate peer: return hidden states for next peer
            final_hidden_states = output.tensors["hidden_states"] + output.tensors["residual"]
            return final_hidden_states

    def _release_request(self, rid: str):
        """Release per-request resources in vLLM."""
        try:
            release_vllm_request(self.model_runner, rid)
        except Exception:
            pass

    def _gen_token_id_from_hidden(self, hidden_states) -> Tuple[int, Any]:
        """
        Inplace modifies hidden_states.
        Returns token_id, hidden_states
        """
        assert hidden_states.dtype in (
            torch.int64,
            torch.int32,
        ), "Single node must generate an output_id."
        next_token_id = int(hidden_states[0])
        return next_token_id, hidden_states

    def _prepare_prefill_batch(self, batched_requests: List[Request]) -> Dict[str, Any]:
        """
        Prepares inputs for CUDA backends from a batch of prefill requests.
        """
        batch_size = len(batched_requests)
        if batch_size == 0:
            return None

        # Prepare PP proxy tensors (common for both backends when not first peer)
        pp_proxy_tensors = None
        if not self.is_first_peer:
            # Concatenate hidden states from all requests
            # For vLLM, we need to flatten to (total_tokens, hidden_size)
            hidden_states_list = []
            for req in batched_requests:
                hs = req.hidden_states
                if hs.ndim == 2:
                    # Already (seq_len, hidden_size) or (1, hidden_size)
                    hidden_states_list.append(hs)
                elif hs.ndim == 3:
                    # (1, seq_len, hidden_size) -> (seq_len, hidden_size)
                    hidden_states_list.append(hs.squeeze(0))
                else:
                    # (hidden_size,) -> (1, hidden_size)
                    hidden_states_list.append(hs.unsqueeze(0))

            # Concatenate along sequence dimension to get (total_tokens, hidden_size)
            hidden_states = torch.cat(hidden_states_list, dim=0)

            # Create residual tensor with same shape
            residual = torch.zeros(
                hidden_states.shape, dtype=hidden_states.dtype, device=hidden_states.device
            )

            pp_proxy_tensors = IntermediateTensors(
                {
                    "hidden_states": hidden_states,
                    "residual": residual,
                }
            )
            logger.debug(f"PP Proxy: hidden_states shape: {hidden_states.shape}")

        # Prepare lengths (common for both backends)
        lengths = []
        for req in batched_requests:
            lengths.append(req.total_length)
        lengths_tensor = torch.tensor(lengths, device=self.device)

        schedule_outputs_prefill = form_vllm_batch_prefill(batched_requests, self.model_runner)

        if not self.is_first_peer and pp_proxy_tensors is not None:
            target_tokens = compute_expected_intermediate_tokens(
                schedule_outputs_prefill, self.model_runner
            )
            pp_proxy_tensors = resize_intermediate_tensors(pp_proxy_tensors, target_tokens)

        ret = {
            "scheduler_output": schedule_outputs_prefill,
            "pp_proxy_tensors": pp_proxy_tensors,
            "context_lengths": lengths_tensor,
            "requests": batched_requests,
        }
        logger.debug(f"Prepared CUDA prefill batch (vllm, size={batch_size})")
        return ret

    def _prepare_decode_batch(self, batched_requests: List[Request]) -> Dict[str, Any]:
        """
        Prepares inputs for CUDA backends from a batch of decode requests.
        """

        batch_size = len(batched_requests)
        if batch_size == 0:
            return None

        # Prepare PP proxy tensors (common for both backends when not first peer)
        pp_proxy_tensors = None
        if not self.is_first_peer:
            # Concatenate hidden states from all requests
            # For vLLM, we need to flatten to (total_tokens, hidden_size)
            hidden_states_list = []
            for req in batched_requests:
                hs = req.hidden_states
                if hs.ndim == 2:
                    # Already (seq_len, hidden_size) or (1, hidden_size)
                    hidden_states_list.append(hs)
                elif hs.ndim == 3:
                    # (1, seq_len, hidden_size) -> (seq_len, hidden_size)
                    hidden_states_list.append(hs.squeeze(0))
                else:
                    # (hidden_size,) -> (1, hidden_size)
                    hidden_states_list.append(hs.unsqueeze(0))

            # Concatenate along sequence dimension to get (total_tokens, hidden_size)
            hidden_states = torch.cat(hidden_states_list, dim=0)

            # Create residual tensor with same shape
            residual = torch.zeros(
                hidden_states.shape, dtype=hidden_states.dtype, device=hidden_states.device
            )

            pp_proxy_tensors = IntermediateTensors(
                {
                    "hidden_states": hidden_states,
                    "residual": residual,
                }
            )
            logger.debug(f"PP Proxy: hidden_states shape: {hidden_states.shape}")

        # Prepare lengths (common for both backends)
        lengths = []
        for req in batched_requests:
            lengths.append(req.total_length)
        lengths_tensor = torch.tensor(lengths, device=self.device)

        scheduler_outputs_decode = form_vllm_batch_decode(batched_requests, self.model_runner)
        ret = {
            "scheduler_output": scheduler_outputs_decode,
            "pp_proxy_tensors": pp_proxy_tensors,
            "context_lengths": lengths_tensor,
            "requests": batched_requests,
        }
        logger.debug(f"Prepared CUDA decode batch (vllm, size={batch_size})")
        return ret


================================================================================
File: src/parallax/server/http_server.py
Size: 21.25 kB
================================================================================

"""
This module contains the http frontend server for Parallax.
Two classes that handles a post request from the frontend service:

  -- ParallaxHttpServer:
    The uvicorn server that communicates with the frontend and posts responses
    to users.
    This module launches a subprocess.

  -- HTTPHandler:
    1.Gets requests from ParallaxHttpServer and maintains status of these requests.
    2.Send raw requests by ipc to parallax executor.
    3.Waits for ipc response from the executor and stores the results.
"""

import asyncio
import json
import multiprocessing as mp
import sys
import time
import traceback
import uuid
from dataclasses import dataclass, field
from http import HTTPStatus
from typing import Dict, Optional

import fastapi
import uvicorn
import zmq
import zmq.asyncio
from fastapi.responses import ORJSONResponse, StreamingResponse
from mlx_lm.tokenizer_utils import StreamingDetokenizer
from mlx_lm.utils import load_config
from pydantic import BaseModel
from starlette.datastructures import State

from parallax.utils.selective_download import download_metadata_only
from parallax.utils.tokenizer_utils import load_detokenizer, load_tokenizer
from parallax.utils.utils import get_zmq_socket
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


def get_exception_traceback():
    """Traceback function to handle asyncio function errors"""
    etype, value, tb = sys.exc_info()
    err_str = "".join(traceback.format_exception(etype, value, tb))
    return err_str


async def print_exception_wrapper(func):
    """
    Sometimes an asyncio function does not print exception.
    We do another wrapper to handle the exception.
    """
    try:
        await func()
    except Exception:
        error_trace = get_exception_traceback()
        logger.error(f"TokenizerManager hit an exception: {error_trace}")
        sys.exit(1)


@dataclass
class HTTPRequestInfo:
    """HTTP Request information"""

    id: str
    text: str = ""
    stream: bool = False
    finish_reason: str = None
    object: str = "chat.completion"
    model: str = "default"
    create_time: float = 0.0
    update_time: float = 0.0
    logprobs: float = None
    matched_stop: int = None
    # usage
    prompt_tokens: int = 0
    completion_tokens: int = 0
    # helper
    is_finish: bool = False
    # Queue for streaming tokens one by one
    token_queue: Optional[asyncio.Queue] = field(default=None, repr=False)
    detokenizer: StreamingDetokenizer = None
    error_message: Optional[str] = None
    error_type: Optional[str] = None
    error_status: HTTPStatus = HTTPStatus.INTERNAL_SERVER_ERROR


class HTTPHandler:
    """
    A global handler that maintains raw requests. It has 2 main functions:
    1.Interprocess communicate with the model executor.
    2.Maintains the request -> prompts dict for ParallaxHTTPServer.
    """

    def __init__(
        self,
        executor_input_ipc_name,
        executor_output_ipc_name,
        model_path_str,
    ):
        self.asyncio_tasks = set()
        # Init inter-process communication
        context = zmq.asyncio.Context(2)
        self.send_to_executor = get_zmq_socket(context, zmq.PUSH, executor_input_ipc_name, True)
        self.recv_from_executor = get_zmq_socket(context, zmq.PULL, executor_output_ipc_name, True)
        self.processing_requests: Dict[str, HTTPRequestInfo] = {}

        # Load tokenizer for separate detokenizers.
        # Important: avoid triggering full weight downloads here.
        # Only download metadata/config/tokenizer files.
        from pathlib import Path

        if Path(model_path_str).exists():
            model_path = Path(model_path_str)
        else:
            model_path = download_metadata_only(model_path_str)
        config = load_config(model_path)
        self.model_path_str = model_path_str
        self.tokenizer = load_tokenizer(model_path, eos_token_ids=config.get("eos_token_id", None))
        self.detokenizer_class, self.tokenmap = load_detokenizer(model_path, self.tokenizer)

    def create_request(self, request: Dict):
        """Creates a new request information"""
        rid = request["rid"]
        stream = request.get("stream", False)
        model = request.get("model", "default")
        chat_object = "chat.completion.chunk" if stream else "chat.completion"
        detokenizer = self.detokenizer_class(self.tokenizer, self.tokenmap)
        create_time = time.time()
        update_time = create_time
        request_info = HTTPRequestInfo(
            id=rid,
            stream=stream,
            model=model,
            object=chat_object,
            create_time=create_time,
            update_time=update_time,
            detokenizer=detokenizer,
        )
        if stream:
            request_info.token_queue = asyncio.Queue()
        self.processing_requests[rid] = request_info

    def release_request(self, rid: str):
        """Releases the request resources"""
        del self.processing_requests[rid]

    def send_request(self, request: Dict):
        """Sends the request to model executor using IPC."""
        self.send_to_executor.send_pyobj(request)

    def abort_request(self, request_id: str):
        """Sends abort request to executor for a specific request ID."""
        logger.info(f"Sending abort request for request ID: {request_id}")
        self.send_to_executor.send_pyobj({"type": "abort", "rid": request_id})

    async def stream_response_wrapper(self, rid):
        """Wraps the generator to handle client disconnects using a finally block."""
        generator = self.generate_stream_response(rid)
        try:
            async for chunk in generator:
                yield chunk
        finally:
            # This block executes when the client disconnects or the stream finishes.
            req_info = self.processing_requests.get(rid)
            # If the request is still in processing and not marked as finished, it means
            # the client disconnected midway.
            if req_info and not req_info.is_finish:
                logger.warning(f"Client disconnected for streaming request {rid}.")
                self.abort_request(rid)
                self.release_request(rid)
            elif req_info:
                self.release_request(rid)

    def _generate_stream_chunk(self, rid, token, is_first=False, is_last=False):
        """Generates a SSE chunk for a single token."""
        request_info = self.processing_requests[rid]

        if is_first:
            role = "assistant"
            content = ""
            if "minimax-m2" in self.model_path_str.lower():
                content = "<think>"
        elif is_last:
            role = None
            content = None
        else:
            role = None
            content = token

        response = {
            "id": rid,
            "object": "chat.completion.chunk",
            "model": request_info.model,
            "created": request_info.create_time,
            "choices": [
                {
                    "index": 0,
                    "logprobs": request_info.logprobs,
                    "finish_reason": request_info.finish_reason if is_last else None,
                    "matched_stop": request_info.matched_stop,
                },
            ],
            "usage": {
                "prompt_tokens": request_info.prompt_tokens,
                "total_tokens": request_info.prompt_tokens + request_info.completion_tokens,
                "completion_tokens": request_info.completion_tokens,
            },
        }
        choice = response["choices"][0]
        choice["delta"] = {"role": role, "content": content}
        response_json = json.dumps(response, separators=(",", ":"))
        return f"data: {response_json}\n\n".encode()

    def _generate_error_stream_chunk(self, rid, error_payload: Dict[str, str]):
        """Generates a SSE chunk representing an error."""
        request_info = self.processing_requests[rid]
        response = {
            "id": rid,
            "object": request_info.object,
            "model": request_info.model,
            "created": request_info.create_time,
            "error": error_payload,
        }
        response_json = json.dumps(response, separators=(",", ":"))
        return f"data: {response_json}\n\n".encode()

    async def generate_stream_response(self, rid):
        """Generates a streaming response by consuming from a token queue."""
        # Send first chunk with role
        yield self._generate_stream_chunk(rid, None, is_first=True)

        request_info = self.processing_requests.get(rid)
        if not request_info or not request_info.stream:
            return

        while True:
            token = await request_info.token_queue.get()
            if token is None:  # End of stream sentinel
                break
            if isinstance(token, dict) and token.get("type") == "error":
                yield self._generate_error_stream_chunk(rid, token.get("payload", {}))
                continue
            yield self._generate_stream_chunk(rid, token)

        # Send final chunk with finish reason
        yield self._generate_stream_chunk(rid, None, is_last=True)
        yield b"data: [DONE]\n\n"

    def generate_non_stream_response(self, rid):
        """Generates a non-streaming response"""
        request_info = self.processing_requests[rid]
        response = {
            "id": rid,
            "object": request_info.object,
            "model": request_info.model,
            "created": request_info.create_time,
            "choices": [
                {
                    "index": 0,
                    "logprobs": request_info.logprobs,
                    "finish_reason": request_info.finish_reason,
                    "matched_stop": request_info.matched_stop,
                },
            ],
            "usage": {
                "prompt_tokens": request_info.prompt_tokens,
                "total_tokens": request_info.prompt_tokens + request_info.completion_tokens,
                "completion_tokens": request_info.completion_tokens,
            },
        }
        choice = response["choices"][0]
        choice["message"] = {
            "role": "assistant",
            "content": request_info.text,
            "reasoning_content": None,
            "tool_calls": None,
        }
        return response

    async def _handle_executor_error(self, rid: str, recv_dict: Dict):
        """Handles error notifications sent from the executor process."""
        request_info = self.processing_requests.get(rid)
        if request_info is None:
            return

        message = recv_dict.get("error", "Unknown error")
        err_type = recv_dict.get("error_type", "InternalServerError")
        status_code = recv_dict.get("status_code", HTTPStatus.BAD_REQUEST.value)
        try:
            status = HTTPStatus(status_code)
        except ValueError:
            status = HTTPStatus.BAD_REQUEST

        request_info.error_message = message
        request_info.error_type = err_type
        request_info.error_status = status
        request_info.finish_reason = "error"
        request_info.is_finish = True

        if request_info.stream and request_info.token_queue is not None:
            payload = {
                "message": message,
                "type": err_type,
                "code": status.value,
            }
            await request_info.token_queue.put({"type": "error", "payload": payload})
            await request_info.token_queue.put(None)

    async def _handle_loop(self):
        """The event loop that handles returned requests"""
        while True:
            recv_dict = await self.recv_from_executor.recv_pyobj()
            rid = recv_dict["rid"]
            if rid not in self.processing_requests:
                continue

            if recv_dict.get("type") == "error":
                await self._handle_executor_error(rid, recv_dict)
                continue

            request_info = self.processing_requests[rid]
            request_info.update_time = time.time()
            request_info.prompt_tokens = recv_dict["prompt_tokens"]
            next_token_id = recv_dict["next_token_id"]
            request_info.completion_tokens += 1
            request_info.detokenizer.add_token(next_token_id)
            output = request_info.detokenizer.last_segment

            is_finished = recv_dict.get("eos", False) or recv_dict.get("length", False)

            # Only process and send non-EOS tokens
            if not is_finished and len(output) > 0:
                # Accumulate full text for non-streaming and potentially for logging
                request_info.text += output

                # For streaming, put the individual token into the queue.
                if request_info.stream:
                    await request_info.token_queue.put(output)

            # If it is the end of the stream, update status and send sentinel
            if is_finished:
                if recv_dict.get("length", False):
                    logger.debug(f"Request {rid} finished with length")
                    request_info.finish_reason = "length"
                elif recv_dict.get("eos", False):
                    logger.debug(f"Request {rid} finished with eos")
                    request_info.finish_reason = "eos"
                    request_info.matched_stop = next_token_id
                else:
                    logger.debug(f"Request {rid} finished with unknown reason")
                    request_info.finish_reason = "unknown"

                request_info.is_finish = True
                if request_info.stream:
                    await request_info.token_queue.put(None)  # Sentinel for stream end

    async def create_handle_loop(self):
        """Create asyncio event loop task function"""
        task_loop = asyncio.create_task(print_exception_wrapper(self._handle_loop))
        await task_loop


class ErrorResponse(BaseModel):
    """An Error data structure."""

    object: str = "error"
    message: str
    type: str
    param: Optional[str] = None
    code: int


def create_error_response(
    message: str,
    err_type: str = "BadRequestError",
    status_code: HTTPStatus = HTTPStatus.BAD_REQUEST,
):
    """Creates a json error response for the frontend."""
    error = ErrorResponse(message=message, type=err_type, code=status_code.value)
    return ORJSONResponse(content=error.model_dump(), status_code=error.code)


# Fast API
app = fastapi.FastAPI(
    openapi_url="/openapi.json",
)


async def init_app_states(
    state: State, executor_input_ipc: str, executor_output_ipc: str, model_path: str
):
    """Init FastAPI app states, including http handler, etc."""
    state.http_handler = HTTPHandler(
        executor_input_ipc,
        executor_output_ipc,
        model_path,
    )


async def v1_chat_completions(raw_request: fastapi.Request):
    """
    Handles the v1/chat/completions requests asynchronously.
    It gets the prompts from HTTPHandler and returns to the frontend.
    """
    try:
        request_json = await raw_request.json()
    except Exception as e:
        return create_error_response("Invalid request body, error: ", str(e))

    # Check if request_json has "rid", otherwise generate new one
    request_id = request_json.get("rid")
    if request_id is None:
        request_id = str(uuid.uuid4())
        request_json["rid"] = request_id

    app.state.http_handler.create_request(request_json)
    app.state.http_handler.send_request(request_json)
    req = app.state.http_handler.processing_requests.get(request_id)
    if req is None:
        return create_error_response("Request not found", "RequestNotFoundError")
    is_stream = req.stream

    if is_stream:
        return StreamingResponse(
            app.state.http_handler.stream_response_wrapper(request_id),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            },
        )
    else:
        try:
            # For non-streaming requests, we poll for completion. This is simpler
            # than event-based synchronization and acceptable for requests that
            # are expected to be relatively short.
            while True:
                # Check if client is still connected
                if await raw_request.is_disconnected():
                    logger.warning(f"Client disconnected for non-streaming request {request_id}")
                    if request_id in app.state.http_handler.processing_requests:
                        app.state.http_handler.abort_request(request_id)
                        app.state.http_handler.release_request(request_id)
                    return create_error_response("Client disconnected", "ClientDisconnectedError")

                await asyncio.sleep(0.01)
                req = app.state.http_handler.processing_requests.get(request_id)
                if req is None:  # Request might have been cleaned up due to error
                    return create_error_response("Request not found", "RequestNotFoundError")
                is_finish = req.is_finish
                if is_finish:
                    break
            if req.error_message:
                response = create_error_response(
                    req.error_message,
                    req.error_type or "InternalServerError",
                    status_code=req.error_status,
                )
                app.state.http_handler.release_request(request_id)
                return response

            response = app.state.http_handler.generate_non_stream_response(request_id)
            app.state.http_handler.release_request(request_id)
            return ORJSONResponse(status_code=200, content=response)
        except Exception as e:
            # Handle any unexpected errors during processing
            logger.error(f"Error processing non-streaming request {request_id}: {e}")
            if request_id in app.state.http_handler.processing_requests:
                logger.info(f"Sending abort request due to error: {request_id}")
                app.state.http_handler.abort_request(request_id)
                app.state.http_handler.release_request(request_id)
            return create_error_response("Internal server error", "InternalServerError")


@app.post("/v1/chat/completions")
async def openai_v1_chat_completions(raw_request: fastapi.Request):
    """OpenAI v1/chat/complete post function"""
    return await v1_chat_completions(raw_request)


class ParallaxHttpServer:
    """
    The frontend http server drived by asyncio.
    Since uvicorn API runs an asyncio task, we need a new uvicorn
    wrapper to run other async tasks.
    """

    def __init__(self, args):
        self.host = args.host
        self.port = args.port
        self.executor_input_ipc_name = args.executor_input_ipc
        self.executor_output_ipc_name = args.executor_output_ipc
        self.model_path = args.model_path

    async def run_uvicorn(self):
        """
        Since uvicorn.run() uses asyncio.run, we need another wrapper
        to create a uvicorn asyncio task to run multiple tasks.
        """
        config = uvicorn.Config(
            app,
            host=self.host,
            port=self.port,
            timeout_keep_alive=5,
            loop="uvloop",
        )
        server = uvicorn.Server(config)
        await server.serve()

    async def run_tasks(self):
        """Gather results of all asyncio tasks"""
        await asyncio.gather(self.run_uvicorn(), app.state.http_handler.create_handle_loop())

    def run(self):
        """
        Launch A FastAPI server that routes requests to the executor.

        Note:
        1. The HTTP server and executor both run in the main process.
        2. Inter-process communication is done through IPC (each process uses a different port) via the ZMQ library.
        """
        asyncio.run(
            init_app_states(
                app.state,
                self.executor_input_ipc_name,
                self.executor_output_ipc_name,
                self.model_path,
            )
        )
        asyncio.run(self.run_tasks())


def launch_http_server(args):
    """
    Launch function of frontend server.
    It creates a sub-process for the http server.
    """
    http_server = ParallaxHttpServer(args)
    process = mp.Process(target=http_server.run)
    process.start()
    return process


def stop_http_server(http_server_process):
    """
    Stop HTTP server process if it exists.
    """
    if http_server_process is not None:
        logger.info("Stopping HTTP server process...")
        try:
            http_server_process.kill()
            http_server_process.join()
        except Exception as e:
            logger.error(f"Failed to terminate HTTP server process: {e}")
        return None
    return http_server_process


def restart_http_server(args, http_server_process):
    """
    Restart HTTP server with new args.
    Stops the old server if it exists and starts a new one.
    """
    http_server_process = stop_http_server(http_server_process)
    logger.info("Restarting HTTP server...")
    return launch_http_server(args)


================================================================================
File: src/parallax/server/kv_cache.py
Size: 14.82 kB
================================================================================

"""
Simplified KV Cache Manager for Parallax Server

This module implements a simplified key-value (KV) cache system to
avoid materializing the entier KV cache pool.
This is a dictionary-based approach where each request has its own growing KV cache.

Core Components:

KVCache:
    - MLX-LM style growing cache that dynamically allocates memory as needed
    - Supports efficient update and fetch operations
    - Automatically handles memory expansion in chunks

KVCacheManager:
    - Uses a dictionary mapping request_id to KVCache instances
    - Supports adding, updating, releasing requests' KV Cache
    - Performs necessary memory checks to avoid exceeding limits
"""

from typing import Dict, List, Optional, Tuple

import mlx.core as mx

from parallax.server.request import Request, RequestStatus
from parallax_utils.logging_config import get_logger
from parallax_utils.utils import compute_max_tokens_in_cache

logger = get_logger(__name__)


class KVCache:
    """Per-Request KV cache for a single request.
    Dynamically grows the cache in chunks of block_size.
    """

    def __init__(
        self,
        num_kv_heads: int,
        head_dim_k: int,
        head_dim_v: int,
        num_layers: int,
        dtype: mx.Dtype,
        block_size: int = 64,
        conv_dim: Optional[int] = None,
        conv_kernel_size: Optional[int] = None,
        linear_k_dim: Optional[int] = None,
        linear_v_dim: Optional[int] = None,
        linear_num_k_heads: Optional[int] = None,
        linear_num_v_heads: Optional[int] = None,
        qk_nope_head_dim: Optional[int] = None,
        qk_rope_head_dim: Optional[int] = None,
        num_initial_tokens: int = 0,
    ):
        """
        Args:
            num_kv_heads: The number of key-value heads.
            head_dim: The dimension of each head.
            num_layers: The number of layers.
            dtype: The data type of the cache.
            block_size: Source length dim growth step size.
            num_initial_tokens: The number of tokens to initialize the cache with.
        """
        self.num_kv_heads = num_kv_heads
        self.dtype = dtype
        self.block_size = block_size
        self.conv_dim = conv_dim
        self.conv_kernel_size = conv_kernel_size
        self.linear_k_dim = linear_k_dim
        self.linear_v_dim = linear_v_dim
        self.linear_num_k_heads = linear_num_k_heads
        self.linear_num_v_heads = linear_num_v_heads
        self.qk_nope_head_dim = qk_nope_head_dim
        self.qk_rope_head_dim = qk_rope_head_dim
        self.head_dim_v = head_dim_v
        self.head_dim_k = head_dim_k

        num_initial_tokens = self.round_up_to_step(num_initial_tokens)
        # (num_layers, num_kv_heads, seq_len, head_dim)

        self.keys = mx.zeros((num_layers, num_kv_heads, num_initial_tokens, self.head_dim_k), dtype)
        self.values = mx.zeros(
            (num_layers, num_kv_heads, num_initial_tokens, self.head_dim_v), dtype
        )
        self.state0 = (
            mx.zeros((num_layers, conv_kernel_size - 1, conv_dim), dtype) if conv_dim else None
        )

        self.state1 = (
            mx.zeros((num_layers, linear_num_v_heads, linear_k_dim, linear_v_dim), dtype)
            if (linear_k_dim and linear_v_dim and linear_num_k_heads and linear_num_v_heads)
            else None
        )
        self.num_tokens = num_initial_tokens
        self.offset = 0

    def round_up_to_step(self, seq_len: int) -> int:
        """
        Rounds up to the nearest multiple of the block_size.
        """
        return (seq_len + self.block_size - 1) // self.block_size * self.block_size

    def needs_grow(self, seq_len: int) -> bool:
        """Checks if the cache needs to grow."""
        return (self.offset + seq_len) > self.num_tokens

    def fetch(self) -> Tuple[mx.array, mx.array]:
        """Fetches the KV cache for the request."""
        return (
            self.keys[..., : self.offset, :],
            self.values[..., : self.offset, :],
            self.state0 if self.state0 is not None else None,
            self.state1 if self.state1 is not None else None,
        )

    def update(
        self,
        keys: mx.array,
        values: mx.array,
        state0: Optional[mx.array],
        state1: Optional[mx.array],
    ) -> int:
        """
        Updates the cache with new key-value pairs.

        Args:
            keys: New keys to add, shape (num_layers, num_kv_heads, target_len, head_dim_k)
            values: New values to add, shape (num_layers, num_kv_heads, target_len, head_dim_v)
        """
        if state0 is not None and self.state0 is not None:
            self.state0 = state0
        if state1 is not None and self.state1 is not None:
            self.state1 = state1

        prev = self.offset
        seq_len = keys.shape[2]
        prev_tokens = self.num_tokens
        # Grow the cache based on the block_size size
        if self.needs_grow(seq_len):
            num_layers, num_kv_heads, _, head_dim_k = keys.shape
            _, _, _, head_dim_v = values.shape
            n_steps = (self.block_size + seq_len - 1) // self.block_size
            k_shape = (num_layers, num_kv_heads, n_steps * self.block_size, head_dim_k)
            v_shape = (num_layers, num_kv_heads, n_steps * self.block_size, head_dim_v)
            new_k = mx.zeros(k_shape, keys.dtype)
            new_v = mx.zeros(v_shape, values.dtype)

            if prev % self.block_size != 0:
                self.keys = self.keys[..., :prev, :]
                self.values = self.values[..., :prev, :]
            self.keys = mx.concatenate([self.keys, new_k], axis=2)
            self.values = mx.concatenate([self.values, new_v], axis=2)
            self.num_tokens = self.keys.shape[2]

        # Update with new keys and values
        self.offset += seq_len
        self.keys[..., prev : self.offset, :] = keys
        self.values[..., prev : self.offset, :] = values
        return self.num_tokens - prev_tokens


class KVCacheManager:
    """Manager for KVCache instances."""

    def __init__(
        self,
        num_kv_heads: int,
        head_dim: int,
        num_layers: int,
        dtype: mx.Dtype,
        block_size: int = 64,
        max_num_tokens: Optional[int] = None,
        cache_memory_fraction: float = 0.5,
        conv_dim: Optional[int] = None,
        conv_kernel_size: Optional[int] = None,
        linear_k_dim: Optional[int] = None,
        linear_v_dim: Optional[int] = None,
        linear_num_k_heads: Optional[int] = None,
        linear_num_v_heads: Optional[int] = None,
        qk_nope_head_dim: Optional[int] = None,
        qk_rope_head_dim: Optional[int] = None,
        v_head_dim: Optional[int] = None,
    ):
        """
        Args:
            num_kv_heads: The number of key-value heads.
            head_dim: The dimension of each head.
            num_layers: The number of layers.
            dtype: The data type of the cache.
            block_size: Source length dim growth step size.
            max_num_tokens: The maximum number of tokens in the cache.
            cache_memory_fraction: The fraction of memory to use for the cache.
        """
        self.num_kv_heads = num_kv_heads
        self.num_layers = num_layers
        self.dtype = dtype
        self.block_size = block_size
        self.conv_dim = conv_dim
        self.conv_kernel_size = conv_kernel_size
        self.linear_k_dim = linear_k_dim
        self.linear_v_dim = linear_v_dim
        self.linear_num_k_heads = linear_num_k_heads
        self.linear_num_v_heads = linear_num_v_heads
        self.qk_nope_head_dim = qk_nope_head_dim
        self.qk_rope_head_dim = qk_rope_head_dim
        self.v_head_dim = v_head_dim
        if qk_nope_head_dim and qk_rope_head_dim:
            self.head_dim_k = qk_nope_head_dim + qk_rope_head_dim
        else:
            self.head_dim_k = head_dim
        self.head_dim_v = v_head_dim if v_head_dim is not None else head_dim

        self.request_caches: Dict[str, KVCache] = {}
        self.tokens_in_cache = 0

        self.max_num_tokens = compute_max_tokens_in_cache(
            device="mlx",
            kv_cache_memory_fraction=cache_memory_fraction,
            num_shard_layers=num_layers,
            num_key_value_heads=num_kv_heads,
            head_dim_k=self.head_dim_k,
            head_dim_v=self.head_dim_v,
            elem_bytes=dtype.size,
        )
        if max_num_tokens is not None:
            self.max_num_tokens = min(self.max_num_tokens, max_num_tokens)

    def round_up_to_step(self, seq_len: int) -> int:
        """
        Rounds up to the nearest multiple of the block_size.
        """
        return (seq_len + self.block_size - 1) // self.block_size * self.block_size

    def has_request(self, request_id: str) -> bool:
        """
        Checks if the request is in the cache.
        """
        return request_id in self.request_caches

    def request_length(self, request_id: str) -> int:
        """
        Returns the length of key/value in the request.
        """
        return self.request_caches[request_id].offset

    def request_num_tokens(self, request_id: str) -> int:
        """
        Returns the number of tokens (including slots not yet filled) in the request.
        """
        assert self.has_request(request_id), "request not in cache"
        return self.request_caches[request_id].num_tokens

    def gather_kv_cache(self, request_id: str) -> Tuple[mx.array, mx.array]:
        """
        Gathers the KV cache for the request.
        """
        assert self.has_request(request_id), "request not in cache"
        return self.request_caches[request_id].fetch()

    def add_request(self, request: Request, num_tokens: int = 128) -> bool:
        """Adds a request to the cache.

        Args:
            request: The request to add.
            num_tokens: The number of tokens in the request.

        Returns:
            True if the request is added.
        """
        assert (
            request.status == RequestStatus.PREFILLING
        ), "add_request can only be called for prefilling requests"

        if request.request_id in self.request_caches:
            logger.warning(f"Request {request.request_id} already in cache")
            return True

        num_tokens = self.round_up_to_step(num_tokens)
        if self.tokens_in_cache + num_tokens > self.max_num_tokens:
            logger.warning(
                f"can't add request {request.request_id} to cache: {self.tokens_in_cache} + "
                f"{num_tokens} > {self.max_num_tokens}"
            )
            return False

        self.request_caches[request.request_id] = KVCache(
            num_kv_heads=self.num_kv_heads,
            head_dim_k=self.head_dim_k,
            head_dim_v=self.head_dim_v,
            num_layers=self.num_layers,
            dtype=self.dtype,
            block_size=self.block_size,
            num_initial_tokens=num_tokens,
            conv_dim=self.conv_dim,
            conv_kernel_size=self.conv_kernel_size,
            linear_k_dim=self.linear_k_dim,
            linear_v_dim=self.linear_v_dim,
            linear_num_k_heads=self.linear_num_k_heads,
            linear_num_v_heads=self.linear_num_v_heads,
        )
        self.tokens_in_cache += self.request_num_tokens(request.request_id)
        return True

    # def add_request_with_prefix_cache():

    def release_request(self, request_id: str) -> bool:
        """
        Releases the request from the cache.
        """
        assert self.has_request(request_id), "request not in cache"
        self.tokens_in_cache -= self.request_num_tokens(request_id)
        del self.request_caches[request_id]
        return True

    def update_requests(
        self,
        requests: List[Request],
        keys: mx.array,
        values: mx.array,
        lengths: List[int],
        states0: Optional[mx.array],
        states1: Optional[mx.array],
    ) -> bool:
        """
        Updates the requests in the cache.

        Args:
            requests: The requests to update.
            keys: The keys to update.
            values: The values to update.
            lengths: The lengths of the requests.

        Returns:
            True if requests are updated.
        """
        batch_size, num_layers, n_kv_heads, _, head_dim_k = keys.shape
        _, _, _, _, head_dim_v = values.shape
        # Validate
        # assert keys.shape == values.shape, "key and value must have the same shape"
        assert num_layers == self.num_layers, "key and value must have the same number of layers"
        assert batch_size == len(requests), "key and value must have the same batch size"
        assert len(lengths) == batch_size, "lengths must have the same batch size as requests"
        assert (
            n_kv_heads == self.num_kv_heads
        ), "key and value must have the same number of key-value heads"
        assert head_dim_k == self.head_dim_k, "key and value must have the same head dimension"
        assert head_dim_v == self.head_dim_v, "key and value must have the same head dimension"
        # TODO: Use vmap for better performance
        for request, key, value, length, state0, state1 in zip(
            requests, keys, values, lengths, states0, states1
        ):
            length = length.item()
            assert self.has_request(request.request_id), "request not in cache"
            # TODO: fix this
            # actual length? double-counted prefill len
            # decode length 1, why rounding up?
            if self.tokens_in_cache + self.round_up_to_step(length) > self.max_num_tokens:
                logger.warning(
                    f"can't add request {request.request_id} to cache: "
                    f"{self.tokens_in_cache} + {length} > {self.max_num_tokens}"
                )
                return False
            self.tokens_in_cache += self.request_caches[request.request_id].update(
                key[..., :length, :], value[..., :length, :], state0, state1
            )
        return True

    def add_matched_prefix_request(
        self, request: Request, key: mx.array, value: mx.array, length: int
    ):
        """If a request matches prefix, add it back to the running kv-cache manager"""
        assert self.has_request(request.request_id), "request not in cache"
        if self.tokens_in_cache + self.round_up_to_step(length) > self.max_num_tokens:
            logger.warning(
                f"can't add request {request.request_id} to cache: "
                f"{self.tokens_in_cache} + {length} > {self.max_num_tokens}"
            )
            return False
        self.tokens_in_cache += self.request_caches[request.request_id].update(
            key[..., :length, :], value[..., :length, :]
        )
        return True


================================================================================
File: src/parallax/server/model.py
Size: 6.03 kB
================================================================================

"""
Defines the ShardedModel class for distributing MLX models across multiple devices.
"""

from typing import Optional, Tuple, Type

import mlx.core as mx
from mlx import nn
from mlx_lm.models.base import BaseModelArgs

from parallax.server.sampling.sampler import Sampler, SamplingBatchInfo
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


class ShardedModel(nn.Module):
    """A general class for MLX sharded model, adapted for Parallax KV cache.

    Assumes self.layers are composed of modules (e.g., TransformerBlocks)
    that internally use ParallaxAttention and their __call__ method returns
    (hidden_state, new_k_for_layer, new_v_for_layer).
    """

    def __init__(
        self,
        config: BaseModelArgs,
        model_id: str,
        start_layer: int,
        end_layer: int,
        block_class: Type[nn.Module],
        *,
        has_norm_in: bool = False,
        dtype: Optional[mx.Dtype] = None,
    ):
        super().__init__()
        self.config = config
        self.model_id = model_id
        self.start_layer = start_layer
        self.end_layer = end_layer
        self.block_class = block_class
        self.has_norm_in = has_norm_in
        self.dtype = dtype if dtype is not None else mx.float16

        self.hidden_size = config.hidden_size
        self.vocab_size = config.vocab_size

        self.is_first_shard = start_layer == 0
        self.is_last_shard = end_layer == config.num_hidden_layers
        self.n_layers_in_shard = end_layer - start_layer

        if self.is_first_shard:
            self.embed_tokens = nn.Embedding(self.vocab_size, self.hidden_size)
            if has_norm_in:
                self.norm_in = nn.RMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        else:
            self.embed_tokens = None
            self.norm_in = None

        self.layers = [
            block_class(config, layer_idx) for layer_idx in range(start_layer, end_layer)
        ]

        if self.is_last_shard:
            self.norm = nn.RMSNorm(self.hidden_size, eps=config.rms_norm_eps)
            self.lm_head = nn.Linear(self.hidden_size, self.vocab_size, bias=False)
        else:
            self.norm = None
            self.lm_head = None

    def logits_to_tokens(
        self,
        logits: mx.array,
        lengths: Optional[mx.array] = None,
        sampling_info: Optional[SamplingBatchInfo] = None,
    ) -> mx.array:
        """Convert logits to token IDs with greedy decoding.

        Args:
            logits: (batch, target_len_padded, vocab_size), logits from final lm_head
            lengths: (batch,), int array of true lengths
            sampling_info: sampling info of the batched requests

        Return:
            Generated tokens of shape (batch,).
        """
        if logits.ndim != 3:
            raise ValueError(f"Logits must be 3D, but got shape {logits.shape}")

        if lengths is not None:
            # To select the logit vector for the last valid token of each sequence,
            # we need to provide indices for both the batch and sequence dimensions.
            batch_indices = mx.arange(logits.shape[0])
            last_token_indices = lengths - 1
            last_token_logits = logits[batch_indices, last_token_indices, :]
        else:
            # If no lengths are provided, assume all sequences are of max length
            # and we are interested in the very last token's logits.
            last_token_logits = logits[:, -1, :]

        # last_token_logits now has shape (batch_size, vocab_size)
        if sampling_info is None:
            next_token_ids = mx.argmax(last_token_logits, axis=-1)
        else:
            sampler = Sampler()
            next_token_ids = sampler(last_token_logits, sampling_info)
        return next_token_ids

    def __call__(
        self,
        h_or_tokens: mx.array,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        mask: Optional[mx.array] = None,
        block_tables: Optional[mx.array] = None,
        context_lengths: Optional[mx.array] = None,
        slot_mapping: Optional[mx.array] = None,
        window_size: Optional[int] = None,
    ) -> mx.array:
        """
        Args:
            h_or_tokens:
                (batch, target_len_padded, D) or (batch, target_len_padded) for prefill,
                (batch, 1, D) or (batch, 1) for decode.
            cache: PagedAttention:
                   (key_cache_global, value_cache_global)
                   has for shape: (num_layers, num_blocks, num_kv_heads, block_size, head_dim)
            lengths: (batch,) true lengths of each sequence in batch.
            mask: Optional causal mask for the current segment.
            window_size: Optional int, if provided, will use a sliding window attention mask.
            block_tables: (batch, max_blocks) for PagedAttention.
            context_lengths: (batch,) for PagedAttention.
            slot_mapping: (total_tokens,) for PagedAttention.
        """
        h = h_or_tokens
        target_len = h.shape[1]

        if self.is_first_shard:
            if self.embed_tokens is None:
                raise ValueError("embed_tokens is None for the first shard.")
            h = self.embed_tokens(h)
            if self.has_norm_in and self.norm_in:
                h = self.norm_in(h)

        if target_len > 1 and mask is None:
            raise ValueError("ShardedModel: mask cannot be None for prefill.")

        for _, layer_module in enumerate(self.layers):
            h = layer_module(
                h,
                mask=mask,
                cache=cache,
                block_tables=block_tables,
                context_lengths=context_lengths,
                slot_mapping=slot_mapping,
            )

        if self.is_last_shard:
            if self.norm is None or self.lm_head is None:
                raise ValueError("ShardedModel: norm or lm_head is None for the last shard.")
            h = self.norm(h)
            h = self.lm_head(h)

        return h


================================================================================
File: src/parallax/server/node_chat_http_server.py
Size: 12.65 kB
================================================================================

import asyncio
import json
import time
from typing import Dict

import fastapi
import uvicorn
from fastapi import Request
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from lattica import Lattica
from starlette.concurrency import iterate_in_threadpool
from starlette.datastructures import State

from backend.server.rpc_connection_handler import RPCConnectionHandler
from parallax_utils.file_util import get_project_root
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)

import uuid

# Fast API
app = fastapi.FastAPI(
    openapi_url="/openapi.json",
)


async def init_app_states(state: State, node_chat_http_server):
    """Init FastAPI app states, including http handler, etc."""
    state.http_server = node_chat_http_server


async def v1_chat_completions(request_data: Dict, request_id: str, received_ts: int):
    return await app.state.http_server.chat_completion(request_data, request_id, received_ts)


async def get_cluster_status():
    return app.state.http_server.get_cluster_status()


@app.post("/v1/chat/completions")
async def openai_v1_chat_completions(raw_request: Request):
    """OpenAI v1/chat/complete post function"""
    request_data = await raw_request.json()
    request_id = uuid.uuid4()
    received_ts = time.time()
    return await v1_chat_completions(request_data, request_id, received_ts)


@app.get("/cluster/status")
async def cluster_status():
    return await get_cluster_status()


# Disable caching for index.html
@app.get("/")
async def serve_index():
    response = FileResponse(str(get_project_root()) + "/src/frontend/dist/chat.html")
    # Disable cache
    response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


# mount the frontend
app.mount(
    "/",
    StaticFiles(directory=str(get_project_root() / "src" / "frontend" / "dist"), html=True),
    name="static",
)


class NodeChatHttpServer:

    def __init__(self, args):
        self.host = args.host
        self.port = args.node_chat_port
        self.tcp_port = args.tcp_port
        self.udp_port = args.udp_port
        self.scheduler_addr = args.scheduler_addr
        self.relay_servers = args.relay_servers
        self.announce_maddrs = args.announce_maddrs
        self.initial_peers = args.initial_peers
        self.host_maddrs = (
            [f"/ip4/0.0.0.0/tcp/{self.tcp_port}", f"/ip4/0.0.0.0/udp/{self.udp_port}/quic-v1"],
        )
        self.scheduler_peer_id = None
        self.scheduler_stub = None
        self.lattica = None

    def build_lattica(self):
        self.lattica = Lattica.builder().with_listen_addrs(self.host_maddrs)

        if self.scheduler_addr is not None and self.scheduler_addr != "auto":
            if self.scheduler_addr.startswith("/"):
                logger.info(f"Using scheduler addr: {self.scheduler_addr}")
                self.lattica.with_bootstraps([self.scheduler_addr])
            self.scheduler_peer_id = self.scheduler_addr.split("/")[-1]

        if len(self.relay_servers) > 0:
            logger.info(f"Using relay servers: {self.relay_servers}")
            self.lattica.with_relay_servers(self.relay_servers).with_dcutr(True)
            if self.scheduler_peer_id is not None:
                logger.info(f"Using protocol: /{self.scheduler_peer_id}")
                self.lattica.with_protocol("/" + self.scheduler_peer_id)

        if len(self.announce_maddrs) > 0:
            logger.info(f"Using announce maddrs: {self.announce_maddrs}")
            self.lattica.with_external_addrs(self.announce_maddrs)

        if len(self.initial_peers) > 0:
            logger.info(f"Using initial peers: {self.initial_peers}")
            self.lattica.with_bootstraps(self.initial_peers)

        self.lattica.build()

        if len(self.relay_servers) > 0:
            try:
                is_symmetric_nat = self.lattica.is_symmetric_nat()
                if is_symmetric_nat is None:
                    logger.warning("Failed to get is symmetric NAT, skip")
                elif is_symmetric_nat:
                    logger.error(
                        "Your network NAT type is symmetric, relay does not work on this type of NAT, see https://en.wikipedia.org/wiki/Network_address_translation"
                    )
                    exit(1)
            except Exception as e:
                logger.exception(f"Error in is symmetric NAT: {e}, skip")

        if self.scheduler_addr == "auto":
            self.scheduler_peer_id = None
            for _ in range(20):
                try:
                    time.sleep(3)
                    self.scheduler_peer_id = self.lattica.get("scheduler_peer_id")
                    if self.scheduler_peer_id is not None:
                        self.scheduler_peer_id = self.scheduler_peer_id.value
                        logger.info(f"Found scheduler peer id: {self.scheduler_peer_id}")
                        break
                    logger.info(
                        f"Discovering scheduler peer id, {_ + 1} times, you can specify scheduler peer id by -s"
                    )
                except Exception as e:
                    logger.warning(f"Failed to get scheduler addr: {e}, waiting for 3 seconds.")
            if self.scheduler_peer_id is None:
                logger.error("Failed to get scheduler peer id")
                return False

        # ensure connect to scheduler
        for _ in range(10):
            if self.scheduler_peer_id in self.lattica.get_all_peers():
                return True
            logger.warning("Scheduler peer id not found, waiting for 1 second.")
            time.sleep(1)

        self.lattica.close()
        return False

    async def chat_completion(self, request_data, request_id: str, received_ts: int):
        if self.scheduler_addr is not None:  # central scheduler mode
            try:
                if self.scheduler_stub is None:
                    self.scheduler_stub = RPCConnectionHandler(self.lattica, None, None).get_stub(
                        self.scheduler_peer_id
                    )
                stub = self.scheduler_stub
                is_stream = request_data.get("stream", False)
                try:
                    if is_stream:

                        async def stream_generator():
                            response = stub.chat_completion(request_data)

                            try:
                                iterator = iterate_in_threadpool(response)
                                async for chunk in iterator:
                                    yield chunk
                            finally:
                                response.cancel()

                        resp = StreamingResponse(
                            stream_generator(),
                            media_type="text/event-stream",
                            headers={
                                "X-Content-Type-Options": "nosniff",
                                "Cache-Control": "no-cache",
                            },
                        )
                        logger.debug(f"Streaming response initiated for {request_id}")
                        return resp
                    else:
                        response = stub.chat_completion(request_data)
                        content = (await anext(iterate_in_threadpool(response))).decode()
                        logger.debug(f"Non-stream response completed for {request_id}")
                        # response is a JSON string; parse to Python object before returning
                        return JSONResponse(content=json.loads(content))
                except Exception as e:
                    logger.exception(f"Error in _forward_request: {e}")
                    return JSONResponse(
                        content={"error": "Internal server error"},
                        status_code=500,
                    )

            except Exception as e:
                logger.exception(f"Error in chat completion: {e}")
                return JSONResponse(
                    content={"error": "Internal server error"},
                    status_code=500,
                )
        else:
            logger.error("No scheduler address specified")
            return JSONResponse(
                content={"error": "No scheduler address specified"},
                status_code=500,
            )

    def get_cluster_status(self):
        if self.scheduler_addr is not None:  # central scheduler mode
            try:
                if self.scheduler_stub is None:
                    self.scheduler_stub = RPCConnectionHandler(self.lattica, None, None).get_stub(
                        self.scheduler_peer_id
                    )
                stub = self.scheduler_stub
                try:

                    async def stream_status():
                        response = stub.cluster_status()

                        async def non_blocking_next_streamiter(stream_iter, timeout=0.01):
                            try:
                                chunk = await asyncio.wait_for(
                                    asyncio.get_event_loop().run_in_executor(
                                        None, next, stream_iter
                                    ),
                                    timeout=timeout,
                                )
                                return chunk
                            except asyncio.TimeoutError:
                                return None
                            except StopIteration:
                                return None

                        try:
                            as_iterator = iter(response)
                            for _ in range(60):
                                await asyncio.sleep(1)
                                chunk = await non_blocking_next_streamiter(as_iterator, timeout=0.1)
                                if chunk is None:
                                    continue
                                yield (chunk)
                        finally:
                            response.cancel()

                    resp = StreamingResponse(
                        stream_status(),
                        media_type="application/x-ndjson",
                        headers={
                            "X-Content-Type-Options": "nosniff",
                            "Cache-Control": "no-cache",
                        },
                    )
                    return resp
                except Exception as e:
                    logger.exception(f"Error in get cluster status: {e}")
                    return JSONResponse(
                        content={"error": "Internal server error"},
                        status_code=500,
                    )

            except Exception as e:
                logger.exception(f"Error in get cluster status: {e}")
                return JSONResponse(
                    content={"error": "Internal server error"},
                    status_code=500,
                )
        else:
            logger.error("No scheduler address specified")
            return JSONResponse(
                content={"error": "No scheduler address specified"},
                status_code=500,
            )

    async def run_uvicorn(self):
        """
        Since uvicorn.run() uses asyncio.run, we need another wrapper
        to create a uvicorn asyncio task to run multiple tasks.
        """
        config = uvicorn.Config(
            app,
            host=self.host,
            port=self.port,
            timeout_keep_alive=5,
            loop="uvloop",
        )
        server = uvicorn.Server(config)
        await server.serve()

    def run(self):
        """
        Launch A FastAPI server that routes requests to the executor.

        Note:
        1. The HTTP server and executor both run in the main process.
        2. Inter-process communication is done through IPC (each process uses a different port) via the ZMQ library.
        """
        while not self.build_lattica():
            logger.error("Failed to build lattica, waiting for 10 seconds")
            time.sleep(10)
        logger.info("Lattica built successfully")

        asyncio.run(
            init_app_states(
                app.state,
                self,
            )
        )
        asyncio.run(self.run_uvicorn())


def run_node_chat_http_server(args):
    node_chat_http_server = NodeChatHttpServer(args)
    node_chat_http_server.run()


================================================================================
File: src/parallax/server/paged_kv_cache.py
Size: 8.61 kB
================================================================================

from typing import Dict, List, Optional, Set, Tuple

import mlx.core as mx

from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


class BlockAllocator:
    """Manages allocation of physical block indices."""

    def __init__(self, num_blocks: int, block_size: int):
        self.num_blocks = num_blocks
        self.block_size = block_size
        # Initialize free blocks stack
        # Using a list as a stack is efficient for LIFO allocation
        self.free_blocks: List[int] = list(range(num_blocks))
        # Keep track of used blocks for safety/debugging
        self.used_blocks: Set[int] = set()

    def allocate(self, num_blocks_needed: int) -> List[int]:
        """Allocates `num_blocks_needed` physical blocks."""
        if len(self.free_blocks) < num_blocks_needed:
            # Out of memory
            return []

        # Pop blocks from the stack
        split_idx = len(self.free_blocks) - num_blocks_needed
        allocated = self.free_blocks[split_idx:]
        self.free_blocks = self.free_blocks[:split_idx]

        for b in allocated:
            self.used_blocks.add(b)

        return allocated

    def free(self, blocks: List[int]):
        """Frees the given physical blocks."""
        for b in blocks:
            if b in self.used_blocks:
                self.used_blocks.remove(b)
                self.free_blocks.append(b)
            else:
                logger.warning(f"Double free detected for block {b}")

    def get_num_free_blocks(self) -> int:
        return len(self.free_blocks)

    def get_num_used_blocks(self) -> int:
        return len(self.used_blocks)


class PagedKVCacheManager:
    """
    Manages the Paged KV Cache tensors and block tables for requests.
    """

    def __init__(
        self,
        num_layers: int,
        num_kv_heads: int,
        head_dim: int,
        dtype: mx.Dtype,
        block_size: int = 16,
        cache_memory_fraction: float = 0.8,
        num_gpu_blocks: Optional[int] = None,
        max_num_seqs: int = 256,  # Max concurrent requests hint
        head_dim_v: Optional[int] = None,
    ):
        self.num_layers = num_layers
        self.num_kv_heads = num_kv_heads
        self.head_dim = head_dim
        self.head_dim_v = head_dim_v if head_dim_v is not None else head_dim
        self.dtype = dtype
        self.block_size = block_size
        self.max_num_seqs = max_num_seqs

        if num_gpu_blocks is None:
            num_gpu_blocks = self._calculate_num_blocks(cache_memory_fraction, dtype)

        self.num_gpu_blocks = num_gpu_blocks

        # 1. Initialize Allocator
        self.allocator = BlockAllocator(num_gpu_blocks, block_size)

        # 2. Allocate Global Cache Tensors
        # Shape: (num_layers, num_blocks, num_kv_heads, block_size, head_dim)
        logger.info(
            f"Allocating Paged KV Cache: {num_gpu_blocks} blocks, {block_size} block_size, "
            f"k_head_dim={self.head_dim}, v_head_dim={self.head_dim_v}"
        )

        self.key_cache = mx.zeros(
            (num_layers, num_gpu_blocks, num_kv_heads, block_size, self.head_dim), dtype=dtype
        )
        self.value_cache = mx.zeros(
            (num_layers, num_gpu_blocks, num_kv_heads, block_size, self.head_dim_v), dtype=dtype
        )

        # Ensure memory is materialized
        mx.eval(self.key_cache, self.value_cache)

        # 3. Request State Management
        # Mapping: request_id -> List of physical block indices
        self.block_tables: Dict[str, List[int]] = {}
        # Mapping: request_id -> current context length (number of tokens)
        self.context_lengths: Dict[str, int] = {}

    def _calculate_num_blocks(self, cache_memory_fraction: float, dtype: mx.Dtype) -> int:

        device_info = mx.metal.device_info()
        total_mem = device_info["max_recommended_working_set_size"]
        current_mem = mx.metal.get_active_memory()
        free_mem = total_mem - current_mem

        # We use a fraction of FREE memory, but for safety in multi-process/multi-model
        # scenarios, we might want to base it on TOTAL memory fraction if we know
        # what we are doing (as in Executor logic).
        # However, to be safe and consistent with previous logic that tried to avoid OOM:
        # Let's stick to the logic that available_for_kv is based on free memory
        # OR total_memory * fraction if we trust the fraction to be per-process adjusted.

        # If fraction is small (e.g. < 0.2), it likely means it's per-process adjusted.
        # But here we stick to "use what is available" to be safe.
        available_for_kv = free_mem * cache_memory_fraction

        dtype_size = 2 if dtype in [mx.float16, mx.bfloat16] else 4

        # Calculate bytes per block considering potentially different K and V head dimensions
        key_block_bytes = (
            self.num_layers * self.num_kv_heads * self.block_size * self.head_dim * dtype_size
        )
        value_block_bytes = (
            self.num_layers * self.num_kv_heads * self.block_size * self.head_dim_v * dtype_size
        )
        block_bytes = key_block_bytes + value_block_bytes

        num_gpu_blocks = int(available_for_kv // block_bytes)

        if num_gpu_blocks <= 0:
            logger.warning(
                f"Not enough memory for KV cache. Total: {total_mem / 1024**3:.2f} GB, "
                f"Used: {current_mem / 1024**3:.2f} GB, Free: {free_mem / 1024**3:.2f} GB. "
                f"Defaulting to minimal 16 blocks."
            )
            num_gpu_blocks = 16

        logger.info(
            f"PagedKVCache: Calculated num_gpu_blocks={num_gpu_blocks} based on "
            f"fraction={cache_memory_fraction:.2f}, "
            f"total_mem={total_mem/1024**3:.2f} GB, "
            f"used_mem={current_mem/1024**3:.2f} GB, "
            f"free_mem={free_mem/1024**3:.2f} GB, "
            f"available_for_kv={available_for_kv/1024**3:.2f} GB"
        )
        return num_gpu_blocks

    def get_num_free_blocks(self) -> int:
        return self.allocator.get_num_free_blocks()

    def can_allocate(self, num_tokens: int) -> bool:
        num_blocks = (num_tokens + self.block_size - 1) // self.block_size
        return self.allocator.get_num_free_blocks() >= num_blocks

    def allocate_request(self, request_id: str, prompt_len: int) -> bool:
        """
        Allocates initial blocks for a new request (Prefill).
        Returns True if successful, False if OOM.
        """
        if request_id in self.block_tables:
            return True

        num_blocks = (prompt_len + self.block_size - 1) // self.block_size
        blocks = self.allocator.allocate(num_blocks)

        if len(blocks) < num_blocks:
            # Allocation failed
            if blocks:
                self.allocator.free(blocks)
            return False

        self.block_tables[request_id] = blocks
        self.context_lengths[request_id] = prompt_len
        return True

    def has_request(self, request_id: str) -> bool:
        return request_id in self.block_tables

    def free_request(self, request_id: str):
        """Frees all blocks associated with a request."""
        if request_id in self.block_tables:
            blocks = self.block_tables[request_id]
            self.allocator.free(blocks)
            del self.block_tables[request_id]
            del self.context_lengths[request_id]

    def release_request(self, request_id: str):
        """Alias for free_request to match Executor expectation."""
        self.free_request(request_id)

    def append_slot(self, request_id: str) -> bool:
        """
        Allocates a new slot for the next token generation (Decode).
        If the last block is full, allocates a new block.
        """
        if request_id not in self.block_tables:
            raise ValueError(f"Request {request_id} not found")

        current_len = self.context_lengths[request_id]

        if current_len % self.block_size == 0:
            new_blocks = self.allocator.allocate(1)
            if not new_blocks:
                return False  # OOM
            self.block_tables[request_id].extend(new_blocks)

        self.context_lengths[request_id] += 1
        return True

    def get_block_table(self, request_id: str) -> List[int]:
        return self.block_tables.get(request_id, [])

    def get_context_length(self, request_id: str) -> int:
        return self.context_lengths.get(request_id, 0)

    def get_cache(self) -> Tuple[mx.array, mx.array]:
        """Returns the global cache tensors."""
        return self.key_cache, self.value_cache


================================================================================
File: src/parallax/server/radix_cache.py
Size: 14.62 kB
================================================================================

"""
Prefix Cache class for KV Cache reuse.
This module is implemented using radix tree, which retains the
same as SGLang.
"""

import heapq
import time
from collections import defaultdict
from functools import partial
from typing import Dict, List, Optional, Tuple

import mlx.core as mx

from parallax.server.kv_cache import KVCache
from parallax.server.request import Request


class TreeNode:
    """
    Radix tree node data structure.
    Key: token id list. It should be an empty list for the root node.
    Value: kv cache positions.
    """

    counter = 0

    def __init__(self, node_id: Optional[int] = None):
        self.children = defaultdict(TreeNode)
        self.parent: TreeNode = None
        self.key: List[int] = None
        self.value: Optional[List[int]] = None
        self.kv_cache = None
        self.lock_ref = 0
        self.last_access_time = time.monotonic()

        self.hit_count = 0

        self.node_id = TreeNode.counter if node_id is None else node_id
        TreeNode.counter += 1

    @property
    def evicted(self):
        """Check if this node has been evicted"""
        return self.value is None

    def __lt__(self, other: "TreeNode"):
        return self.last_access_time < other.last_access_time


def _key_match_page_size1(key0: List, key1: List):
    """Key match function especially for page_size=1"""
    i = 0
    for k0, k1 in zip(key0, key1):
        if k0 != k1:
            break
        i += 1
    return i


def _key_match_paged(key0: List, key1: List, page_size: int):
    """Key match function for page_size>1"""
    min_len = min(len(key0), len(key1))

    i = 0
    while i < min_len:
        if key0[i : i + page_size] != key1[i : i + page_size]:
            break
        i += page_size

    return i


class RadixCache:
    """
    Manages Radix Cache for the running executor.
    Note: Currently only support page_size=1.
    """

    def __init__(
        self,
        num_kv_heads: int,
        head_dim: int,
        num_layers: int,
        dtype: mx.Dtype,
        page_size: int = 1,
        max_num_tokens: int = None,
    ):
        self.num_kv_heads = num_kv_heads
        self.head_dim = head_dim
        self.num_layers = num_layers
        self.dtype = dtype
        self.page_size = page_size
        self.req_to_token: Dict[str, List[int]] = {}
        if max_num_tokens is None:
            self.max_num_tokens = 10000
        else:
            self.max_num_tokens = max_num_tokens

        if self.page_size == 1:
            self.key_match_fn = _key_match_page_size1
            self.get_child_key_fn = lambda key: key[0]
        else:
            self.key_match_fn = partial(_key_match_paged, page_size=page_size)
            self.get_child_key_fn = lambda key: tuple(key[:page_size])
        self.reset()

    def reset(self):
        """Reset function for the whole tree"""
        self.root_node = TreeNode()
        self.root_node.key = []
        self.root_node.value = []
        self.root_node.lock_ref = 1
        self.evictable_size_ = 0
        self.protected_size_ = 0
        self.req_to_token = {}

    def update_req_to_token(self, req_id: str, token_ids: List[int]):
        """Update the req->tokens dict"""
        value = self.req_to_token.get(req_id)
        if value:
            self.req_to_token[req_id] = self.req_to_token[req_id] + token_ids
        else:
            self.req_to_token[req_id] = token_ids

    def evict_request(self, req_id: str):
        """Remove a single request. Used when request if finished or cached"""
        del self.req_to_token[req_id]

    def match_prefix(
        self,
        key: List[int],
    ) -> Tuple[mx.array, mx.array, int]:
        """Find the matching prefix from the radix tree.
        Args:
            key: A list of token IDs to find a matching prefix.
        Returns:
            A tuple of (value, matched last node)
        Note that this API can modify the internal state of the Radix tree.
        The last node creates a new child if the prefix is shorter than
        the last node's value.
        """
        if len(key) == 0:
            return (
                [],
                self.root_node,
            )

        if self.page_size != 1:
            page_aligned_len = len(key) // self.page_size * self.page_size
            key = key[:page_aligned_len]

        value, last_node = self._match_prefix_helper(self.root_node, key)
        return value, last_node

    def fetch_kv_cache(self, node: TreeNode):
        """
        Get and concat kv cache from a tree node to the root.
        """
        assert node != self.root_node, "should not fetch from the root node."
        k_cache, v_cache = node.kv_cache.fetch()
        node = node.parent
        while node != self.root_node:
            cur_k_cache, cur_v_cache = node.kv_cache.fetch()
            k_cache = mx.concatenate([cur_k_cache, k_cache], axis=2)
            v_cache = mx.concatenate([cur_v_cache, v_cache], axis=2)
            node = node.parent
        return k_cache, v_cache

    def insert(self, key: List, value, k_cache: mx.array, v_cache: mx.array):
        """Insert a tree node."""
        if value is None:
            value = list(key)
        return self._insert_helper(self.root_node, key, value, k_cache, v_cache)

    def evict(self, num_tokens: int):
        """Remove cached tokens until the total tokens stored is reduced by num_tokens"""
        leaves = self._collect_leaves()
        heapq.heapify(leaves)

        num_evicted = 0
        while num_evicted < num_tokens and len(leaves) > 0:
            x = heapq.heappop(leaves)

            if x == self.root_node:
                break
            if x.lock_ref > 0:
                continue

            # self.token_to_kv_pool_allocator.free(x.value) TODO
            num_evicted += len(x.value)
            self._delete_leaf(x)

            if len(x.parent.children) == 0:
                heapq.heappush(leaves, x.parent)

    def pretty_print(self):
        """Print the whole tree."""
        self._print_helper(self.root_node, 0)
        print(f"#tokens: {self.total_size()}")

    def total_size(self):
        """Get the total number of tokens stored in the tree."""
        return self._total_size_helper()

    def increase_lock_ref(self, node: TreeNode):
        """Increase the lock reference by 1 from a node to the root."""
        delta = 0
        while node != self.root_node:
            if node.lock_ref == 0:
                self.evictable_size_ -= len(node.value)
                self.protected_size_ += len(node.value)
                delta -= len(node.value)
            node.lock_ref += 1
            node = node.parent
        return delta

    def decrease_lock_ref(self, node: TreeNode):
        """decrease the lock reference by 1 from a node to the root."""
        delta = 0
        while node != self.root_node:
            if node.lock_ref == 1:
                self.evictable_size_ += len(node.value)
                self.protected_size_ -= len(node.value)
                delta += len(node.value)
            if node.lock_ref > 0:
                node.lock_ref -= 1
            node = node.parent
        return delta

    def cache_finished_request(self, req: Request, k_cache: mx.array, v_cache: mx.array):
        """Cache request when it finishes."""
        token_ids = self.req_to_token[req.request_id]
        _, node = self.insert(key=token_ids, value=None, k_cache=k_cache, v_cache=v_cache)
        self.decrease_lock_ref(node)

        if self.protected_size_ > self.max_num_tokens:
            self.evict(self.protected_size_)
        elif self.protected_size_ + self.evictable_size_ > self.max_num_tokens:
            self.evict(self.protected_size_ + self.evictable_size_ - self.max_num_tokens)

    def cache_unfinished_request(self, req: Request, k_cache: mx.array, v_cache: mx.array):
        """Cache request when it is unfinished."""
        token_ids = self.req_to_token[req.request_id]
        _, node = self.insert(key=token_ids, value=None, k_cache=k_cache, v_cache=v_cache)
        self.increase_lock_ref(node)
        if self.protected_size_ > self.max_num_tokens:
            self.evict(self.protected_size_)
        elif self.protected_size_ + self.evictable_size_ > self.max_num_tokens:
            self.evict(self.protected_size_ + self.evictable_size_ - self.max_num_tokens)

    def _match_prefix_helper(self, node: TreeNode, key: List):
        """Match prefix helper function"""
        node.last_access_time = time.monotonic()

        child_key = self.get_child_key_fn(key)

        value = []
        while len(key) > 0 and child_key in node.children.keys():
            child = node.children[child_key]
            child.last_access_time = time.monotonic()
            prefix_len = self.key_match_fn(child.key, key)
            if prefix_len >= len(child.key):
                value += child.value
                node = child
                key = key[prefix_len:]
                if len(key):
                    child_key = self.get_child_key_fn(key)
            else:
                new_node = self._split_node(child.key, child, prefix_len)
                value += new_node.value
                node = new_node
                break

        return value, node

    def _split_node(self, key, child: TreeNode, split_len: int):
        """Split a node for insertion. Note that this node be any nodes in the tree."""
        # new_node -> child
        new_node = TreeNode()
        new_node.children = {self.get_child_key_fn(key[split_len:]): child}
        new_node.parent = child.parent
        new_node.lock_ref = child.lock_ref
        new_node.key = child.key[:split_len]
        new_node.value = child.value[:split_len]
        child.parent = new_node
        child.key = child.key[split_len:]
        child.value = child.value[split_len:]
        new_node.parent.children[self.get_child_key_fn(key)] = new_node

        child_k_cache, child_v_cache = child.kv_cache.fetch()
        # create kv cache for new_node
        new_k_cache = child_k_cache[..., :split_len, :]
        new_v_cache = child_v_cache[..., :split_len, :]
        new_node.kv_cache = KVCache(
            num_kv_heads=self.num_kv_heads,
            head_dim=self.head_dim,
            num_layers=self.num_layers,
            dtype=self.dtype,
            block_size=self.page_size,
            num_initial_tokens=self.page_size,
        )
        new_node.kv_cache.update(new_k_cache, new_v_cache)
        # update kv cache for child
        child_k_cache = child_k_cache[..., split_len:, :]
        child_v_cache = child_v_cache[..., split_len:, :]
        child.kv_cache = KVCache(
            num_kv_heads=self.num_kv_heads,
            head_dim=self.head_dim,
            num_layers=self.num_layers,
            dtype=self.dtype,
            block_size=self.page_size,
            num_initial_tokens=self.page_size,
        )
        child.kv_cache.update(child_k_cache, child_v_cache)

        return new_node

    def _collect_leaves(self):
        """Returns all the leaf nodes from the root"""
        ret_list = []
        stack = [self.root_node]

        while stack:
            cur_node = stack.pop()
            if len(cur_node.children) == 0:
                ret_list.append(cur_node)
            else:
                stack.extend(cur_node.children.values())

        return ret_list

    def _insert_helper(
        self, node: TreeNode, key: List, value: List, k_cache: mx.array, v_cache: mx.array
    ):
        """Insert key-value helper function"""
        node.last_access_time = time.monotonic()
        if len(key) == 0:
            return 0

        child_key = self.get_child_key_fn(key)

        total_prefix_length = 0
        while len(key) > 0 and child_key in node.children.keys():
            node = node.children[child_key]
            node.last_access_time = time.monotonic()
            prefix_len = self.key_match_fn(node.key, key)
            total_prefix_length += prefix_len
            key = key[prefix_len:]
            value = value[prefix_len:]

            if prefix_len < len(node.key):
                new_node = self._split_node(node.key, node, prefix_len)
                node = new_node

            if len(key):
                child_key = self.get_child_key_fn(key)

        if len(key):
            new_node = TreeNode()
            new_node.parent = node
            new_node.key = key
            new_node.value = value
            node.children[child_key] = new_node
            self.evictable_size_ += len(value)

            # create kvcache for new_node
            kv_cache = KVCache(
                num_kv_heads=self.num_kv_heads,
                head_dim=self.head_dim,
                num_layers=self.num_layers,
                dtype=self.dtype,
                block_size=self.page_size,
                num_initial_tokens=self.page_size,
            )
            k_cache = k_cache[..., total_prefix_length:, :]
            v_cache = v_cache[..., total_prefix_length:, :]
            kv_cache.update(k_cache, v_cache)
            new_node.kv_cache = kv_cache

            node = new_node

        return total_prefix_length, node

    def _delete_leaf(self, node):
        """Deletes a leaf node."""
        for k, v in node.parent.children.items():
            if v == node:
                del node.parent.children[k]
                break
        self.evictable_size_ -= len(node.key)

    def _print_helper(self, node: TreeNode, indent: int):
        """Prints the radix tree in a human-readable format."""
        stack = [(node, indent)]
        while stack:
            current_node, current_indent = stack.pop()
            print(
                " " * current_indent,
                len(current_node.key),
                current_node.key[:10],
                f"r={current_node.lock_ref}",
                current_node.kv_cache,
            )
            for key, child in current_node.children.items():
                stack.append((child, current_indent + 2))

                assert key == self.get_child_key_fn(
                    child.key
                ), f"{key=}, {self.get_child_key_fn(child.key)=}"

    def _total_size_helper(self):
        """Get total number of tokens stored helper function"""
        total_size = 0
        stack = [self.root_node]
        while stack:
            current_node = stack.pop()
            total_size += len(current_node.value)
            for child in current_node.children.values():
                if child.evicted:
                    continue
                stack.append(child)
        return total_size


================================================================================
File: src/parallax/server/request.py
Size: 14 kB
================================================================================

"""
A minimum implementation of Request objects for managing inference requests.

Each time a client initialize a generation request by sending prompt texts to the First Peer,
    it will initialize an `InitialRequest` instance.
Each Intermediate Peer will only hold `IntermediateRequest`,
    which contains part of the info of `InitialRequest`.
        * request id,
        * current position.
        * relevant hidden_states (prompt_len, hidden_size) or (1, hidden_size)
    We will pack `IntermediateRequest` and send to the following peers.

In our current design, the Last Peer won't hold `InitialRequest` for several reasons:
    * in case of weight tying, it doens't need to load and compute lm_head GEMM;
    * easier management for Request state updates (only the First Peer will handle it);
    * no need to hold tokenizer so that we can decokenize first and then stream to Client;
    * we need to announce to the First Peer to evict the request from its running batch anyway.
    - slight delay for client to recieve the streaming token.

Forming requests to batches (see scheduler.py for details):
    Our scheduler can manage both `InitialRequest` and `IntermediateRequest`.

A complete workflow:
Prefill:
    First Peer:
        * Initialize `InitialRequest`;
        (* Put the request into the schduler's pool;)
        * Feed `input_ids` into executor;
        * Store each layer's hidden states to paged KV Cache;
        * Generates the last layer (that it holds)'s hidden states;
        * Build `IntermediateRequest` with the request id,
            current position, and hidden states (prompt_len, hidden_size);
        * Sends `IntermediateRequest` to the next Peer.
    Intermediate Peers:
        * Accepts `IntermediateRequest`;
        * Feed `hidden_states` (prompt_len, hidden_size) to executor, updates caches
        * Update `IntermediateRequest` with the hidden states and send it to the next peer;
    Last Peer:
        * Accepts `IntermediateRequest`;
        * Generates and sends the first `output_id` (without weight tying) to the first Peer.

    Now, the first peer will
        1. Detokenize the request and stream it to the client;
        2. Switch request status from Prefill to Decode
        3. Add output token id to the ouputs and increase the processed length.
        4. Check if the request should be switch to Finished state, evict if necessary.

Decode:
    First Peer: Feed singleton `output_id` to executor,
        generates and sends `hidden_states` (1, hidden_size);
    Intermediate Peers: takes in `hidden_states`,
        generates and sends `hidden_states` (1, hidden_size);
    Last Peer: takes in `hidden_states`,
        generates and sends `output_id` to the first Peer.

TODO:
    1. Add support for multiple output_ids in a single step (e.g. beam width, top-k sampling, etc.);
    2. Accepts more generation configs like repetition penalties.
"""

import uuid
from enum import Enum
from typing import Any, List, Optional

from parallax.server.sampling.sampling_params import SamplingParams
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


class RequestStatus(Enum):
    """Enumeration of possible request statuses for the First Peer."""

    PREFILLING = "PREFILLING"
    DECODING = "DECODING"
    FINISHED_EOS = "FINISHED_EOS"
    FINISHED_MAX_LENGTH = "FINISHED_MAX_LENGTH"
    ERROR = "ERROR"
    CANCELLED = "CANCELLED"


class Request:
    """
    Base class for requests in the Parallax server.
    This is a placeholder and can be extended for specific request types.
    """

    def __init__(
        self,
        request_id: Optional[str] = None,
        status: RequestStatus = RequestStatus.PREFILLING,
        prompt_len: int = 0,
        input_ids: Optional[List[int]] = None,
        output_ids: Optional[List[int]] = None,
        routing_table: Optional[List[str]] = [],
        sampling_params: Optional[SamplingParams] = None,
        lora_path: Optional[str] = None,
    ):
        self.request_id = request_id or str(uuid.uuid4())
        self.status = status
        self.prompt_len = prompt_len
        self.output_ids = output_ids or []
        self.input_ids = input_ids or []
        self.routing_table = routing_table
        self.sampling_params = sampling_params or SamplingParams()
        self.abort = False
        self.ready_for_next_step = False
        self.last_updated_time: Optional[float] = None
        self.lora_id: Optional[str] = None
        self.lora_path = lora_path

    @property
    def is_finished(self) -> bool:
        """Checks if the request has finished processing."""
        return self.status in [
            RequestStatus.FINISHED_EOS,
            RequestStatus.FINISHED_MAX_LENGTH,
            RequestStatus.ERROR,
            RequestStatus.CANCELLED,
        ]

    @property
    def is_prefill(self) -> bool:
        """Checks if the request is in the prefill stage."""
        return self.status == RequestStatus.PREFILLING

    @property
    def is_decoding(self) -> bool:
        """Checks if the request is in the decoding stage."""
        return self.status == RequestStatus.DECODING

    def update_status(self, new_status: RequestStatus = RequestStatus.DECODING):
        """
        Update the status of the request.
        """
        if self.is_finished:
            logger.warning(
                f"Request {self.request_id}: Attempted to update status of a finished request."
            )
            return
        self.status = new_status
        logger.debug(f"Request {self.request_id} status updated to {self.status}.")


class InitialRequest(Request):
    """
    Represents the full state of a user's generation request, managed by the First Peer.
    """

    def __init__(
        self,
        prompt: Optional[str] = None,
        request_id: Optional[str] = None,
        output_ids: Optional[List[int]] = None,
        input_ids: Optional[List[int]] = None,
        sampling_params: Optional[SamplingParams] = None,
        max_new_tokens: int = 512,
        max_total_length: int = 1024,
        status: RequestStatus = RequestStatus.PREFILLING,
        lora_path: Optional[str] = None,
    ):
        if not prompt and not input_ids:
            raise ValueError("prompt or input_ids cannot be empty.")
        super().__init__(
            request_id=request_id,
            status=status,
            prompt_len=len(input_ids) if input_ids else 0,
            input_ids=input_ids,
            sampling_params=sampling_params,
            lora_path=lora_path,
        )
        self.prompt = prompt

        if max_new_tokens < 1:
            raise ValueError("max_new_tokens must be at least 1.")
        self.max_new_tokens = max_new_tokens
        self.max_total_length = max_total_length
        self.output_ids = output_ids or []
        self.hidden_states = None

        if len(self.output_ids) > 0 and self.status == RequestStatus.PREFILLING:
            raise ValueError(f"Cannot initialize with output_ids given {self.status}.")

    @property
    def input_length(self) -> int:
        """Length of the input sequence (input_ids)."""
        if self.input_ids is None:
            raise ValueError("Cannot get input length before tokenization.")
        return len(self.input_ids)

    @property
    def output_length(self) -> int:
        """Length of the generated output (output_ids)."""
        return len(self.output_ids)

    @property
    def total_length(self) -> int:
        """Total length of the sequence (input + output)."""
        return self.prompt_len + self.output_length

    def get_model_input_for_first_peer(self) -> List[int]:
        """
        Returns the token IDs the First Peer's model should process for the current step.
        """
        if self.input_ids is None:
            raise ValueError("Cannot get model input before tokenization.")

        if not self.output_ids:
            return self.input_ids
        return [self.output_ids[-1]]

    def commit_new_token(self, token_id: int):
        """
        Called by the First Peer when a new token is received from the Last Peer.
        Appends the token, updates length, and checks finishing conditions.
        """
        if self.is_finished:
            logger.warning(
                f"Request {self.request_id}: Attempted to commit token to a finished request."
            )
            return

        self.output_ids.append(token_id)

        # Finishing condition checks are now handled by the Scheduler.
        if self.status == RequestStatus.PREFILLING:
            self.status = RequestStatus.DECODING

    @classmethod
    def from_prompt_ids(
        cls,
        prompt_ids: List[int],
        max_new_tokens: int,
        max_total_length: int,
    ) -> "InitialRequest":
        """
        Convert a prompt string to an InitialRequest.
        """
        return cls(
            input_ids=prompt_ids,
            max_new_tokens=max_new_tokens,
            max_total_length=max_total_length,
        )


class IntermediateRequest(Request):
    """
    Lightweight data packet sent between intermediate peers in the pipeline.
    This is what gets packed and sent over the network.

    # TODO: add attention_mask, logits...
    """

    def __init__(
        self,
        request_id: str,
        current_position: int,
        status: RequestStatus = RequestStatus.PREFILLING,
        input_ids: Optional[List[int]] = None,
        hidden_states: Optional[Any] = None,
        next_token_id: Optional[int] = None,
        routing_table: Optional[List[str]] = [],
        sampling_params: Optional[SamplingParams] = None,
        lora_path: Optional[str] = None,
    ):
        super().__init__(
            request_id=request_id,
            status=status,
            routing_table=routing_table,
            input_ids=input_ids,
            sampling_params=sampling_params,
            lora_path=lora_path,
        )
        # Hidden states from the previous peer's computation.
        # Shape:
        #   prefill: (prompt_len, hidden_dim)
        #   decode: (1, hidden_dim)
        # For data sent from Last Peer to First Peer, this can also be a single token_id
        # wrapped in a numpy array, e.g., np.array([token_id]).
        if not self.is_finished and hidden_states is None:
            raise ValueError(f"hidden_states cannot be None for unfinished request {request_id}.")

        self.current_position = current_position
        self.hidden_states = hidden_states
        self.next_token_id = next_token_id

    @property
    def input_length(self) -> int:
        """Length of the input sequence (hidden_states)."""
        assert self.is_prefill
        return self.current_position

    @property
    def total_length(self) -> int:
        """Total length of the sequence (input + output)."""
        return self.current_position

    @classmethod
    def from_initial_request(
        cls,
        initial_request: InitialRequest,
        hidden_states: Optional[Any] = None,
        lora_path: Optional[str] = None,
    ) -> "IntermediateRequest":
        """Convert an InitialRequest to an IntermediateRequest.

        Pack hidden states and set current position.
        This is typically used by the First Peer to start the pipeline.

        Args:
            initial_request: The initial request to convert.
            hidden_states: The hidden states from the previous peer.
                If None, it indicates that the request is finished.

        Returns:
            An IntermediateRequest instance with the request ID,
            status, current position, and hidden states from InitialRequest.
        """
        if hidden_states is None:
            assert initial_request.is_finished, "Hidden states can't be None for unfinished request"
        if initial_request.output_ids is None or len(initial_request.output_ids) == 0:
            next_token_id = None
        else:
            next_token_id = initial_request.output_ids[-1]

        return IntermediateRequest(
            request_id=initial_request.request_id,
            status=initial_request.status,
            input_ids=initial_request.input_ids,
            next_token_id=next_token_id,
            current_position=initial_request.total_length,
            hidden_states=hidden_states,
            sampling_params=initial_request.sampling_params,
            routing_table=initial_request.routing_table,
            lora_path=lora_path,
        )

    @classmethod
    def from_intermediate_request(
        cls,
        old_request: "IntermediateRequest",
        new_hidden_states: Any,
        lora_path: Optional[str] = None,
    ) -> "IntermediateRequest":
        """
        Creates a new IntermediateRequest from an old one, with updated hidden states.
        This is used by intermediate peers to pass the request along the pipeline.
        """
        return IntermediateRequest(
            request_id=old_request.request_id,
            status=old_request.status,
            current_position=old_request.total_length,
            input_ids=old_request.input_ids,
            next_token_id=old_request.next_token_id,
            hidden_states=new_hidden_states,
            routing_table=old_request.routing_table,
            sampling_params=old_request.sampling_params,
            lora_path=lora_path,
        )

    def __repr__(self):
        fields = [
            f"request_id={self.request_id}",
            f"status={self.status}",
            f"current_position={self.current_position}",
            f"input_ids={self.input_ids}",
            f"hidden_states={self.hidden_states}",
            f"routing_table={self.routing_table}",
        ]

        if self.hidden_states is not None:
            fields.append(f"hidden_states_shape={self.hidden_states.shape}")

        fields.append(f"next_token_id={self.next_token_id}")

        field_str = ",\n    ".join(fields)
        return f"IntermediateRequest(\n    {field_str}\n)"


================================================================================
File: src/parallax/server/sampling/sampler.py
Size: 4.06 kB
================================================================================

"""
Postprocesses logit_outputs to get tokens with different sampling methods
specified by requests.

Components:
    SamplingBatchInfo: Sampling info for a batch of requests
    Sampler: Module class for sampling.
TODO: Add penalizer support.
"""

import dataclasses
from functools import partial

import mlx.core as mx
from mlx import nn

from parallax.server.request import Request
from parallax.server.sampling.sampling_params import SamplingParams


@dataclasses.dataclass
class SamplingBatchInfo:
    """Maintains batched sampling information"""

    # Basic batched sampling params
    temperatures: mx.array
    top_ps: mx.array
    top_ks: mx.array
    min_ps: mx.array

    # Whether all requests use greedy sampling
    is_all_greedy: bool

    # Whether any request needs min_p sampling
    need_min_p_sampling: bool

    @classmethod
    def from_reqs(cls, reqs: list[Request]):
        """Retrieves sampling infos from a list of requests"""
        for r in reqs:
            if r.sampling_params is None:
                r.sampling_params = SamplingParams()

        is_all_greedy = all(r.sampling_params.top_k <= 1 for r in reqs)
        need_min_p_sampling = any(r.sampling_params.min_p > 0 for r in reqs)

        temperatures = mx.array(
            [r.sampling_params.temperature for r in reqs], dtype=mx.float32
        ).reshape(-1, 1)
        top_ps = mx.array([r.sampling_params.top_p for r in reqs], dtype=mx.float32)
        top_ks = mx.array([r.sampling_params.top_k for r in reqs], dtype=mx.int32)
        min_ps = mx.array([r.sampling_params.min_p for r in reqs], dtype=mx.float32)

        ret = cls(
            temperatures=temperatures,
            top_ps=top_ps,
            top_ks=top_ks,
            min_ps=min_ps,
            is_all_greedy=is_all_greedy,
            need_min_p_sampling=need_min_p_sampling,
        )
        return ret


class Sampler(nn.Module):
    """Sampler that completes Topk/Topp sampling for logits"""

    def __call__(self, logits: mx.array, sampling_info: SamplingBatchInfo):
        """Run a sampler & compute logprobs and update logits accordingly

        Args:
            logits: Logits from the model forward
            sampling_info: Metadata for sampling
        Returns:
            next_token_ids: next token IDs.
        """
        batch_next_token_ids = None
        if sampling_info.is_all_greedy:
            # Use argmax if all requests use greedy sampling
            batch_next_token_ids = mx.argmax(logits, axis=-1)
        else:
            logits = logits / sampling_info.temperatures.reshape(-1, 1)
            logits[:] = mx.softmax(logits, axis=-1)
            batch_next_token_ids = apply_top_k_top_p_min_p_sampling(
                logits,
                sampling_info.top_ks,
                sampling_info.top_ps,
                sampling_info.min_ps,
                sampling_info.need_min_p_sampling,
            )
        return batch_next_token_ids


@partial(mx.compile, inputs=mx.random.state, outputs=mx.random.state)
def apply_top_k_top_p_min_p_sampling(
    logits: mx.array,
    top_ks: mx.array,
    top_ps: mx.array,
    min_ps: mx.array,
    need_min_p_sampling: bool,
):
    """Mlx compiled kernel for calculating topk/topp/minp sampling"""
    probs_idx = mx.argsort(-logits, axis=-1)
    probs_sort = mx.take_along_axis(logits, probs_idx, axis=-1)
    probs_sum = mx.cumsum(probs_sort, axis=-1)
    top_k_mask = mx.arange(0, logits.shape[-1]).reshape(1, -1) < top_ks.reshape(-1, 1)
    probs_sort = probs_sort * top_k_mask
    top_p_mask = (probs_sum - probs_sort) <= top_ps.reshape(-1, 1)
    probs_sort = probs_sort * top_p_mask
    if need_min_p_sampling:
        min_p_thresholds = probs_sort[:, 0] * min_ps
        min_p_mask = probs_sort >= min_p_thresholds.reshape(-1, 1)
        probs_sort = probs_sort * min_p_mask

    probs_sort = mx.log(probs_sort)
    sampled_index = mx.random.categorical(probs_sort, num_samples=1)
    batch_next_token_ids = mx.take_along_axis(probs_idx, indices=sampled_index, axis=1)

    return batch_next_token_ids


================================================================================
File: src/parallax/server/sampling/sampling_params.py
Size: 2.41 kB
================================================================================

"""
Sampling parameters of each request
"""

from typing import List, Optional, Union


class SamplingParams:
    """Sampling parameter class for a single request"""

    def __init__(
        self,
        max_new_tokens: int = 128,
        min_new_tokens: int = 0,
        temperature: float = 1.0,
        top_p: float = 1.0,
        min_p: float = 0.0,
        top_k: int = -1,
        stop_token_ids: Optional[List[int]] = None,
        ignore_eos: bool = False,
        stop_strs: Optional[Union[str, List[str]]] = None,
        repetition_penalty: float = 1.0,
        presence_penalty: float = 0.0,
        frequency_penalty: float = 0.0,
        json_schema: Optional[str] = None,
    ) -> None:
        self.max_new_tokens = max_new_tokens
        self.min_new_tokens = min_new_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.min_p = min_p
        self.top_k = top_k
        if stop_token_ids:
            self.stop_token_ids = set(stop_token_ids)
        else:
            self.stop_token_ids = None
        self.ignore_eos = ignore_eos
        self.stop_strs = stop_strs
        self.repetition_penalty = repetition_penalty
        self.presence_penalty = presence_penalty
        self.frequency_penalty = frequency_penalty
        self.json_schema = json_schema

        # Some special cases
        if self.temperature == 0.0:
            # greedy sampling
            self.temperature = 1.0
            self.top_k = 1

    def verify(self):
        """Basic verifications for the sampling parameters"""
        if self.temperature < 0.0:
            raise ValueError(f"temperature must be non-negetive, got {self.temperature}.")
        if not 0.0 < self.top_p <= 1.0:
            raise ValueError(f"top_p must be in (0, 1], got {self.top_p}.")
        if not 0.0 < self.min_p <= 1.0:
            raise ValueError(f"min_p must be in (0, 1], got {self.min_p}.")
        if not -2.0 <= self.frequency_penalty <= 2.0:
            raise ValueError(f"frequency_penalty must be in [-2, 2], got {self.frequency_penalty}.")
        if not -2.0 <= self.presence_penalty <= 2.0:
            raise ValueError(f"presence_penalty must be in [-2, 2], got {self.presence_penalty}.")
        if not 0.0 <= self.repetition_penalty <= 2.0:
            raise ValueError(
                f"repetition_penalty must be in [0, 2], got {self.repetition_penalty}."
            )


================================================================================
File: src/parallax/server/scheduler.py
Size: 13.98 kB
================================================================================

"""
Continuous Batching Scheduler.

State managed by the scheduler:
    1. Prefill Wait Queue (FIFO): incoming prefill requests waiting for admission;
    2. Running Requests: inflight requests with KV-cache residency;
Main `form_batch` function will return the concrete batch chosen for the next model forward.

We use an explicit 2-Phase approach:
    * Phase 1 (Admission): wait queue -> running requests
        Implemented by `admit_requests`. We admit requests when capacity
        allows (e.g., max concurrent requests, memory availability). Admitted
        requests get KV-cache residency and become inflight.
    * Phase 2 (Batching): running requests -> active batch for actual forward
        Implemented by `form_batch`. We prioritize PREFILL requests
        first within `max_num_tokens_per_batch` and `micro_batch_size`,
        then include DECODE requests that are marked ready for the next decode step.

Our scheduler also handles tokenization and pre-processing for the First Peer's requests.
"""

import time
from collections import OrderedDict
from typing import Dict, List, Optional

from parallax.server.kv_cache import KVCacheManager
from parallax.server.request import InitialRequest, Request, RequestStatus
from parallax.utils.shared_state import SharedState
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


class Scheduler:
    """
    2-Phase approach:
        * Phase 1: wait queue -> running requests (all inflight requests)
        * Phase 2: running requests -> active batch (actual model forward)
    """

    def __init__(
        self,
        max_batch_size: int = 16,
        max_num_tokens_per_batch: int = 4096,
        scheduler_wait_ms: int = 200,
        micro_batch_ratio: int = 2,
        is_first_peer: bool = False,
        kv_cache_manager: Optional[KVCacheManager] = None,
        request_timeout_s: Optional[int] = 600,
        shared_state: Optional[SharedState] = None,
        **kwargs,
    ):
        """
        Args:
            max_batch_size: Maximum number of running / inflight requests;
            max_num_tokens_per_batch: Maxmimum number of prefill + decode tokens in a single batch;
            scheduler_wait_ms: The minimum time to wait before dispatching a batch;
            micro_batch_ratio: micro_batch_size = max_batch_size // micro_batch_ratio;
            tokenizer: The tokenizer to use for the model;
            kv_cache_manager: The KV cache manager to use for the scheduler.
            request_timeout_s: timeout for each inflight request (default 10mins).
        """
        self.max_batch_size = max_batch_size
        self.max_num_tokens_per_batch = max_num_tokens_per_batch
        self.micro_batch_size = max(1, max_batch_size // micro_batch_ratio)
        self.scheduler_wait_ms = scheduler_wait_ms
        self.is_first_peer = is_first_peer
        if is_first_peer:
            # Load configs for building InitialRequest
            self.tokenizer = kwargs.get("tokenizer", None)
            self.eos_token_id = kwargs.get("eos_token_id", None)
            self.max_new_tokens = kwargs.get("max_new_tokens", 512)
            self.max_total_length = kwargs.get("max_total_length", 1024)

        # Prefill wait queue (FIFO) for admission
        self._wait_queue: List[Request] = []
        # Keeps track of all in-flight requests
        self._running_requests: Dict[str, Request] = OrderedDict()

        self.kv_cache_manager = kv_cache_manager
        self.shared_state = shared_state
        # Default timeout for requests if not set on request object
        self.request_timeout_s = request_timeout_s

        self._last_dispatch_ts = time.time()
        # Track last reported running requests to avoid redundant metric updates
        self._last_reported_running_requests: int = 0
        logger.debug(
            f"Scheduler initialized: max_batch_size={self.max_batch_size}, "
            f"max_num_tokens_per_batch={self.max_num_tokens_per_batch}"
        )

    @property
    def num_queued_requests(self) -> int:
        """Get the number of requests in the scheduler."""
        return len(self._wait_queue)

    @property
    def num_running_requests(self) -> int:
        """Get the number of requests currently being processed."""
        return len(self._running_requests)

    def get_running_request(self, request_id: str) -> Optional[Request]:
        """Gets a request that is currently in the running state."""
        return self._running_requests.get(request_id)

    def _prompt_string_to_request(self, request_str: str) -> InitialRequest:
        """Convert the prompt string to InitialRequest."""
        assert self.is_first_peer, "Only first peer can enqueue InitialRequest."
        input_ids = self.tokenizer.encode(request_str)
        return InitialRequest.from_prompt_ids(
            input_ids, self.eos_token_id, self.max_new_tokens, self.max_total_length
        )

    def enque_request(self, request: Request | str):
        """Enque a request to the scheduler's wait queue."""
        if isinstance(request, str):
            request = self._prompt_string_to_request(request)

        if request.is_finished:
            logger.warning(
                f"Request {request.request_id} is already "
                f"{request.status}. Not adding to the scheduler."
            )
            return

        request.ready_for_next_step = True
        request.last_updated_time = time.time()
        # TODO: Handle chunked prefill.
        if request.is_decoding:
            rid = request.request_id
            if rid not in self._running_requests:
                raise ValueError(
                    f"Decode request {rid} must already be admitted (in running requests)."
                )
            # Merge incoming decode readiness/state into the existing running request
            self._running_requests[rid] = request
            # Update recency ordering so earlier-ready decodes are encountered first during batching
            self._running_requests.move_to_end(rid)
            logger.debug(f"Decode request {rid} marked ready for next decode.")
            return

        self._wait_queue.append(request)
        logger.debug(
            f"Prefill request {request.request_id} added to the prefill wait queue (size={len(self._wait_queue)})."
        )

    def evict_request(self, request_id: str):
        """Removes a request from the scheduler's running queue."""
        if request_id in self._running_requests:
            self._running_requests.pop(request_id)
            logger.debug(f"Evicted request {request_id} from scheduler.")
            # Update metrics only if running count changed since last report
            try:
                if self.shared_state is not None:
                    curr = self.num_running_requests
                    self.shared_state.update_metrics(current_requests=curr)
            except Exception:
                pass
        else:
            raise ValueError(f"Attempted to evict non-existent request {request_id}.")

    def cancel_request(self, request_id: str):
        """Cancels a request from the scheduler."""
        if request_id in self._running_requests:
            req = self._running_requests[request_id]
            req.abort = True
            logger.debug(f"Cancelled request {request_id} from scheduler.")
        else:
            raise ValueError(f"Attempted to cancel non-existent request {request_id}.")

    def check_and_update_request_status(self, request: InitialRequest) -> bool:
        """Checks if a request has met any finishing conditions and updates its status."""
        assert self.is_first_peer, "Only first peer can check and update request status."
        assert (
            self.eos_token_id is not None
        ), "EOS token ID must be set for request status checking."
        if request.is_finished:
            return True

        finished = False
        last_token_id = request.output_ids[-1] if request.output_ids else None
        if request.abort:
            finished = True
        if not request.sampling_params.ignore_eos and (
            self.eos_token_id
            and (
                last_token_id == self.eos_token_id
                or (isinstance(self.eos_token_id, list) and last_token_id in self.eos_token_id)
            )
        ):
            request.update_status(RequestStatus.FINISHED_EOS)
            finished = True
        elif not request.sampling_params.ignore_eos and (
            self.tokenizer
            and self.tokenizer.eos_token_id
            and last_token_id == self.tokenizer.eos_token_id
        ):
            request.update_status(RequestStatus.FINISHED_EOS)
            finished = True
        elif request.output_length >= request.max_new_tokens:
            request.update_status(RequestStatus.FINISHED_MAX_LENGTH)
            finished = True
        elif request.total_length >= request.max_total_length:
            request.update_status(RequestStatus.FINISHED_MAX_LENGTH)
            finished = True

        if finished:
            logger.debug(f"Request {request.request_id} finished with status {request.status}.")
            # Remove from running requests. The executor will handle KV cache release.
            self.evict_request(request.request_id)

        return finished

    def admit_requests(self):
        """Move requests from wait queue into running (inflight) set, up to capacity.

        Pushes admitted requests directly into the running set.
        """
        # TODO: pop directly from wait queue ?
        while self._wait_queue and len(self._running_requests) < self.max_batch_size:
            req = self._wait_queue.pop(0)
            rid = req.request_id
            if rid in self._running_requests:
                continue

            # Check kv cache pool
            if self.kv_cache_manager is not None:
                if not self.kv_cache_manager.has_request(req.request_id):
                    # TODO: Handle chunked prefill, and support preemption.
                    if not self.kv_cache_manager.allocate_request(req.request_id, req.total_length):
                        logger.warning(
                            f"Request {rid} can't be admit to running batch due to KV cache size."
                        )
                        continue

            # Add request to running requests
            self._running_requests[rid] = req
            # Initialize timing for timeout enforcement
            req.last_updated_time = time.time()
            logger.debug(
                f"Admitted to running: rid={rid}, status={req.status}, running_size={len(self._running_requests)}, ready={req.ready_for_next_step}"
            )

        # Reflect current running requests metric after admission
        try:
            if self.shared_state is not None:
                curr = self.num_running_requests
                if curr != self._last_reported_running_requests:
                    self.shared_state.update_metrics(current_requests=curr)
                    self._last_reported_running_requests = curr
        except Exception:
            pass

        return

    def get_timed_out_requests(self) -> List[Request]:
        """Return running requests that exceeded their timeout and mark them aborted.

        This does not evict or release resources; callers must handle cleanup.
        """
        timed_out: List[Request] = []
        now = time.time()
        for req in list(self._running_requests.values()):
            try:
                if req.last_updated_time is None:
                    raise ValueError("Requests should have last updated time set.")
                if now - req.last_updated_time > self.request_timeout_s:
                    req.abort = True
                    timed_out.append(req)
            except Exception:
                continue
        return timed_out

    def form_batch(self) -> List[Request]:
        """Form the active batch for the next forward pass.

        - Select prefills first (FIFO by admission), then decodes that are ready
          following the OrderedDict iteration order where ready decodes are
          moved-to-end upon readiness, while respecting micro_batch_size and
          max_num_tokens_per_batch.
        """
        self.admit_requests()
        if not self._running_requests:
            return []

        inflight_tokens = 0
        batch: List[Request] = []

        # Prefill candidates: preserve admission order via OrderedDict iteration
        prefill_candidates = []
        decode_candidates = []
        for req in self._running_requests.values():
            if req.ready_for_next_step:
                if req.is_prefill:
                    prefill_candidates.append(req)
                elif req.is_decoding:
                    decode_candidates.append(req)

        # 1) Fill with prefills first
        for req in prefill_candidates:
            if len(batch) >= self.micro_batch_size:
                break
            cost = req.prompt_len
            if cost + inflight_tokens > self.max_num_tokens_per_batch:
                continue
            batch.append(req)
            inflight_tokens += cost

        # 2) Fill remaining with ready decodes
        for req in decode_candidates:
            if len(batch) >= self.micro_batch_size:
                break
            cost = 1
            if cost + inflight_tokens > self.max_num_tokens_per_batch:
                continue
            batch.append(req)
            inflight_tokens += cost

        # Clear ready flags for decodes included in this batch
        for r in batch:
            r.ready_for_next_step = False
            r.last_updated_time = time.time()

        if batch:
            logger.debug(
                "Form batch selected=%s inflight_tokens=%d",
                [f"{r.request_id}:{r.status}, ready:{r.ready_for_next_step}" for r in batch],
                inflight_tokens,
            )
        return batch


================================================================================
File: src/parallax/server/server_args.py
Size: 10.47 kB
================================================================================

"""
CLI argument parser for Parallax server.

This module provides argument parsing functionality for the Parallax executor,
supporting various configuration options for model loading, layer sharding,
and performance tuning.
"""

import argparse

from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


def parse_args() -> argparse.Namespace:
    """
    Parse command line arguments for the Parallax executor.

    Returns:
        Parsed arguments namespace
    """
    parser = argparse.ArgumentParser(
        description="Parallax Executor - Distributed LLM Inference",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # HTTP server configuration
    parser.add_argument("--host", type=str, default="localhost", help="Host of the HTTP server.")
    parser.add_argument("--port", type=int, default=3000, help="Port of the HTTP server")
    parser.add_argument(
        "--node-chat-port", type=int, default=3002, help="Port of the node chat HTTP server"
    )

    # Lattica configuration
    parser.add_argument("--initial-peers", nargs="+", default=[], help="List of initial DHT peers")
    parser.add_argument("--scheduler-addr", type=str, default=None, help="Scheduler address")
    parser.add_argument("--relay-servers", nargs="+", default=[], help="List of relay DHT peers")
    parser.add_argument("--tcp-port", type=int, default=0, help="Port for Lattica TCP listening")
    parser.add_argument("--udp-port", type=int, default=0, help="Port for Lattica UDP listening")
    parser.add_argument(
        "--announce-maddrs", nargs="+", default=[], help="List of multiaddresses to announce"
    )
    parser.add_argument("--dht-prefix", type=str, default="gradient", help="Prefix for DHT keys")
    parser.add_argument(
        "--notify-url", type=str, default=None, help="URL to notify when a request is finished"
    )

    # Model configuration
    parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="Path to the model repository or model name (e.g., 'mlx-community/Qwen3-0.6B-bf16')",
    )
    parser.add_argument(
        "--max-sequence-length",
        type=int,
        default=None,
        help="Maximum sequence length for the model",
    )

    parser.add_argument(
        "--param-mem-ratio",
        type=float,
        default=0.65,
        help="Ratio of GPU memory to use for parameter hosting",
    )

    parser.add_argument(
        "--kvcache-mem-ratio",
        type=float,
        default=0.25,
        help="Ratio of GPU memory to use for KV cache",
    )

    parser.add_argument(
        "--start-layer",
        type=int,
        default=None,
        help="Starting layer index for this shard (inclusive)",
    )

    parser.add_argument(
        "--end-layer", type=int, default=None, help="Ending layer index for this shard (exclusive)"
    )

    parser.add_argument(
        "--dtype",
        type=str,
        default="bfloat16",
        choices=["float16", "bfloat16", "float32"],
        help="Data type for model weights and computations",
    )

    # KV Cache configuration
    parser.add_argument(
        "--kv-cache-memory-fraction",
        type=float,
        default=0.8,
        help="Fraction of available memory to use for KV cache (0.0 to 1.0)",
    )

    parser.add_argument(
        "--kv-block-size", type=int, default=64, help="Block size for KV cache management"
    )

    parser.add_argument(
        "--enable-prefix-cache", action="store_true", help="Enable prefix cache reuse"
    )

    # Scheduler configuration
    parser.add_argument(
        "--max-batch-size",
        type=int,
        default=8,
        help="Maximum batch size for processing requests",
    )

    parser.add_argument(
        "--max-num-tokens-per-batch",
        type=int,
        default=1024,
        help="Maximum number of tokens in a batch",
    )

    parser.add_argument(
        "--prefill-priority",
        type=int,
        default=0,
        choices=[0, 1],
        help="Priority for prefill requests (0 or 1)",
    )

    parser.add_argument(
        "--micro-batch-ratio", type=int, default=2, help="Micro batch ratio for scheduling"
    )

    parser.add_argument(
        "--scheduler-wait-ms", type=int, default=500, help="Scheduler wait time in milliseconds"
    )

    parser.add_argument(
        "--request-timeout-s",
        type=int,
        default=600,
        help="Per-request timeout in seconds before automatic abort",
    )

    # GPU/SGLang specialized configuration
    parser.add_argument(
        "--attention-backend",
        type=str,
        default="flashinfer",
        choices=["torch_native", "flashinfer", "triton", "fa3"],
        help="Choose the GPU attention kernels",
    )

    parser.add_argument(
        "--moe-runner-backend",
        type=str,
        default="auto",
        choices=[
            "auto",
            "triton",
            "triton_kernel",
            "flashinfer_trtllm",
            "flashinfer_cutlass",
            "flashinfer_mxfp4",
        ],
        help="Choose the GPU moe kernels",
    )

    parser.add_argument(
        "--enable-lora", action="store_true", help="Enable LoRA adapter support for SGLang backend"
    )

    parser.add_argument(
        "--max-lora-rank",
        type=int,
        default=None,
        help="The maximum rank of LoRA adapters. If not specified, it will be automatically inferred from the adapters provided in --lora-paths.",
    )

    parser.add_argument(
        "--lora-target-modules",
        nargs="*",
        type=str,
        default=None,
        help="The union set of all target modules where LoRA should be applied. If not specified, it will be automatically inferred from the adapters provided in --lora-paths. If 'all' is specified, all supported modules will be targeted.",
    )

    parser.add_argument(
        "--lora-paths",
        nargs="*",
        type=str,
        default=None,
        help="The list of LoRA adapters to load. Each adapter must be specified in one of the following formats: <PATH> | <NAME>=<PATH> | JSON with schema {'lora_name':str,'lora_path':str,'pinned':bool}.",
    )

    parser.add_argument(
        "--max-loras-per-batch",
        type=int,
        default=8,
        help="Maximum number of adapters for a running batch, include base-only request.",
    )

    parser.add_argument(
        "--max-loaded-loras",
        type=int,
        default=None,
        help="If specified, it limits the maximum number of LoRA adapters loaded in CPU memory at a time. The value must be greater than or equal to --max-loras-per-batch.",
    )

    parser.add_argument(
        "--lora-eviction-policy",
        choices=["lru", "fifo"],
        default="lru",
        help="LoRA adapter eviction policy when memory pool is full. 'lru': Least Recently Used (default, better cache efficiency). 'fifo': First-In-First-Out.",
    )

    parser.add_argument(
        "--lora-backend",
        choices=["triton", "csgmv"],
        default="triton",
        help="Choose the kernel backend for multi-LoRA serving.",
    )

    parser.add_argument(
        "--max-lora-chunk-size",
        choices=[16, 32, 64, 128],
        type=int,
        default=16,
        help="Maximum chunk size for the ChunkedSGMV LoRA backend. Only used when --lora-backend is 'csgmv'. Choosing a larger value might improve performance.",
    )

    # Tensor parallel configuration
    parser.add_argument("--tp-size", type=int, default=1, help="Tensor parallel size")

    parser.add_argument(
        "--nccl-port",
        type=int,
        default=None,
        help="The port for NCCL distributed environment setup",
    )

    # Logging and debugging
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Logging level",
    )

    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    parser.add_argument(
        "--gpu-backend",
        type=str,
        default="sglang",
        choices=["sglang", "vllm"],
        help="GPU backend to use",
    )

    parser.add_argument(
        "--use-hfcache",
        action="store_true",
        default=False,
        help="Use local Hugging Face cache only (no network download)",
    )

    args = parser.parse_args()

    # Validate arguments
    validate_args(args)

    return args


def validate_args(args: argparse.Namespace) -> None:
    """
    Validate parsed arguments.

    Args:
        args: Parsed arguments namespace

    Raises:
        ValueError: If arguments are invalid
    """
    # Validate layer indices
    if args.start_layer is not None and args.start_layer < 0:
        raise ValueError("start_layer must be non-negative")

    if args.end_layer is not None and args.end_layer <= args.start_layer:
        raise ValueError("end_layer must be greater than start_layer")

    # Validate memory fraction
    if not 0.0 <= args.kv_cache_memory_fraction <= 1.0:
        raise ValueError("kv_cache_memory_fraction must be between 0.0 and 1.0")

    # Validate batch sizes
    if getattr(args, "max_batch_size", None) is not None and args.max_batch_size <= 0:
        raise ValueError("max_batch_size must be positive")

    max_seq_len = getattr(args, "max_sequence_length", None)
    if max_seq_len is not None and max_seq_len <= 0:
        raise ValueError("max_sequence_len must be positive")

    if max_seq_len is None and getattr(args, "max_batch_size", None) is None:
        raise ValueError("max_sequence_len or max_batch_size must be provided")

    if args.max_num_tokens_per_batch <= 0:
        raise ValueError("max_num_tokens_per_batch must be positive")

    if args.kv_block_size <= 0:
        raise ValueError("kv_block_size must be positive")

    if args.micro_batch_ratio <= 0:
        raise ValueError("micro_batch_ratio must be positive")

    if args.scheduler_wait_ms < 0:
        raise ValueError("scheduler_wait_ms must be non-negative")

    if getattr(args, "request_timeout_s", None) is not None and args.request_timeout_s <= 0:
        raise ValueError("request_timeout_s must be positive")

    # Validate supported dtypes
    dtype_list = [
        "float16",
        "bfloat16",
        "float32",
    ]
    if args.dtype not in dtype_list:
        raise ValueError(f"Unsupported dtype: {args.dtype}. Supported dtypes: {dtype_list}")


================================================================================
File: src/parallax/server/server_info.py
Size: 9.3 kB
================================================================================

"""
ServerInfo that will be announce to DHT and used for client's routing.
    HardwareInfo: Detects and summarizes hardware information, RAM and FLOPs
We haven't used other info, will wait until DHT implemented.
"""

import platform
import subprocess
from dataclasses import asdict, dataclass
from typing import Any, ClassVar, Dict, Optional

import mlx.core as mx
from mlx import nn
from mlx.utils import tree_reduce
from mlx_lm.tuner.utils import get_total_parameters

try:
    import torch
except Exception:  # pragma: no cover - torch may be unavailable in some envs
    torch = None

try:
    import psutil
except ImportError:
    psutil = None


@dataclass
class HardwareInfo:
    """Generic hardware summary for a peer."""

    total_ram_gb: float
    chip: str
    tflops_fp16: float
    num_gpus: int

    def dumps(self) -> Dict[str, Any]:
        """Serializes the HardwareInfo object to a dictionary."""
        return asdict(self)

    @classmethod
    def loads(cls, obj: Dict[str, Any]) -> "HardwareInfo":
        """Deserializes a dictionary into a HardwareInfo object."""
        return cls(**obj)

    @staticmethod
    def detect() -> "HardwareInfo":
        """Dispatch to the correct subclass for the current machine.

        Prefers CUDA when available; falls back to Apple silicon on macOS.
        """
        if torch is not None and torch.cuda.is_available():
            return NvidiaHardwareInfo.detect()
        if platform.system() == "Darwin" and platform.machine().startswith("arm"):
            return AppleSiliconHardwareInfo.detect()
        raise NotImplementedError("Unsupported hardware; add a subclass.")


@dataclass
class AppleSiliconHardwareInfo(HardwareInfo):
    """HardwareInfo specialised for Apple silicon (M-series)."""

    # From cpu-monkey.com
    _APPLE_PEAK_FP16: ClassVar[Dict[str, float]] = {
        "M1": 4.58,
        "M1 Pro": 10.6,
        "M1 Max": 21.2,
        "M2": 7.1,
        "M2 Pro": 11.36,
        "M2 Max": 26.98,
        "M2 Ultra": 53.96,
        "M3": 7.1,
        "M3 Pro": 9.94,
        "M3 Max": 28.4,
        "M4": 8.52,
        "M4 Pro": 17.04,
        "M4 Max": 34.08,
    }

    @classmethod
    def detect(cls) -> "AppleSiliconHardwareInfo":
        if psutil:
            total_gb = psutil.virtual_memory().total / 2**30
        else:
            total_gb = int(subprocess.check_output(["sysctl", "-n", "hw.memsize"])) / 2**30

        chip = subprocess.check_output(
            ["sysctl", "-n", "machdep.cpu.brand_string"], text=True
        ).strip()

        short_name = chip.rsplit("Apple ", maxsplit=1)[-1]
        # For github action, we need to remove the "(Virtual)" suffix
        if short_name.endswith(" (Virtual)"):
            short_name = short_name.rsplit(" (Virtual)", maxsplit=1)[0]
        try:
            flops = cls._APPLE_PEAK_FP16[short_name]
        except KeyError as e:
            raise RuntimeError(
                f"Unknown Apple silicon chip '{short_name}' detected. "
                "Please add it to the _APPLE_PEAK_FP16 dictionary."
            ) from e

        return cls(num_gpus=1, total_ram_gb=round(total_gb, 1), chip=chip, tflops_fp16=flops)


@dataclass
class NvidiaHardwareInfo(HardwareInfo):
    """HardwareInfo specialised for NVIDIA CUDA devices.

    Captures peak FP16 TFLOPS and memory bandwidth using a best-effort mapping
    from device name. VRAM is reported via CUDA device properties.
    """

    vram_gb: float = 0.0
    memory_bandwidth_gbps: float = 0.0

    # Best-effort device database; can be extended as needed
    _GPU_DB: ClassVar[Dict[str, Dict[str, float]]] = {
        # key: substring to match in CUDA device name (case-insensitive)
        "a100-80g": {"tflops_fp16": 312.0, "bandwidth_gbps": 2039.0},
        "a100 80": {"tflops_fp16": 312.0, "bandwidth_gbps": 2039.0},
        "a100-40g": {"tflops_fp16": 312.0, "bandwidth_gbps": 1935.0},
        "a100 40": {"tflops_fp16": 312.0, "bandwidth_gbps": 1935.0},
        "rtx 5090": {"tflops_fp16": 104.8, "bandwidth_gbps": 1792.0},
        "rtx 4090": {"tflops_fp16": 82.6, "bandwidth_gbps": 1008.0},
    }

    @classmethod
    def _match_gpu_specs(cls, name: str, vram_gb: float) -> Dict[str, float]:
        key = name.lower()
        # Specialize A100 by VRAM size when name is generic
        if "a100" in key and ("80" in key or vram_gb >= 60):
            return cls._GPU_DB.get("a100-80g", {"tflops_fp16": 312.0, "bandwidth_gbps": 2039.0})
        if "a100" in key and ("40" in key or vram_gb < 60):
            return cls._GPU_DB.get("a100-40g", {"tflops_fp16": 312.0, "bandwidth_gbps": 1935.0})
        for sub, spec in cls._GPU_DB.items():
            if sub in key:
                return spec
        # Conservative fallback when unknown
        return {"tflops_fp16": 50.0, "bandwidth_gbps": 600.0}

    @classmethod
    def detect(cls) -> "NvidiaHardwareInfo":
        if torch is None or not torch.cuda.is_available():
            raise RuntimeError("CUDA not available; cannot detect NVIDIA hardware")

        device_count = torch.cuda.device_count()
        device_index = torch.cuda.current_device()
        props = torch.cuda.get_device_properties(device_index)
        name = getattr(props, "name", f"cuda:{device_index}")
        total_vram_gb = round(props.total_memory / (1024**3), 1)

        # Host RAM (for completeness)
        if psutil:
            total_gb = psutil.virtual_memory().total / 2**30
        else:
            total_gb = 0.0

        spec = cls._match_gpu_specs(name, total_vram_gb)
        return cls(
            num_gpus=device_count,
            total_ram_gb=round(total_gb, 1),
            chip=name,
            tflops_fp16=float(spec["tflops_fp16"]),
            vram_gb=total_vram_gb,
            memory_bandwidth_gbps=float(spec["bandwidth_gbps"]),
        )


def detect_node_hardware(node_id: Optional[str]) -> Dict[str, Any]:
    """Detect local hardware and return a dict for scheduling.

    Returns a dictionary with keys compatible with `NodeHardwareInfo` builder:
    - node_id: The peer/node id
    - tflops_fp16: Peak FP16 TFLOPS
    - memory_gb: Device memory in GB (VRAM for CUDA, total RAM for Apple)
    - memory_bandwidth_gbps: Estimated memory bandwidth in GB/s
    """
    try:
        hw = HardwareInfo.detect()
    except NotImplementedError:
        # Fallback to a conservative default
        return {
            "node_id": node_id,
            "num_gpus": 1,
            "tflops_fp16": 50.0,
            "gpu_name": "Unknown",
            "memory_gb": 16.0,
            "memory_bandwidth_gbps": 100.0,
            "device": "Unknown",
        }

    if isinstance(hw, NvidiaHardwareInfo):
        return {
            "node_id": node_id,
            "num_gpus": hw.num_gpus,
            "tflops_fp16": hw.tflops_fp16,
            "gpu_name": hw.chip,
            "memory_gb": hw.vram_gb,
            "memory_bandwidth_gbps": hw.memory_bandwidth_gbps,
            "device": "cuda",
        }
    if isinstance(hw, AppleSiliconHardwareInfo):
        # Use unified memory size as memory_gb; bandwidth rough estimate per family
        est_bandwidth = 100.0
        return {
            "node_id": node_id,
            "num_gpus": hw.num_gpus,
            "tflops_fp16": hw.tflops_fp16,
            "gpu_name": hw.chip,
            "memory_gb": hw.total_ram_gb,
            "memory_bandwidth_gbps": est_bandwidth,
            "device": "mlx",
        }
    # Generic fallback
    return {
        "node_id": node_id,
        "num_gpus": hw.num_gpus,
        "tflops_fp16": hw.tflops_fp16,
        "gpu_name": "Unknown",
        "memory_gb": 16.0,
        "memory_bandwidth_gbps": 100.0,
        "device": "Unknown",
    }


@dataclass
class ShardedModelInfo:
    """
    Detailed information about the specific model shard hosted by a server.
    """

    model_name: str
    start_layer: int
    end_layer: int
    parameter_count: int = 0
    memory_consumption_mb: float = 0.0

    def dumps(self) -> Dict[str, Any]:
        """Serializes the HardwareInfo object to a dictionary."""
        data = asdict(self)
        return data

    @classmethod
    def loads(cls, data: Dict[str, Any]) -> "ShardedModelInfo":
        """Deserializes a dictionary into a HardwareInfo object."""
        return cls(**data)

    @classmethod
    def from_sharded_model(
        cls, sharded_model_instance: nn.Module  # Instance of your ShardedModel
    ) -> "ShardedModelInfo":
        """
        Constructs ShardedModelInfo from a loaded ShardedModel instance.
        Assumes sharded_model_instance has start_layer, end_layer, and model_id_original attributes.
        """
        # Calculate parameter count
        count = get_total_parameters(sharded_model_instance)

        model_bytes = tree_reduce(
            lambda acc, x: acc + x.nbytes if isinstance(x, mx.array) else acc,
            sharded_model_instance.parameters(),
            0,
        )
        memory_mb = round(model_bytes / (1024 * 1024), 2)

        return cls(
            model_name=sharded_model_instance.model_id,  # Use the cleaned name
            start_layer=sharded_model_instance.start_layer,
            end_layer=sharded_model_instance.end_layer,
            parameter_count=count,
            memory_consumption_mb=memory_mb,
        )


================================================================================
File: src/parallax/server/shard_loader.py
Size: 18.54 kB
================================================================================

"""
Loads sharded MLX models from Hugging Face Hub or local paths.
"""

import glob
import importlib
import json
import pathlib
import types
from typing import Any, Dict, Optional, Tuple

import mlx.core as mx
import safetensors
from huggingface_hub import snapshot_download
from mlx import nn
from mlx.utils import tree_unflatten
from mlx_lm.models.switch_layers import QuantizedSwitchLinear, SwitchLinear
from mlx_lm.tuner.dora import DoRAEmbedding, DoRALinear
from mlx_lm.tuner.lora import LoRAEmbedding, LoRALinear, LoRASwitchLinear
from mlx_lm.utils import get_model_path, load_config

from parallax.server.model import ShardedModel
from parallax.utils.tokenizer_utils import load_tokenizer
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)

MODEL_CLASS_MAP = {
    "kimi_k2": "mlx_lm.models.deepseek_v3",
    "minimax": "parallax.models.minimax",
}


class MLXModelLoader:
    """
    Handles downloading model assets from Hugging Face (if needed) and loading
    a specified shard of an MLX model.
    """

    def __init__(
        self,
        model_path_or_hf_repo: str,
        *,
        start_layer: Optional[int] = None,
        end_layer: Optional[int] = None,
        use_hfcache: bool = False,
    ):
        """
        Initializes the model loader.

        Args:
            model_path_or_hf_repo (str): The Hugging Face Hub model ID or a local path
                                         to the model directory.
            start_layer (Optional[int]): The starting layer index for the shard (inclusive).
                                         Defaults to the beginning of the model.
            end_layer (Optional[int]): The ending layer index for the shard (exclusive).
                                       Defaults to the end of the model.
            use_hfcache (bool): If True, use local Hugging Face cache only (no network download).
        """
        self.model_path_str = model_path_or_hf_repo
        self.start_layer = start_layer
        self.end_layer = end_layer
        self.use_hfcache = use_hfcache
        self.register_block_class()

    def register_block_class(self):
        """Automatically read all EntryClass from models directory and generate block class map."""
        self.block_class_map = {}

        # Get models directory path
        models_dir = pathlib.Path(__file__).parent.parent / "models"

        # Find all .py files in models directory (excluding __init__.py)
        model_files = [f for f in models_dir.glob("*.py") if f.name != "__init__.py"]

        for model_file in model_files:
            try:
                # Import the module
                module_name = f"parallax.models.{model_file.stem}"
                module = importlib.import_module(module_name)

                # Get EntryClass from the module
                if hasattr(module, "EntryClass"):
                    entry_class = getattr(module, "EntryClass")

                    # Get architecture from class attribute
                    if hasattr(entry_class, "get_architecture"):
                        architecture = entry_class.get_architecture()
                        self.block_class_map[architecture] = entry_class
                        # logger.info(f"Registered {architecture} -> {entry_class.__name__}")
                    else:
                        logger.warning(f"No architecture attribute found in {entry_class.__name__}")

            except Exception as e:
                logger.warning(f"Failed to load model from {model_file}: {e}")

    def linear_to_lora_layers(
        self,
        model: nn.Module,
        num_layers: int,
        config: Dict,
        use_dora: bool = False,
    ):
        """
        Convert some of the models linear layers to lora layers.

        Args:
            model (nn.Module): The neural network model.
            num_layers (int): The number of blocks to convert to lora layers
            starting from the last layer.
            config (dict): More configuration parameters for LoRA, including the
            rank, scale, and optional layer keys.
            use_dora (bool): If True, uses DoRA instead of LoRA.
            Default: ``False``
        """

        def to_lora(layer):
            if not use_dora and hasattr(layer, "to_lora"):
                return layer.to_lora(
                    r=config["rank"],
                    scale=config["scale"],
                    dropout=config["dropout"],
                )

            if isinstance(layer, (nn.Linear, nn.QuantizedLinear)):
                LoRALayer = DoRALinear if use_dora else LoRALinear
            elif isinstance(layer, (SwitchLinear, QuantizedSwitchLinear)):
                if use_dora:
                    raise ValueError(f"{type(layer).__name__} doesn't support DoRA yet.")
                LoRALayer = LoRASwitchLinear
            elif isinstance(layer, (nn.Embedding, nn.QuantizedEmbedding)):
                LoRALayer = DoRAEmbedding if use_dora else LoRAEmbedding
            else:
                raise ValueError(f"Can't convert layer of type {type(layer).__name__} to LoRA")

            return LoRALayer.from_base(
                layer,
                r=config["rank"],
                scale=config["scale"],
                dropout=config["dropout"],
            )

        if (keys := config.get("keys", None)) is None:
            keys = set()

            def get_keys_for_lora(p, m):
                types = (
                    nn.Linear,
                    nn.QuantizedLinear,
                    SwitchLinear,
                    QuantizedSwitchLinear,
                    nn.Embedding,
                    nn.QuantizedEmbedding,
                )
                if hasattr(m, "to_lora") or isinstance(m, types):
                    keys.add(p)

            for l in model.layers:
                l.apply_to_modules(get_keys_for_lora)

        for l in model.layers[-max(num_layers, 0) :]:
            lora_layers = [(k, to_lora(m)) for k, m in l.named_modules() if k in keys]
            if lora_layers:
                l.update_modules(tree_unflatten(lora_layers))

        lora_modules = [(k, to_lora(m)) for k, m in model.named_modules() if k in keys]
        if lora_modules:
            model.update_modules(tree_unflatten(lora_modules))

    def load_lora(self, base_model: nn.Module, adapter_path: str) -> nn.Module:
        """
        Loads LoRA weights from the specified path and applies them to the base model.

        Args:
            adapter_path (str): Path to the LoRA weights file (safetensors format).
            base_model (nn.Module): The base model to which LoRA weights will be applied.

        Returns:
            nn.Module: The base model with LoRA weights applied.
        """

        adapter_path = pathlib.Path(adapter_path)
        if not adapter_path.exists():
            try:
                logger.info(
                    f"Adapter path {adapter_path} not found locally. Attempting to download from Hugging Face..."
                )
                downloaded_path = snapshot_download(
                    repo_id=str(adapter_path), local_dir=str(adapter_path)
                )
                adapter_path = pathlib.Path(downloaded_path)
                logger.info(f"Downloaded adapter to {adapter_path}")
            except Exception as e:
                logger.error(f"Failed to download adapter from Hugging Face: {e}")
                raise FileNotFoundError(
                    f"The adapter path does not exist: {adapter_path}. Download failed: {e}"
                )
        with open(adapter_path / "adapter_config.json", "r") as fid:
            config = types.SimpleNamespace(**json.load(fid))
        fine_tune_type = getattr(config, "fine_tune_type", "lora")
        if fine_tune_type != "full":
            self.linear_to_lora_layers(
                base_model,
                config.num_layers,
                config.lora_parameters,
                use_dora=(fine_tune_type == "dora"),
            )
        base_model.load_weights(str(adapter_path / "adapters.safetensors"), strict=False)
        return base_model

    def load(
        self, lazy: bool = False, strict: bool = True, use_selective_download: bool = True
    ) -> Tuple[nn.Module, Dict[str, Any], Any]:
        """
        Loads the specified model shard by loading only the necessary weights
        from the safetensor files, saving significant memory.

        Args:
            lazy (bool): If False, evaluates model parameters to ensure they are loaded
                         into memory. Defaults to False.
            strict (bool): If True, raises an exception if weights do not match.
                           Defaults to True.
            use_selective_download (bool): If True, only download necessary weight files
                                          from Hugging Face. Defaults to True.
        Returns:
            A tuple containing the loaded sharded MLX model and its configuration dictionary.
        """
        if use_selective_download and self.start_layer is not None and self.end_layer is not None:
            from parallax.utils.selective_download import (
                get_model_path_with_selective_download,
            )

            logger.info(
                f"Using selective download for layers [{self.start_layer}, {self.end_layer})"
            )
            model_path = get_model_path_with_selective_download(
                self.model_path_str,
                start_layer=self.start_layer,
                end_layer=self.end_layer,
                local_files_only=self.use_hfcache,
            )
        else:
            model_path = get_model_path(self.model_path_str)[0]

        config = load_config(model_path)
        tokenizer = load_tokenizer(model_path, eos_token_ids=config.get("eos_token_id", None))

        architectures = config.get("architectures", None)
        if architectures is None:
            raise ValueError("architectures not found in config.json")
        if len(architectures) != 1:
            raise ValueError("only one architecture is supported")
        architecture = architectures[0]
        block_class = self.block_class_map.get(architecture, None)
        if block_class is None:
            raise ValueError(f"block_class not found for architecture: {architecture}")

        num_hidden_layers = config.get("num_hidden_layers", 0)
        current_start_layer = self.start_layer if self.start_layer is not None else 0
        current_end_layer = self.end_layer if self.end_layer is not None else num_hidden_layers

        # We need the model object to know its structure and which layers it owns.
        # This part mirrors the logic from the provided utils.py to get model_args.
        model_type = config.get("model_type")
        if not model_type:
            raise ValueError("model_type not found in config.json")

        if model_type in MODEL_CLASS_MAP:
            model_class = MODEL_CLASS_MAP[model_type]
        else:
            model_class = f"mlx_lm.models.{model_type}"

        try:
            arch_module = importlib.import_module(model_class)
            model_args_class = getattr(arch_module, "ModelArgs")
            model_args = model_args_class.from_dict(config)

        except (ImportError, AttributeError) as e:
            raise ValueError(f"Failed to load architecture for model_type '{model_type}'.") from e

        dtype = getattr(mx, config.get("torch_dtype", "bfloat16"))

        # Extract the base model name from model_id_original if it's a repo ID
        model_id = self.model_path_str
        if "/" in model_id:
            model_id = model_id.split("/")[-1]
        else:  # If it's already a clean name or a local path (take basename)
            model_id = pathlib.Path(model_id).name
        model_shard = ShardedModel(
            config=model_args,
            model_id=model_id,
            start_layer=current_start_layer,
            end_layer=current_end_layer,
            block_class=block_class,
            dtype=dtype,
        )

        weight_files = glob.glob(str(model_path / "model*.safetensors"))
        if not weight_files:
            weight_files = glob.glob(str(model_path / "weight*.safetensors"))

        # Sort weight files by name for consistent loading order
        weight_files = sorted(weight_files)

        # Use shared utility to filter weight files
        from parallax.utils.weight_filter_utils import (
            filter_weight_files_by_layer_range_for_load,
        )

        weight_files = filter_weight_files_by_layer_range_for_load(
            model_path=model_path,
            weight_files=weight_files,
            start_layer=current_start_layer,
            end_layer=current_end_layer,
            is_first_shard=model_shard.is_first_shard,
            is_last_shard=model_shard.is_last_shard,
            config=config,
        )

        if not weight_files and strict:
            raise FileNotFoundError(f"No safetensors found in {model_path}")

        # Instead of loading all weights, we iterate through files and keys,
        # loading only what we need.
        shard_weights = {}
        layer_key_prefix = "model.layers"  # Common prefix

        for file_idx, wf in enumerate(weight_files):
            logger.debug(
                f"Scanning weight file {file_idx + 1}/{len(weight_files)}: {pathlib.Path(wf).name}"
            )

            with safetensors.safe_open(wf, framework="pt") as f:
                for key in f.keys():
                    is_needed = False
                    remapped_key = None

                    # Check if the key belongs to the shard and remap it
                    if (
                        model_shard.is_first_shard
                        and "embed_tokens" in key
                        and key.startswith("model.")
                    ):
                        is_needed = True
                        remapped_key = key.replace("model.", "", 1)
                        if model_shard.is_last_shard and config.get("tie_word_embeddings", False):
                            # Also add lm_head mapping for tied embeddings
                            lm_head_key = remapped_key.replace("embed_tokens", "lm_head")
                            shard_weights[lm_head_key] = mx.array(f.get_tensor(key))
                    elif model_shard.is_last_shard:
                        if "model.norm" in key:
                            is_needed = True
                            remapped_key = key.replace("model.", "", 1)
                        if "lm_head" in key:
                            is_needed = True
                            remapped_key = key
                        elif (
                            config.get("tie_word_embeddings", False)
                            and "embed_tokens" in key
                            and key.startswith("model.embed_tokens")
                        ):
                            is_needed = True
                            remapped_key = key.replace("model.", "", 1).replace(
                                "embed_tokens", "lm_head"
                            )
                    if layer_key_prefix in key:
                        try:
                            parts = key.split(".")
                            layer_idx = int(parts[2])
                            if current_start_layer <= layer_idx < current_end_layer:
                                is_needed = True
                                local_layer_idx = layer_idx - current_start_layer
                                remapped_key = f"layers.{local_layer_idx}.{'.'.join(parts[3:])}"
                        except (ValueError, IndexError):
                            continue

                    # If the key is needed, load only that tensor from the file
                    if is_needed:
                        # Load tensor
                        tensor = f.get_tensor(key)
                        weight_array = mx.array(tensor)

                        # Only convert dtype for non-quantized weights
                        # Quantized weights (uint32, int32) and their scales/biases should keep their original dtype
                        # Scales are typically float32 and should not be downcast to bfloat16
                        is_quantized_param = weight_array.dtype in (mx.uint32, mx.int32, mx.uint8)
                        if not is_quantized_param:
                            weight_array = weight_array.astype(dtype)

                        shard_weights[remapped_key] = weight_array

        if (quantization := config.get("quantization", None)) is not None:
            logger.debug("Model is quantized. Applying quantization parameters...")

            def class_predicate(p, m):
                # Handle custom per-layer quantizations from the config
                qcfg = config.get("quantization", {})
                # Direct key (Parallax remapped keys usually drop the 'model.' prefix)
                if p in qcfg:
                    override = qcfg[p]
                    if isinstance(override, dict):
                        logger.debug(
                            f"[quantize] Using override for '{p}': bits={override.get('bits')} group_size={override.get('group_size')}"
                        )
                    return override
                # Allow config keys that still include the original 'model.' prefix (as in mlx-lm)
                prefixed = f"model.{p}"
                if prefixed in qcfg:
                    override = qcfg[prefixed]
                    return override
                if not hasattr(m, "to_quantized"):
                    return False
                # Handle legacy models by checking if quantized weights exist
                return f"{p}.scales" in shard_weights

            nn.quantize(
                model_shard,
                group_size=quantization["group_size"],
                bits=quantization["bits"],
                mode=quantization.get("mode", "affine"),
                class_predicate=class_predicate,
            )

        model_shard.load_weights(list(shard_weights.items()), strict=strict)

        if not lazy:
            mx.eval(model_shard.parameters())
        model_shard.eval()
        logger.info(
            "Successfully loaded model shard (layers [%d-%d)), memory usage: %.3f GB",
            current_start_layer,
            current_end_layer,
            mx.get_active_memory() / 1024**3,
        )
        return model_shard, config, tokenizer


================================================================================
File: src/parallax/sglang/batch_info.py
Size: 8.71 kB
================================================================================

"""
Store information about a SGLang batch.
The following is the flow of data structures for a batch in SGLang:

ScheduleBatch -> ModelWorkerBatch -> ForwardBatch
"""

from types import SimpleNamespace
from typing import List

import torch
from sglang.srt.managers.schedule_batch import Req, ScheduleBatch
from sglang.srt.model_executor.forward_batch_info import ForwardBatch
from sglang.srt.model_executor.model_runner import ModelRunner
from sglang.srt.sampling.sampling_batch_info import (
    SamplingBatchInfo as SGLSamplingBatchInfo,
)
from sglang.srt.sampling.sampling_params import SamplingParams as SGLSamplingParams
from sglang.srt.speculative.spec_info import SpeculativeAlgorithm

from parallax.server.request import Request
from parallax.server.sampling.sampling_params import (
    SamplingParams as ParallaxSamplingParams,
)
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


def transform_sampling_params_to_sglang(old_params: ParallaxSamplingParams) -> SGLSamplingParams:
    """Transforms Parallax SamplingParams to SGLang.SamplingParams format"""
    params = SGLSamplingParams(
        max_new_tokens=old_params.max_new_tokens,
        min_new_tokens=old_params.min_new_tokens,
        temperature=old_params.temperature,
        top_p=old_params.top_p,
        min_p=old_params.min_p,
        top_k=old_params.top_k,
        stop_token_ids=old_params.stop_token_ids,
        ignore_eos=old_params.ignore_eos,
        stop=old_params.stop_strs,
        repetition_penalty=old_params.repetition_penalty,
        presence_penalty=old_params.presence_penalty,
        json_schema=old_params.json_schema,
    )
    return params


def transform_requests_to_sglang(old_requests: List[Request]) -> List[Req]:
    """Transforms Parallax Request to SGLang.Req format"""
    reqs = []
    for old_req in old_requests:
        sampling_params = transform_sampling_params_to_sglang(old_req.sampling_params)
        req = Req(
            rid=old_req.request_id,
            origin_input_text="",
            origin_input_ids=old_req.input_ids,
            sampling_params=sampling_params,
            lora_id=old_req.lora_id,
        )
        req.init_next_round_input()
        reqs.append(req)
    return reqs


def form_sgl_batch_prefill(
    requests: List[Request],
    model_runner: ModelRunner,
) -> ForwardBatch:
    """Initialize a prefill ScheduleBatch -> ModelWorkerBatch -> ForwardBatch workflow"""

    sgl_reqs = transform_requests_to_sglang(requests)

    def dummy_evict(*args):
        pass

    dummy_tree_cache = SimpleNamespace(
        page_size=model_runner.server_args.page_size,
        device=model_runner.device,
        token_to_kv_pool_allocator=model_runner.token_to_kv_pool_allocator,
        evictable_size=0,
    )
    dummy_tree_cache.evict = dummy_evict
    schedule_batch = ScheduleBatch.init_new(
        reqs=sgl_reqs,
        req_to_token_pool=model_runner.req_to_token_pool,
        token_to_kv_pool_allocator=model_runner.token_to_kv_pool_allocator,
        tree_cache=dummy_tree_cache,
        model_config=model_runner.model_config,
        enable_overlap=False,
        spec_algorithm=SpeculativeAlgorithm.NONE,
    )
    schedule_batch.prepare_for_extend()
    model_worker_batch = schedule_batch.get_model_worker_batch()
    forward_batch = ForwardBatch.init_new(model_worker_batch, model_runner)
    return schedule_batch, forward_batch


def select_batch(
    origin_batch: ScheduleBatch,
    keep_indices: List[int],
) -> ScheduleBatch:
    """
    Copy a subset of requests to form a new ScheduleBatch from the running ScheduleBatch.
    Since the requests are not necessary selected in the loop, we need to copy by indicies to select
    the real requests to run.
    """
    ret = origin_batch.copy()
    if keep_indices is None or len(keep_indices) == 0:
        return None

    keep_indices_device = torch.tensor(keep_indices, dtype=torch.int64).to(
        origin_batch.device, non_blocking=True
    )

    ret.token_to_kv_pool_allocator = origin_batch.token_to_kv_pool_allocator
    ret.req_to_token_pool = origin_batch.req_to_token_pool
    ret.tree_cache = origin_batch.tree_cache

    if origin_batch.model_config.is_encoder_decoder:
        ret.encoder_lens = origin_batch.encoder_lens[keep_indices_device]
        ret.encoder_lens_cpu = [origin_batch.encoder_lens_cpu[i] for i in keep_indices]

    ret.reqs = [origin_batch.reqs[i] for i in keep_indices]
    if origin_batch.multimodal_inputs is not None:
        ret.multimodal_inputs = [origin_batch.multimodal_inputs[i] for i in keep_indices]
    ret.seq_lens_cpu = origin_batch.seq_lens_cpu[keep_indices]
    ret.req_pool_indices = origin_batch.req_pool_indices[keep_indices_device]
    ret.seq_lens = origin_batch.seq_lens[keep_indices_device]
    ret.orig_seq_lens = origin_batch.orig_seq_lens[keep_indices_device]

    if origin_batch.out_cache_loc is not None:
        ret.out_cache_loc = origin_batch.out_cache_loc[keep_indices_device]
    ret.seq_lens_sum = ret.seq_lens.sum().item()

    if origin_batch.output_ids is not None:
        ret.output_ids = origin_batch.output_ids[keep_indices_device]

    ret.return_logprob = any(req.return_logprob for req in origin_batch.reqs)
    if ret.return_logprob:
        ret.top_logprobs_nums = [origin_batch.top_logprobs_nums[i] for i in keep_indices]
        ret.token_ids_logprobs = [origin_batch.token_ids_logprobs[i] for i in keep_indices]
    else:
        ret.top_logprobs_nums = None
        ret.token_ids_logprobs = None

    ret.has_stream = any(req.stream for req in origin_batch.reqs)
    ret.has_grammar = any(req.grammar for req in origin_batch.reqs)

    ret.sampling_info = SGLSamplingBatchInfo.from_schedule_batch(
        ret, origin_batch.model_config.vocab_size
    )

    return ret


def find_index(running_batch: ScheduleBatch, request_id: str):
    """Helper function for finding the requests in the running batch by request_id"""
    for index, req in enumerate(running_batch.reqs):
        if req.rid == request_id:
            return index
    logger.exception(
        f"Request {request_id} not found in running batch, size: {len(running_batch.reqs)}, \
        reqs: {[request.rid for request in running_batch.reqs]}"
    )
    return -1


def form_sgl_batch_decode(
    requests: List[Request],
    model_runner: ModelRunner,
    running_batch: ScheduleBatch,
    is_first_rank: bool,
) -> ForwardBatch:
    """
    Forms the decoding batch in this round.
    The returned ScheduleBatch is a copy of subset of the running batch.
    ModelWorkerBatch -> ForwardBatch are generated from the selected ScheduleBatch.
    """
    ready_indices = list(
        filter(lambda x: x != -1, [find_index(running_batch, req.request_id) for req in requests])
    )
    ret = select_batch(running_batch, ready_indices)
    if is_first_rank:
        output_ids = []
        for request in requests:
            output_ids.append(request.output_ids[-1])
        ret.output_ids = torch.tensor(output_ids, dtype=torch.int64).to(
            ret.device, non_blocking=True
        )
    else:
        # Set an empty output_ids tensor
        batch_size = len(ready_indices)
        ret.output_ids = torch.empty(batch_size, dtype=torch.int64).to(
            ret.device, non_blocking=True
        )
    ret.prepare_for_decode()
    # TODO: this is a hack to make the seq_lens correct due to select_batch is not refference running batch's seq_lens
    # need to fix this
    running_batch.seq_lens[ready_indices] += 1
    running_batch.seq_lens_cpu[ready_indices] += 1
    running_batch.orig_seq_lens[ready_indices] += 1

    model_worker_batch = ret.get_model_worker_batch()
    if requests[0].lora_id is not None:
        model_worker_batch.lora_ids = [req.lora_id or "" for req in requests]
    forward_batch = ForwardBatch.init_new(model_worker_batch, model_runner)

    return forward_batch


def release_sglang_request(running_batch: ScheduleBatch, request_id: str):
    """Release KV Cache and other resources for finished/aborted requests."""
    if running_batch is None or running_batch.is_empty():
        return
    seq_lens_cpu = running_batch.seq_lens.cpu().numpy()
    idx = find_index(running_batch, request_id)
    req = running_batch.reqs.pop(idx)

    # Free kv cache
    page_size = running_batch.token_to_kv_pool_allocator.page_size
    last_uncached_pos = (len(req.prefix_indices) // page_size) * page_size
    token_indices = running_batch.req_to_token_pool.req_to_token[
        req.req_pool_idx, last_uncached_pos : seq_lens_cpu[idx]
    ]
    running_batch.token_to_kv_pool_allocator.free(token_indices)
    running_batch.req_to_token_pool.free(req.req_pool_idx)


================================================================================
File: src/parallax/sglang/model_runner.py
Size: 12.98 kB
================================================================================

"""
Imports sglang ModelRunner related modules and wrap them into create functions.
We use monkey patch to modify sglang originated methods. The main purpose is to pass
arguments needed by decentralized inference.
"""

import logging
import os
import random
from typing import List, Optional

import sglang
import sglang.srt.distributed.parallel_state
import torch
from mlx_lm.utils import load_config
from sglang.srt.configs.model_config import ModelConfig
from sglang.srt.distributed import (
    get_tp_group,
    get_world_group,
    init_distributed_environment,
    set_custom_all_reduce,
    set_mscclpp_all_reduce,
)
from sglang.srt.layers.dp_attention import (
    get_attention_tp_group,
    initialize_dp_attention,
)
from sglang.srt.layers.moe import initialize_moe_config
from sglang.srt.model_executor.model_runner import ModelRunner as SGLModelRunner
from sglang.srt.server_args import ServerArgs
from sglang.srt.utils import (
    cpu_has_amx_support,
    get_available_gpu_memory,
    get_bool_env_var,
    monkey_patch_p2p_access_check,
)

from parallax.sglang.monkey_patch import apply_parallax_sglang_monkey_patch
from parallax.sglang.monkey_patch_utils.weight_loader_filter import (
    set_layer_range_for_filtering,
)
from parallax.utils.tokenizer_utils import load_tokenizer

logger = logging.getLogger(__name__)

_is_cpu_amx_available = cpu_has_amx_support()


class ParallaxModelRunner(SGLModelRunner):
    """
    Parallax ModelRunner module.
    pp_start_layer and pp_end_layer are passed to initialize states of distribution.
    """

    def __init__(
        self,
        model_config: ModelConfig,
        mem_fraction_static: float,
        gpu_id: int,
        tp_rank: int,
        tp_size: int,
        moe_ep_rank: int,
        moe_ep_size: int,
        pp_rank: int,
        pp_size: int,
        nccl_port: int,
        server_args: ServerArgs,
        pp_start_layer: int,
        pp_end_layer: int,
    ):
        """Add pp_start_layer and pp_end_layer for decentralized model"""
        self.pp_start_layer = pp_start_layer
        self.pp_end_layer = pp_end_layer
        num_hidden_layers = model_config.hf_config.num_hidden_layers
        set_layer_range_for_filtering(pp_start_layer, pp_end_layer, num_hidden_layers)

        super().__init__(
            model_config=model_config,
            mem_fraction_static=mem_fraction_static,
            gpu_id=gpu_id,
            tp_rank=tp_rank,
            tp_size=tp_size,
            pp_rank=pp_rank,
            pp_size=pp_size,
            moe_ep_rank=moe_ep_rank,
            moe_ep_size=moe_ep_size,
            nccl_port=nccl_port,
            server_args=server_args,
        )

    def init_torch_distributed(self):
        """
        Modifies init_torch_distributed in sglang.
        The only difference is to replace initialize_model_parallel.
        """
        logger.info("Init torch distributed begin.")

        try:
            torch.get_device_module(self.device).set_device(self.gpu_id)
        except Exception:
            logger.warning(
                f"Context: {self.device=} {self.gpu_id=} {os.environ.get('CUDA_VISIBLE_DEVICES')=} \
                {self.tp_rank=} {self.tp_size=}"
            )
            raise

        if self.device == "cuda":
            backend = "nccl"
        elif self.device == "xpu":
            backend = "xccl"
        elif self.device == "hpu":
            backend = "hccl"
        elif self.device == "cpu":
            backend = "gloo"
        elif self.device == "npu":
            backend = "hccl"

        before_avail_memory = get_available_gpu_memory(self.device, self.gpu_id)
        if not self.server_args.enable_p2p_check:
            monkey_patch_p2p_access_check()

        if self.server_args.dist_init_addr:
            dist_init_method = f"tcp://{self.server_args.dist_init_addr}"
        else:
            dist_init_method = f"tcp://127.0.0.1:{self.dist_port}"
        set_custom_all_reduce(not self.server_args.disable_custom_all_reduce)
        set_mscclpp_all_reduce(self.server_args.enable_mscclpp)

        if not self.is_draft_worker:
            if self.device == "cpu":
                if _is_cpu_amx_available:
                    # Bind OpenMP threads to CPU cores
                    torch.ops.sgl_kernel.init_cpu_threads_env(self.local_omp_cpuid)

                    # Set local size to hint SGLang to use shared memory based AllReduce
                    os.environ["LOCAL_SIZE"] = str(self.tp_size)
                    torch.ops.sgl_kernel.initialize(self.tp_size, self.tp_rank)
                else:
                    logger.warning(
                        "init_cpu_threads_env and shared memory based AllReduce is disabled \
                         since intel amx backend is not available"
                    )

            # Only initialize the distributed environment on the target model worker.
            init_distributed_environment(
                backend=backend,
                world_size=self.tp_size * self.pp_size,
                rank=self.tp_size * self.pp_rank + self.tp_rank,
                local_rank=self.gpu_id,
                distributed_init_method=dist_init_method,
                timeout=self.server_args.dist_timeout,
            )

            # Use monkey patch modified function
            sglang.srt.distributed.parallel_state.initialize_model_parallel(
                tensor_model_parallel_size=self.tp_size,
                pipeline_model_parallel_size=self.pp_size,
                expert_model_parallel_size=self.moe_ep_size,
                duplicate_tp_group=self.server_args.enable_pdmux,
                pp_start_layer=self.pp_start_layer,
                pp_end_layer=self.pp_end_layer,
                hidden_layers=self.model_config.num_hidden_layers,
            )

            initialize_dp_attention(
                self.server_args,
                self.model_config,
            )

        min_per_gpu_memory = get_available_gpu_memory(
            self.device,
            self.gpu_id,
            distributed=get_world_group().world_size > 1,
            cpu_group=get_world_group().cpu_group,
        )
        self.tp_group = get_tp_group()
        self.attention_tp_group = get_attention_tp_group()

        # Check memory for tensor parallelism
        local_gpu_memory = get_available_gpu_memory(self.device, self.gpu_id)
        if self.tp_size > 1 and not self.is_draft_worker:
            if min_per_gpu_memory < local_gpu_memory * 0.9:
                if get_bool_env_var("SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK"):
                    logger.warning(
                        "The memory capacity is unbalanced. Some GPUs may be occupied by other processes. "
                        f"{min_per_gpu_memory=}, {local_gpu_memory=}, {local_gpu_memory * 0.9=}"
                    )
                else:
                    raise ValueError(
                        "The memory capacity is unbalanced. Some GPUs may be occupied by other processes. "
                        f"{min_per_gpu_memory=}, {local_gpu_memory=}, {local_gpu_memory * 0.9=}"
                    )

        logger.info(
            f"Init torch distributed ends. mem usage={(before_avail_memory - local_gpu_memory):.2f} GB"
        )

        # This is a hack for initializing CudaGraphRunner
        self.server_args.pp_size = 2

        return min_per_gpu_memory


def form_sgl_server_args(
    model_path: str,
    dtype: str = "bfloat16",
    kv_cache_memory_fraction: float = 0.85,
    tp_size: int = 1,
    attention_backend: str = "flashinfer",
    kv_block_size: int = 64,
    moe_runner_backend="auto",
    enable_lora: Optional[bool] = False,
    max_lora_rank: Optional[int] = None,
    lora_target_modules: Optional[List[str]] = None,
    lora_paths: Optional[List[str]] = None,
    max_loras_per_batch: Optional[int] = None,
    max_loaded_loras: Optional[int] = None,
    lora_eviction_policy: Optional[str] = "lru",
    lora_backend: Optional[str] = "triton",
    max_lora_chunk_size: Optional[int] = 128,
):
    """Creates a SGL ServerArgs object"""
    sgl_server_args = ServerArgs(
        model_path=model_path,
        dtype=dtype,
        attention_backend=attention_backend,
        page_size=kv_block_size,
        mem_fraction_static=kv_cache_memory_fraction,
        moe_runner_backend=moe_runner_backend,
        tp_size=tp_size,
        trust_remote_code=True,
        enable_lora=enable_lora,
        max_lora_rank=max_lora_rank,
        lora_target_modules=lora_target_modules,
        lora_paths=lora_paths,
        max_loras_per_batch=max_loras_per_batch,
        max_loaded_loras=max_loaded_loras,
        lora_eviction_policy=lora_eviction_policy,
        lora_backend=lora_backend,
        max_lora_chunk_size=max_lora_chunk_size,
    )
    return sgl_server_args


def initialize_sgl_model_runner(
    model_repo: str,
    start_layer: int,
    end_layer: int,
    kv_cache_memory_fraction: float,
    attention_backend: str,
    kv_block_size: int,
    moe_runner_backend: str,
    max_num_tokens_per_batch: int = 1024,
    enable_lora: Optional[bool] = False,
    max_lora_rank: Optional[int] = None,
    lora_target_modules: Optional[List[str]] = None,
    lora_paths: Optional[List[str]] = None,
    max_loras_per_batch: Optional[int] = None,
    max_loaded_loras: Optional[int] = None,
    lora_eviction_policy: Optional[str] = "lru",
    lora_backend: Optional[str] = "triton",
    max_lora_chunk_size: Optional[int] = 128,
    **kwargs,
):
    """
    Creates a SGL ModelRunner object.
    Returns:
      - model_runner: SGL model runner
      - config: model config driven by mlx-lm
      - tokenizer: tokenizer driven by mlx-lm
    """
    apply_parallax_sglang_monkey_patch()

    # Extract TP-related parameters from kwargs or use defaults
    tp_rank = kwargs.get("tp_rank", 0)
    tp_size = kwargs.get("tp_size", 1)
    use_hfcache = kwargs.get("use_hfcache", False)
    nccl_port = kwargs.get("nccl_port", None)
    # Use selective download for GPU models to save bandwidth and disk space
    from parallax.utils.selective_download import get_model_path_with_selective_download

    logger.info(
        f"Downloading model with selective weight files for layers [{start_layer}, {end_layer})"
    )
    model_path = get_model_path_with_selective_download(
        model_repo, start_layer=start_layer, end_layer=end_layer, local_files_only=use_hfcache
    )

    config = load_config(model_path)
    tokenizer = load_tokenizer(model_path, eos_token_ids=config.get("eos_token_id", None))
    dtype = config.get("torch_dtype", "bfloat16")

    if nccl_port is None:
        nccl_port = random.randint(4000, 5000)

    # Handling mxfp4 arguments
    quant_method = config.get("quant_method", None)
    quantization_config = config.get("quantization_config", None)
    if quant_method is None and quantization_config is not None:
        quant_method = quantization_config.get("quant_method", None)
    if quant_method == "mxfp4":
        attention_backend = "triton"
        moe_runner_backend = "triton_kernel"

    architectures = config.get("architectures", [])
    if architectures and any("Qwen3Next" in arch for arch in architectures):
        logger.debug(f"Qwen3-Next model detected, setting kv_block_size to 1")
        kv_block_size = 1

    server_args = form_sgl_server_args(
        str(model_path),
        dtype,
        kv_cache_memory_fraction,
        tp_size,
        attention_backend,
        kv_block_size,
        moe_runner_backend,
        enable_lora,
        max_lora_rank,
        lora_target_modules,
        lora_paths,
        max_loras_per_batch,
        max_loaded_loras,
        lora_eviction_policy,
        lora_backend,
        max_lora_chunk_size,
    )
    initialize_moe_config(server_args)
    quant_method = None
    if (quantization_config := config.get("quantization_config", None)) is not None:
        quant_method = quantization_config.get("quant_method")
    model_config = ModelConfig(
        model_path=str(model_path),
        model_override_args="{}",
        dtype=dtype,
        quantization=quant_method,
    )
    # TODO: Fix me
    model_config.hf_config.tie_word_embeddings = False
    model_config.hf_config.start_layer = start_layer
    model_config.hf_config.end_layer = end_layer

    logger.debug(f"model_start_layer: {model_config.hf_config.start_layer}")
    logger.debug(f"model_end_layer: {model_config.hf_config.end_layer}")

    model_runner = ParallaxModelRunner(
        model_config=model_config,
        mem_fraction_static=kv_cache_memory_fraction,
        gpu_id=tp_rank,  # Currently reuse tp_rank to only support TP.
        tp_rank=tp_rank,
        tp_size=tp_size,
        pp_rank=0,
        pp_size=1,
        moe_ep_rank=0,
        moe_ep_size=1,
        nccl_port=nccl_port,
        server_args=server_args,
        pp_start_layer=start_layer,
        pp_end_layer=end_layer,
    )
    return model_runner, config, tokenizer


================================================================================
File: src/parallax/sglang/monkey_patch.py
Size: 1.43 kB
================================================================================

"""
Here is some patch func for sglang
Hopefully, when sglang support pipeline parallelism natively, we can remove these patches
"""


def apply_parallax_sglang_monkey_patch():
    from parallax.sglang.monkey_patch_utils.model_parallel import (
        apply_model_parallel_monkey_patch,
    )

    apply_model_parallel_monkey_patch()

    from parallax.sglang.monkey_patch_utils.triton_backend import (
        apply_triton_backend_init_monkey_patch,
    )

    apply_triton_backend_init_monkey_patch()

    from parallax.sglang.monkey_patch_utils.weight_loader_filter import (
        apply_weight_loader_filter_patch,
    )

    apply_weight_loader_filter_patch()

    from parallax.sglang.monkey_patch_utils.qwen3_next_config import (
        apply_qwen3_next_config_monkey_patch,
    )

    apply_qwen3_next_config_monkey_patch()

    from parallax.sglang.monkey_patch_utils.qwen3_next_model import (
        apply_qwen3_next_monkey_patch,
    )

    apply_qwen3_next_monkey_patch()

    from parallax.sglang.monkey_patch_utils.gpt_oss_model import (
        apply_gpt_oss_monkey_patch,
    )

    apply_gpt_oss_monkey_patch()

    from parallax.sglang.monkey_patch_utils.minimax_m2_model import (
        apply_minimax_m2_monkey_patch,
    )

    apply_minimax_m2_monkey_patch()

    from parallax.sglang.monkey_patch_utils.glm4_moe_model import (
        apply_glm4_moe_monkey_patch,
    )

    apply_glm4_moe_monkey_patch()


================================================================================
File: src/parallax/sglang/monkey_patch_utils/glm4_moe_model.py
Size: 8.23 kB
================================================================================

## This is a patch file for sglang glm4_moe model to support pipeline parallelism

import logging
from typing import Iterable, Optional, Tuple

import torch
from sglang.srt.distributed import get_pp_group
from sglang.srt.layers.moe.fused_moe_triton.layer import FusedMoE
from sglang.srt.layers.utils import get_layer_id
from sglang.srt.model_executor.forward_batch_info import PPProxyTensors
from sglang.srt.model_loader.weight_utils import default_weight_loader

logger = logging.getLogger(__name__)


def monkey_patch_load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn=False):
    """Load model weights with proper mapping for GLM4 Moe architecture."""
    if is_nextn:
        if hasattr(self.config, "num_nextn_predict_layers"):
            num_nextn_layers = self.config.num_nextn_predict_layers
            assert num_nextn_layers == 1, "Only 1 nextn layer is supported"
            # compatible with old design
            nextn_layer_id = (
                0 if self.config.num_hidden_layers == 1 else self.config.num_hidden_layers
            )
        else:
            raise ValueError("num_nextn_predict_layers is not in the config")

    stacked_params_mapping = [
        # (param_name, shard_name, shard_id)
        ("qkv_proj", "q_proj", "q"),
        ("qkv_proj", "k_proj", "k"),
        ("qkv_proj", "v_proj", "v"),
        ("gate_up_proj", "gate_proj", 0),
        ("gate_up_proj", "up_proj", 1),
    ]

    expert_params_mapping = FusedMoE.make_expert_params_mapping(
        ckpt_gate_proj_name="gate_proj",
        ckpt_down_proj_name="down_proj",
        ckpt_up_proj_name="up_proj",
        num_experts=self.config.n_routed_experts,
    )

    if is_nextn:
        nextn_layer_prefix = f"model.layers.{nextn_layer_id}"
        nextn_spec_weight_names = [
            "shared_head.norm",
            "eh_proj",
            "enorm",
            "hnorm",
        ]

    params_dict = dict(self.named_parameters())
    weight_names = []
    for name, loaded_weight in weights:
        ############################################################################
        ## TODO: remove when sglang code support pipeline parallelism
        ## This is a patch code for sgalng
        if "lm_head" in name:
            pp_group = getattr(self, "pp_group", None) or get_pp_group()
            if not pp_group.is_last_rank:
                logger.debug("Skipping lm_head weight '%s' on non-last PP rank", name)
                continue
        layer_id = get_layer_id(name)
        if (
            layer_id is not None
            and hasattr(self.model, "start_layer")
            and (layer_id < self.model.start_layer or layer_id >= self.model.end_layer)
        ):
            continue
        ## End of patch
        ############################################################################
        weight_names.append(name)

        if not is_nextn:
            if hasattr(self.config, "num_nextn_predict_layers"):
                num_nextn_layers = self.config.num_nextn_predict_layers
                if num_nextn_layers > 0 and name.startswith("model.layers"):
                    name_list = name.split(".")
                    if len(name_list) >= 3 and int(name_list[2]) >= self.config.num_hidden_layers:
                        continue
        else:
            if not name.startswith(nextn_layer_prefix):
                continue

            # Use shared head and embed weights from target model
            if "shared_head.head" in name or "embed_tokens" in name:
                continue

            is_decoder = True
            # For nextn specific weights
            for weight_name in nextn_spec_weight_names:
                if weight_name in name:
                    name = name.replace(nextn_layer_prefix, "model")
                    is_decoder = False
                    break
            # For decoder layer weights
            if is_decoder:
                name = name.replace(nextn_layer_prefix, "model.decoder")

        if "rotary_emb.inv_freq" in name:
            continue
        for param_name, weight_name, shard_id in stacked_params_mapping:
            # Skip non-stacked layers and experts (experts handled below).
            if weight_name not in name:
                continue
            # We have mlp.experts[0].gate_proj in the checkpoint.
            # Since we handle the experts below in expert_params_mapping,
            # we need to skip here BEFORE we update the name, otherwise
            # name will be updated to mlp.experts[0].gate_up_proj, which
            # will then be updated below in expert_params_mapping
            # for mlp.experts[0].gate_gate_up_proj, which breaks load.
            if "mlp.experts" in name:
                continue
            name = name.replace(weight_name, param_name)
            # Skip loading extra bias for GPTQ models.
            if name.endswith(".bias") and name not in params_dict:
                continue
            if name not in params_dict:
                continue

            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            # Track if this is an expert weight to enable early skipping
            is_expert_weight = False

            for mapping in expert_params_mapping:
                param_name, weight_name, expert_id, shard_id = mapping
                if weight_name not in name:
                    continue

                # Mark as expert weight regardless of whether we can process it
                is_expert_weight = True

                name = name.replace(weight_name, param_name)
                if name not in params_dict:
                    # Expert weight not on this rank, will be skipped below
                    continue

                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(
                    param,
                    loaded_weight,
                    name,
                    shard_id=shard_id,
                    expert_id=expert_id,
                )
                break
            else:
                if is_expert_weight:
                    # This is an expert weight but not mapped to this rank, skip all remaining processing
                    continue

                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue
                if name not in params_dict:
                    continue

                if name in params_dict.keys():
                    param = params_dict[name]
                    weight_loader = getattr(param, "weight_loader", default_weight_loader)
                    weight_loader(param, loaded_weight)
                else:
                    logger.warning(f"Parameter {name} not found in params_dict")


def apply_glm4_moe_monkey_patch():
    """Apply monkey patches to GLM4 Moe for PP support and weight loading."""
    import sglang.srt.models.glm4_moe as glm4_moe_module

    def pp_forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch,
        inputs_embeds: Optional[torch.Tensor] = None,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
        **kwargs,
    ):
        hidden_states = self.model(
            input_ids, positions, forward_batch, inputs_embeds, pp_proxy_tensors
        )

        if isinstance(hidden_states, PPProxyTensors):
            return hidden_states
        ################################################################################
        ## Patch for PP: only last PP rank compute logits
        pp_group = getattr(self, "pp_group", None) or get_pp_group()
        if pp_group.is_last_rank:
            return self.logits_processor(input_ids, hidden_states, self.lm_head, forward_batch)
        else:
            return hidden_states
        ## End of patch
        ################################################################################

    glm4_moe_module.Glm4MoeForCausalLM.forward = pp_forward
    glm4_moe_module.Glm4MoeForCausalLM.load_weights = monkey_patch_load_weights


================================================================================
File: src/parallax/sglang/monkey_patch_utils/gpt_oss_model.py
Size: 7.3 kB
================================================================================

## This is a patch file for sglang GPT-OSS model to support loading mxFP4 MoE experts weights

import math

import torch
from sglang.srt.distributed import (
    get_moe_expert_parallel_rank,
    get_moe_expert_parallel_world_size,
    get_moe_tensor_parallel_rank,
    get_moe_tensor_parallel_world_size,
)
from sglang.srt.layers.utils import get_layer_id
from sglang.srt.model_loader.weight_utils import default_weight_loader
from sglang.srt.models.gpt_oss import GptOssForCausalLM


def _parallax_load_mxfp4_experts_weights(self, weights):
    params_dict = dict(self.named_parameters())
    loaded_params: set[str] = set()
    mxfp4_block = 32

    moe_tp_rank = get_moe_tensor_parallel_rank()
    moe_tp_size = get_moe_tensor_parallel_world_size()
    moe_ep_rank = get_moe_expert_parallel_rank()
    moe_ep_size = get_moe_expert_parallel_world_size()

    intermediate_size = self.config.intermediate_size
    assert (
        intermediate_size % mxfp4_block == 0
    ), f"{intermediate_size=} must be divisible by {mxfp4_block=}"
    intermediate_size_block = intermediate_size // mxfp4_block

    per_rank_intermediate_size_block = math.ceil(intermediate_size_block / moe_tp_size)

    per_rank_intermediate_size = per_rank_intermediate_size_block * mxfp4_block

    # Calculate common slicing bounds for current rank
    assert self.config.num_local_experts % moe_ep_size == 0
    moe_num_global_experts = self.config.num_local_experts
    moe_num_local_experts = self.config.num_local_experts // moe_ep_size

    moe_tp_rank_start = moe_tp_rank * per_rank_intermediate_size
    moe_tp_rank_end = min((moe_tp_rank + 1) * per_rank_intermediate_size, intermediate_size)

    moe_ep_rank_start = moe_ep_rank * moe_num_local_experts
    moe_ep_rank_end = (moe_ep_rank + 1) * moe_num_local_experts

    for name, weight in weights:
        ############################################################################
        ## TODO: remove when sglang code support pipeline parallelism
        ## This is a patch code for sgalng
        layer_id = get_layer_id(name)
        if (
            layer_id is not None
            and hasattr(self.model, "start_layer")
            and (layer_id < self.model.start_layer or layer_id >= self.model.end_layer)
        ):
            continue
        ## End of patch
        ############################################################################
        weight = weight.cuda()

        if "gate_up_proj_blocks" in name:
            # Handle MLP gate and up projection weights
            new_name = name.replace("gate_up_proj_blocks", "w13_weight")

            # flat weight from (E, 2 * N, block_size, entry_per_block)
            # to (E, 2 * N, -1), shouldn't trigger copy for contiguous
            weight = weight.view(moe_num_global_experts, 2 * intermediate_size, -1).contiguous()

            narrow_weight = weight[
                moe_ep_rank_start:moe_ep_rank_end,
                2 * moe_tp_rank_start : 2 * moe_tp_rank_end,
                ...,
            ]

            param = params_dict[new_name]
            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(
                param,
                narrow_weight,
                weight_name=new_name,
                shard_id=None,
                expert_id=None,
            )
            loaded_params.add(new_name)

        elif "down_proj_blocks" in name:
            # Handle MLP down projection weights
            new_name = name.replace("down_proj_blocks", "w2_weight")
            # same flatten here, but since 2 mx4 value are packed in 1
            # uint8, divide by 2
            weight = weight.view(moe_num_global_experts, -1, intermediate_size // 2).contiguous()
            narrow_weight = weight[
                moe_ep_rank_start:moe_ep_rank_end,
                ...,
                moe_tp_rank_start // 2 : moe_tp_rank_end // 2,
            ]

            param = params_dict[new_name]
            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(
                param,
                narrow_weight,
                weight_name=new_name,
                shard_id=None,
                expert_id=None,
            )
            loaded_params.add(new_name)

        elif "gate_up_proj_scales" in name:
            # Handle MLP gate and up projection weights scale
            new_name = name.replace("gate_up_proj_scales", "w13_weight_scale")
            narrow_weight = weight[
                moe_ep_rank_start:moe_ep_rank_end,
                2 * moe_tp_rank_start : 2 * moe_tp_rank_end,
                ...,
            ]

            param = params_dict[new_name]
            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(
                param,
                narrow_weight,
                weight_name=new_name,
                shard_id=None,
                expert_id=None,
            )
            loaded_params.add(new_name)

        elif "down_proj_scales" in name:
            # Handle MLP down projection weights
            new_name = name.replace("down_proj_scales", "w2_weight_scale")
            narrow_weight = weight[
                moe_ep_rank_start:moe_ep_rank_end,
                ...,
                moe_tp_rank_start // mxfp4_block : moe_tp_rank_end // mxfp4_block,
            ]

            param = params_dict[new_name]
            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(
                param,
                narrow_weight,
                weight_name=new_name,
                shard_id=None,
                expert_id=None,
            )
            loaded_params.add(new_name)
        elif "gate_up_proj_bias" in name:
            # Handle MLP gate and up projection biases
            new_name = name.replace("gate_up_proj_bias", "w13_weight_bias")

            narrow_weight = weight[
                moe_ep_rank_start:moe_ep_rank_end,
                2 * moe_tp_rank_start : 2 * moe_tp_rank_end,
            ]

            param = params_dict[new_name]
            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(
                param,
                narrow_weight,
                weight_name=new_name,
                shard_id=None,
                expert_id=None,
            )
            loaded_params.add(new_name)

        elif "down_proj_bias" in name:
            narrow_weight = weight[moe_ep_rank_start:moe_ep_rank_end, ...]
            if moe_tp_rank != 0:
                narrow_weight = torch.zeros_like(narrow_weight)

            # Handle MLP down projection bias
            new_name = name.replace("down_proj_bias", "w2_weight_bias")
            param = params_dict[new_name]
            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(
                param,
                narrow_weight,
                weight_name=new_name,
                shard_id=None,
                expert_id=None,
            )
            loaded_params.add(new_name)

    return loaded_params


def apply_gpt_oss_monkey_patch():
    GptOssForCausalLM._load_mxfp4_experts_weights = _parallax_load_mxfp4_experts_weights


================================================================================
File: src/parallax/sglang/monkey_patch_utils/minimax_m2_model.py
Size: 6.18 kB
================================================================================

## This is a patch file for sglang MiniMax M2 model to support pipeline parallelism

import logging
from typing import Iterable, Optional, Set, Tuple

import torch
from sglang.srt.distributed import get_pp_group
from sglang.srt.layers.moe.fused_moe_triton.layer import FusedMoE
from sglang.srt.layers.utils import get_layer_id
from sglang.srt.model_executor.forward_batch_info import PPProxyTensors
from sglang.srt.model_loader.weight_utils import (
    default_weight_loader,
    maybe_remap_kv_scale_name,
)
from sglang.srt.models.minimax_m2 import get_spec_layer_idx_from_weight_name

logger = logging.getLogger(__name__)


def monkey_patch_load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    """Load model weights with proper mapping for MiniMax architecture."""

    stacked_params_mapping = [
        # (param_name, shard_name, shard_id)
        ("qkv_proj", "q_proj", "q"),
        ("qkv_proj", "k_proj", "k"),
        ("qkv_proj", "v_proj", "v"),
        ("gate_up_proj", "gate_proj", 0),
        ("gate_up_proj", "up_proj", 1),
    ]

    # Params for weights, fp8 weight scales, fp8 activation scales
    # (param_name, weight_name, expert_id, shard_id)
    expert_params_mapping = FusedMoE.make_expert_params_mapping(
        ckpt_gate_proj_name="w1",
        ckpt_down_proj_name="w2",
        ckpt_up_proj_name="w3",
        num_experts=self.config.num_local_experts,
    )

    params_dict = dict(self.named_parameters())
    loaded_params: Set[str] = set()
    for name, loaded_weight in weights:
        if "lm_head" in name:
            pp_group = getattr(self, "pp_group", None) or get_pp_group()
            if not pp_group.is_last_rank:
                logger.debug("Skipping lm_head weight '%s' on non-last PP rank", name)
                continue

        layer_id = get_layer_id(name)
        if (
            layer_id is not None
            and hasattr(self.model, "start_layer")
            and (layer_id < self.model.start_layer or layer_id >= self.model.end_layer)
        ):
            continue
        if "rotary_emb.inv_freq" in name:
            continue

        spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)
        if spec_layer is not None:
            continue  # skip spec decode layers for main model

        for param_name, weight_name, shard_id in stacked_params_mapping:
            # Skip non-stacked layers and experts (experts handled below).
            if weight_name not in name:
                continue
            # We have mlp.experts[0].gate_proj in the checkpoint.
            # Since we handle the experts below in expert_params_mapping,
            # we need to skip here BEFORE we update the name, otherwise
            # name will be updated to mlp.experts[0].gate_up_proj, which
            # will then be updated below in expert_params_mapping
            # for mlp.experts[0].gate_gate_up_proj, which breaks load.
            if ("mlp.experts." in name) and name not in params_dict:
                continue
            name = name.replace(weight_name, param_name)
            # Skip loading extra bias for GPTQ models.
            if name.endswith(".bias") and name not in params_dict:
                continue

            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            for mapping in expert_params_mapping:
                param_name, weight_name, expert_id, shard_id = mapping
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)

                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(
                    param,
                    loaded_weight,
                    name,
                    shard_id=shard_id,
                    expert_id=expert_id,
                )
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(".bias") and name not in params_dict:
                    continue

                # Remapping the name of FP8 kv-scale.
                name = maybe_remap_kv_scale_name(name, params_dict)
                if name is None:
                    continue

                param = params_dict[name]
                weight_loader = getattr(param, "weight_loader", default_weight_loader)
                weight_loader(param, loaded_weight)
        loaded_params.add(name)
    return loaded_params


def apply_minimax_m2_monkey_patch():
    """Apply monkey patches to MiniMax M2 for PP support and weight loading."""
    import sglang.srt.models.minimax_m2 as m2_module

    orig_init = m2_module.MiniMaxM2ForCausalLM.__init__

    def pp_init(self, config, quant_config=None, prefix=""):
        orig_init(self, config, quant_config, prefix)
        self.pp_group = get_pp_group()

    def pp_forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch,
        inputs_embeds: Optional[torch.Tensor] = None,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
        **kwargs,
    ):
        hidden_states = self.model(
            input_ids, positions, forward_batch, inputs_embeds, pp_proxy_tensors
        )

        if isinstance(hidden_states, PPProxyTensors):
            return hidden_states
        ##########################################################################
        ## TODO: remove when sglang code support pipeline parallelism
        ## This is a patch code for sgalng
        pp_group = getattr(self, "pp_group", None) or get_pp_group()
        if pp_group.is_last_rank:
            return self.logits_processor(input_ids, hidden_states, self.lm_head, forward_batch)
        else:
            return hidden_states
        ## End of patch
        ##########################################################################

    m2_module.MiniMaxM2ForCausalLM.__init__ = pp_init
    m2_module.MiniMaxM2ForCausalLM.forward = pp_forward
    m2_module.MiniMaxM2ForCausalLM.load_weights = monkey_patch_load_weights


================================================================================
File: src/parallax/sglang/monkey_patch_utils/model_parallel.py
Size: 12.98 kB
================================================================================

"""Parallax model-parallel monkey patches for sglang.

Summary:
- ParallaxGroupCoordinator (subclasses sglang.srt.distributed.parallel_state.GroupCoordinator):
    adds pp_start_layer, pp_end_layer, hidden_layers and redefines is_first_rank/is_last_rank to use
    layer ranges.
- monkey_patch_init_model_parallel_group: replaces
    sglang.srt.distributed.parallel_state.init_model_parallel_group to return ParallaxGroupCoordinator.
- monkey_patch_initialize_model_parallel: replaces
    sglang.srt.distributed.parallel_state.initialize_model_parallel and passes PP layer bounds when
    creating pipeline-parallel groups.
- monkey_patch_make_layers: replaces sglang.srt.utils.make_layers; uses
    get_pp_group().pp_start_layer/end_layer to instantiate local layers and PPMissingLayer placeholders
    for non-local layers.

These are minimal, reversible patches to support decentralized per-layer pipeline parallelism. Remove
when upstream sglang provides native support.
"""

import logging
from typing import Any, Dict, List, Optional, Tuple, Union

import sglang
import sglang.srt.distributed.parallel_state
import torch
from sglang.srt.distributed import get_world_group
from sglang.srt.distributed.parallel_state import (
    GroupCoordinator as SGLGroupCoordinator,
)
from sglang.srt.utils import (
    LayerFn,
    add_prefix,
    cpu_has_amx_support,
    get_bool_env_var,
    is_npu,
)
from torch.distributed import Backend

# from parallax.sglang.monkey_patch.model_runner import ModelRunner as SGLModelRunner

logger = logging.getLogger(__name__)

_is_cpu_amx_available = cpu_has_amx_support()


class ParallaxGroupCoordinator(SGLGroupCoordinator):
    """
    Parallax GroupCoordinator module.
    pp_start_layer, pp_end_layer, hidden_layers are necessary for decentralized inference.
    Also change the definition of first_rank/last_rank.
    """

    def __init__(
        self,
        group_ranks: List[List[int]],
        local_rank: int,
        torch_distributed_backend: Union[str, Backend],
        use_pynccl: bool,
        use_pymscclpp: bool,
        use_custom_allreduce: bool,
        use_hpu_communicator: bool,
        use_xpu_communicator: bool,
        use_npu_communicator: bool,
        use_torch_symm_mem: bool = False,
        use_message_queue_broadcaster: bool = False,
        group_name: Optional[str] = None,
        pp_start_layer: int = 0,
        pp_end_layer: int = 0,
        hidden_layers: int = 0,
    ):
        """Add pp_start_layer, pp_end_layer, hidden_layers for decentralized model"""
        super().__init__(
            group_ranks=group_ranks,
            local_rank=local_rank,
            torch_distributed_backend=torch_distributed_backend,
            use_pynccl=use_pynccl,
            use_pymscclpp=use_pymscclpp,
            use_custom_allreduce=use_custom_allreduce,
            use_hpu_communicator=use_hpu_communicator,
            use_xpu_communicator=use_xpu_communicator,
            use_npu_communicator=use_npu_communicator,
            use_torch_symm_mem_all_reduce=use_torch_symm_mem,
            use_message_queue_broadcaster=use_message_queue_broadcaster,
            group_name=group_name,
        )
        self.pp_start_layer = pp_start_layer
        self.pp_end_layer = pp_end_layer
        self.hidden_layers = hidden_layers

    @property
    def is_first_rank(self):
        """Return whether the caller is the first process in the group"""
        return self.pp_start_layer == 0

    @property
    def is_last_rank(self):
        """Return whether the caller is the last process in the group"""
        return self.pp_end_layer == self.hidden_layers


def monkey_patch_init_model_parallel_group(
    group_ranks: List[List[int]],
    local_rank: int,
    backend: str,
    use_custom_allreduce: Optional[bool] = None,
    use_message_queue_broadcaster: bool = False,
    group_name: Optional[str] = None,
    use_mscclpp_allreduce: Optional[bool] = None,
    pp_start_layer: int = 0,
    pp_end_layer: int = 0,
    hidden_layers: int = 0,
) -> SGLGroupCoordinator:
    """A monkey patch to replace sglang.srt.distributed.parallel_state.init_model_parallel_group"""
    if use_custom_allreduce is None:
        use_custom_allreduce = sglang.srt.distributed.parallel_state._ENABLE_CUSTOM_ALL_REDUCE
    if use_mscclpp_allreduce is None:
        use_mscclpp_allreduce = sglang.srt.distributed.parallel_state._ENABLE_MSCCLPP_ALL_REDUCE
    return ParallaxGroupCoordinator(
        group_ranks=group_ranks,
        local_rank=local_rank,
        torch_distributed_backend=backend,
        use_pynccl=not is_npu(),
        use_pymscclpp=use_mscclpp_allreduce,
        use_custom_allreduce=use_custom_allreduce,
        use_hpu_communicator=True,
        use_xpu_communicator=True,
        use_npu_communicator=True,
        use_message_queue_broadcaster=use_message_queue_broadcaster,
        group_name=group_name,
        pp_start_layer=pp_start_layer,
        pp_end_layer=pp_end_layer,
        hidden_layers=hidden_layers,
    )


def monkey_patch_initialize_model_parallel(
    tensor_model_parallel_size: int = 1,
    expert_model_parallel_size: int = 1,
    pipeline_model_parallel_size: int = 1,
    backend: Optional[str] = None,
    duplicate_tp_group: bool = False,
    pp_start_layer: int = 0,
    pp_end_layer: int = 0,
    hidden_layers: int = 0,
) -> None:
    """A monkey patch to replace sglang.srt.distributed.parallel_state.initialize_model_parallel"""
    # Get world size and rank. Ensure some consistencies.
    assert torch.distributed.is_initialized()
    world_size: int = torch.distributed.get_world_size()
    backend = backend or torch.distributed.get_backend(get_world_group().device_group)

    if world_size != tensor_model_parallel_size * pipeline_model_parallel_size:
        raise RuntimeError(
            f"world_size ({world_size}) is not equal to "
            f"tensor_model_parallel_size ({tensor_model_parallel_size}) x "
            f"pipeline_model_parallel_size ({pipeline_model_parallel_size})"
        )

    # Build the tensor model-parallel groups.
    num_tensor_model_parallel_groups: int = world_size // tensor_model_parallel_size
    ############################################################################
    ## This is a patch code for sgalng
    ## Ignore parallel state already set alert
    # assert (
    #     sglang.srt.distributed.parallel_state._TP is None
    # ), "tensor model parallel group is already initialized"
    ## End of patch
    ############################################################################
    group_ranks = []
    for i in range(num_tensor_model_parallel_groups):
        ranks = list(range(i * tensor_model_parallel_size, (i + 1) * tensor_model_parallel_size))
        group_ranks.append(ranks)

    # message queue broadcaster is only used in tensor model parallel group
    sglang.srt.distributed.parallel_state._TP = (
        sglang.srt.distributed.parallel_state.init_model_parallel_group(
            group_ranks,
            get_world_group().local_rank,
            backend,
            use_message_queue_broadcaster=get_bool_env_var(
                "SGLANG_USE_MESSAGE_QUEUE_BROADCASTER", "true"
            ),
            group_name="tp",
        )
    )

    if duplicate_tp_group:
        global _PDMUX_PREFILL_TP_GROUP
        assert (
            _PDMUX_PREFILL_TP_GROUP is None
        ), "tensor model parallel group for PD-Multiplexing Prefill is already initialized"
        _PDMUX_PREFILL_TP_GROUP = sglang.srt.distributed.parallel_state.init_model_parallel_group(
            group_ranks,
            get_world_group().local_rank,
            backend,
            use_message_queue_broadcaster=get_bool_env_var(
                "SGLANG_USE_MESSAGE_QUEUE_BROADCASTER", "true"
            ),
            group_name="pdmux_prefill_tp",
        )
        sglang.srt.distributed.parallel_state._TP.pynccl_comm.disabled = False
        _PDMUX_PREFILL_TP_GROUP.pynccl_comm.disabled = False

    moe_ep_size = expert_model_parallel_size

    moe_tp_size = tensor_model_parallel_size // moe_ep_size
    ############################################################################
    ## This is a patch code for sgalng
    ## Ignore parallel state already set alert
    # assert (
    #     sglang.srt.distributed.parallel_state._MOE_EP is None
    # ), "expert model parallel group is already initialized"
    ## End of patch
    ############################################################################
    group_ranks = []
    for i in range(num_tensor_model_parallel_groups):
        for j in range(moe_tp_size):
            st = i * tensor_model_parallel_size + j
            en = (i + 1) * tensor_model_parallel_size + j
            ranks = list(range(st, en, moe_tp_size))
            group_ranks.append(ranks)

    sglang.srt.distributed.parallel_state._MOE_EP = (
        sglang.srt.distributed.parallel_state.init_model_parallel_group(
            group_ranks,
            get_world_group().local_rank,
            backend,
            use_custom_allreduce=False,
            group_name="moe_ep",
        )
    )

    ############################################################################
    ## This is a patch code for sgalng
    ## Ignore parallel state already set alert
    # assert (
    #     sglang.srt.distributed.parallel_state._MOE_TP is None
    # ), "expert model parallel group is already initialized"
    ## End of patch
    ############################################################################
    group_ranks = []
    for i in range(num_tensor_model_parallel_groups):
        for j in range(moe_ep_size):
            st = i * tensor_model_parallel_size + j * moe_tp_size
            en = i * tensor_model_parallel_size + (j + 1) * moe_tp_size
            ranks = list(range(st, en))
            group_ranks.append(ranks)

    sglang.srt.distributed.parallel_state._MOE_TP = (
        sglang.srt.distributed.parallel_state.init_model_parallel_group(
            group_ranks,
            get_world_group().local_rank,
            backend,
            use_custom_allreduce=False,
            group_name="moe_tp",
        )
    )

    # Build the pipeline model-parallel groups.
    num_pipeline_model_parallel_groups: int = world_size // pipeline_model_parallel_size
    ############################################################################
    ## This is a patch code for sgalng
    ## Ignore parallel state already set alert
    # assert (
    #     sglang.srt.distributed.parallel_state._PP is None
    # ), "pipeline model parallel group is already initialized"
    ## End of patch
    ############################################################################
    group_ranks = []
    for i in range(num_pipeline_model_parallel_groups):
        ranks = list(range(i, world_size, num_pipeline_model_parallel_groups))
        group_ranks.append(ranks)
    # pipeline parallel does not need custom allreduce
    sglang.srt.distributed.parallel_state._PP = (
        sglang.srt.distributed.parallel_state.init_model_parallel_group(
            group_ranks,
            get_world_group().local_rank,
            backend,
            use_custom_allreduce=False,
            group_name="pp",
            pp_start_layer=pp_start_layer,
            pp_end_layer=pp_end_layer,
            hidden_layers=hidden_layers,
        )
    )


def monkey_patch_make_layers(
    num_hidden_layers: int,
    layer_fn: LayerFn,
    pp_rank: Optional[int] = None,
    pp_size: Optional[int] = None,
    prefix: str = "",
    return_tuple: bool = True,
    offloader_kwargs: Dict[str, Any] = {},
) -> Tuple[int, int, torch.nn.ModuleList]:
    """A monkey patch to replace sglang.srt.utils.make_layers"""
    # circula imports
    from sglang.srt.distributed import get_pp_group
    from sglang.srt.layers.utils import PPMissingLayer
    from sglang.srt.utils.offloader import get_offloader

    assert not pp_size or num_hidden_layers >= pp_size
    start_layer, end_layer = get_pp_group().pp_start_layer, get_pp_group().pp_end_layer

    modules = torch.nn.ModuleList(
        [PPMissingLayer(return_tuple=return_tuple) for _ in range(start_layer)]
        + get_offloader().wrap_modules(
            (
                layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
                for idx in range(start_layer, end_layer)
            ),
            **offloader_kwargs,
        )
        + [PPMissingLayer(return_tuple=return_tuple) for _ in range(end_layer, num_hidden_layers)]
    )
    if pp_rank is None or pp_size is None:
        return modules
    return modules, start_layer, end_layer


def apply_model_parallel_monkey_patch():
    sglang.srt.distributed.parallel_state.init_model_parallel_group = (
        monkey_patch_init_model_parallel_group
    )
    sglang.srt.distributed.parallel_state.initialize_model_parallel = (
        monkey_patch_initialize_model_parallel
    )
    sglang.srt.utils.make_layers = monkey_patch_make_layers


================================================================================
File: src/parallax/sglang/monkey_patch_utils/qwen3_next_config.py
Size: 2.08 kB
================================================================================

"""Qwen3Hybrid model configuration"""

import enum

from transformers.utils import logging

logger = logging.get_logger(__name__)


# NOTE: HybridLayerType
class HybridLayerType(enum.Enum):
    full_attention = "attention"
    swa_attention = "swa_attention"
    linear_attention = "linear_attention"
    mamba2 = "mamba"


## overwirite due to pipeline parallelism
@property
def monkey_patch_linear_layer_ids(self):
    """Return linear-attention layer ids restricted to the PP slice.

    This is intended to be bound as a property on
    `sglang.srt.configs.qwen3_next.Qwen3NextConfig`.
    """
    lst = [
        i
        for i, type_value in enumerate(self.layers_block_type)
        if type_value == HybridLayerType.linear_attention.value
        and i >= self.start_layer
        and i < self.end_layer
    ]
    ## If no matching layer id, return at least [-1]
    ## It is for memory pool calcuate tokens
    return lst if lst else [-1]


## overwirite due to pipeline parallelism
@property
def monkey_patch_full_attention_layer_ids(self):
    """Return full-attention layer ids restricted to the PP slice.

    This is intended to be bound as a property on
    `sglang.srt.configs.qwen3_next.Qwen3NextConfig`.
    """
    lst = [
        i
        for i, type_value in enumerate(self.layers_block_type)
        if type_value == HybridLayerType.full_attention.value
        and i >= self.start_layer
        and i < self.end_layer
    ]
    ## If no matching layer id, return at least [-1]
    ## It is for memory pool calcuate tokens
    return lst if lst else [-1]


def apply_qwen3_next_config_monkey_patch():
    """Bind monkey-patch helpers to the upstream Qwen3NextConfig class.

    We attach the two helpers above as properties so callers can access
    `config.linear_layer_ids` / `config.full_attention_layer_ids` the same
    way upstream expects.
    """

    import sglang.srt.configs.qwen3_next as s

    s.Qwen3NextConfig.linear_layer_ids = monkey_patch_linear_layer_ids
    s.Qwen3NextConfig.full_attention_layer_ids = monkey_patch_full_attention_layer_ids


================================================================================
File: src/parallax/sglang/monkey_patch_utils/qwen3_next_model.py
Size: 8.92 kB
================================================================================

import logging
from typing import Iterable, Optional, Set, Tuple

from sglang.srt.configs.qwen3_next import Qwen3NextConfig
from sglang.srt.layers.quantization.base_config import QuantizationConfig
from sglang.srt.utils import is_cuda
from torch import nn

logger = logging.getLogger(__name__)
_is_cuda = is_cuda()


# ---- Minimal method-level monkey patch to reuse sglang source ----
# Due to Qwen3NextModel not support pipeline parallelism (PP) natively
def apply_qwen3_next_monkey_patch():
    """Apply minimal monkey patches to sglang's qwen3_next to support PP without copying code.

    We override only a few methods:
    - Qwen3NextModel.__init__: build layers with PP slicing, gate embed/norm by first/last rank.
    - Qwen3NextModel.forward: accept/return PPProxyTensors between stages.
    - Qwen3NextForCausalLM.__init__: remove single-rank assertion, keep original wiring.
    - Qwen3NextForCausalLM.forward: only last rank computes logits; others pass proxies.
    - Qwen3NextForCausalLM.load_weights: pre-filter weights by layer_id to load only local slice.
    """
    import torch
    from sglang.srt.distributed import get_pp_group
    from sglang.srt.layers.dp_attention import is_dp_attention_enabled
    from sglang.srt.layers.layernorm import GemmaRMSNorm
    from sglang.srt.layers.utils import PPMissingLayer, get_layer_id
    from sglang.srt.layers.vocab_parallel_embedding import (
        ParallelLMHead,
        VocabParallelEmbedding,
    )
    from sglang.srt.model_executor.forward_batch_info import PPProxyTensors
    from sglang.srt.server_args import get_global_server_args
    from sglang.srt.utils import add_prefix, is_cuda, make_layers

    try:
        import sglang.srt.models.qwen3_next as m
    except Exception as e:  # Fallback: keep current module as-is
        logger.warning(
            f"Failed to import sglang.srt.models.qwen3_next for monkey patch: {e}. Using local copy."
        )
        return

    # --- Patch Qwen3NextModel.__init__ ---
    def _pp_model_init(
        self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = ""
    ):
        nn.Module.__init__(self)
        self.config = config
        self.pp_group = get_pp_group()
        alt_stream = torch.cuda.Stream() if is_cuda() else None

        if self.pp_group.is_first_rank:
            self.embed_tokens = VocabParallelEmbedding(
                config.vocab_size,
                config.hidden_size,
                org_num_embeddings=config.vocab_size,
                enable_tp=not is_dp_attention_enabled(),
            )
        else:
            self.embed_tokens = PPMissingLayer()

        def get_layer(idx: int, prefix: str):
            layer_class = m.ALL_DECODER_LAYER_TYPES[config.layers_block_type[idx]]
            return layer_class(
                config,
                idx,
                quant_config=quant_config,
                prefix=prefix,
                alt_stream=alt_stream,
            )

        self.layers, self.start_layer, self.end_layer = make_layers(
            config.num_hidden_layers,
            get_layer,
            prefix=f"{prefix}.layers",
            pp_rank=self.pp_group.rank_in_group,
            pp_size=self.pp_group.world_size,
        )
        if self.pp_group.is_last_rank:
            self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        else:
            self.norm = PPMissingLayer(return_tuple=True)
        self.infer_count = 0

    # --- Patch Qwen3NextModel.forward ---
    def _pp_model_forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch,
        inputs_embeds: Optional[torch.Tensor] = None,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
        **kwargs,
    ):
        if self.pp_group.is_first_rank:
            hidden_states = (
                inputs_embeds if inputs_embeds is not None else self.embed_tokens(input_ids)
            )
            residual = None
        else:
            assert (
                pp_proxy_tensors is not None
            ), "pp_proxy_tensors must be provided on non-first PP ranks"
            hidden_states = pp_proxy_tensors["hidden_states"]
            residual = pp_proxy_tensors["residual"]

        for i in range(self.start_layer, self.end_layer):
            layer = self.layers[i]
            hidden_states, residual = layer(
                layer_id=i,
                positions=positions,
                hidden_states=hidden_states,
                residual=residual,
                forward_batch=forward_batch,
            )

        if not self.pp_group.is_last_rank:
            return PPProxyTensors({"hidden_states": hidden_states, "residual": residual})
        else:
            if hidden_states.shape[0] != 0:
                if residual is None:
                    hidden_states = self.norm(hidden_states)
                else:
                    hidden_states, _ = self.norm(hidden_states, residual)
            return hidden_states

    # --- Patch Qwen3NextForCausalLM.__init__ (remove single-rank assert) ---
    def _pp_for_causal_init(
        self,
        config: Qwen3NextConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = "",
    ):
        nn.Module.__init__(self)
        self.config = config
        self.pp_group = get_pp_group()
        self.quant_config = quant_config
        self.model = m.Qwen3NextModel(config, quant_config, prefix=add_prefix("model", prefix))
        self.lm_head = ParallelLMHead(
            config.vocab_size,
            config.hidden_size,
            quant_config=quant_config,
            org_num_embeddings=config.vocab_size,
            prefix=add_prefix("lm_head", prefix),
            use_attn_tp_group=get_global_server_args().enable_dp_lm_head,
        ).float()
        self.logits_processor = m.LogitsProcessor(config)

    # --- Patch Qwen3NextForCausalLM.forward ---
    @torch.no_grad()
    def _pp_for_causal_forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch,
        inputs_embeds: Optional[torch.Tensor] = None,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
        **kwargs,
    ):
        hidden_states = self.model(
            input_ids,
            positions,
            forward_batch,
            inputs_embeds,
            pp_proxy_tensors,
        )
        if self.pp_group.is_last_rank:
            return self.logits_processor(input_ids, hidden_states, self.lm_head, forward_batch)
        else:
            return hidden_states

    # --- Patch Qwen3NextForCausalLM.load_weights (filter by PP slice) ---
    orig_load_weights = m.Qwen3NextForCausalLM.load_weights

    def _pp_load_weights(
        self, weights: Iterable[Tuple[str, torch.Tensor]], is_mtp: bool = False
    ) -> Set[str]:
        """Filter incoming weights to only those relevant for this PP slice.

        Rules:
        - Layer weights: keep only if layer_id in [start, end).
        - Non-layer weights (layer_id is None):
            * keep if they correspond to names present in current params_dict (e.g., model.norm on last rank,
              embed on first rank, lm_head on all ranks), or
            * keep if they match known mapping keywords (so original loader can rename and resolve), or
            * keep if they are explicitly skipped by original loader (e.g., rotary_emb.inv_freq), harmless to pass.
        This prevents KeyError like 'model.norm.weight' on non-last ranks where norm is a PPMissingLayer.
        """
        start = getattr(self.model, "start_layer", None)
        end = getattr(self.model, "end_layer", None)
        params_dict = dict(self.named_parameters())
        mapping_keywords = (
            "q_proj",
            "k_proj",
            "v_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
            "self_attn",
        )

        filtered: list[tuple[str, torch.Tensor]] = []
        for name, w in weights:
            layer_id = get_layer_id(name)
            if layer_id is not None:
                if (start is None) or (start <= layer_id < end):
                    filtered.append((name, w))
            else:
                if (
                    (name in params_dict)
                    or any(k in name for k in mapping_keywords)
                    or ("rotary_emb.inv_freq" in name)
                ):
                    filtered.append((name, w))

        return orig_load_weights(self, filtered, is_mtp=is_mtp)

    # Bind patches
    m.Qwen3NextModel.__init__ = _pp_model_init  # type: ignore
    m.Qwen3NextModel.forward = _pp_model_forward  # type: ignore
    m.Qwen3NextForCausalLM.__init__ = _pp_for_causal_init  # type: ignore
    m.Qwen3NextForCausalLM.forward = _pp_for_causal_forward  # type: ignore
    m.Qwen3NextForCausalLM.load_weights = _pp_load_weights  # type: ignore


================================================================================
File: src/parallax/sglang/monkey_patch_utils/triton_backend.py
Size: 5.03 kB
================================================================================

from typing import Optional

import torch
from sglang.srt.layers.attention.triton_backend import TritonAttnBackend
from sglang.srt.layers.dp_attention import get_attention_tp_size
from sglang.srt.model_executor.model_runner import ModelRunner
from sglang.srt.utils import get_bool_env_var, get_device_core_count, get_int_env_var


def parallax_triton_backend_init(
    self,
    model_runner: ModelRunner,
    skip_prefill: bool = False,
    kv_indptr_buf: Optional[torch.Tensor] = None,
):
    # Lazy import to avoid the initialization of cuda context
    from sglang.srt.layers.attention.triton_ops.decode_attention import (
        decode_attention_fwd,
    )
    from sglang.srt.layers.attention.triton_ops.extend_attention import (
        extend_attention_fwd,
    )

    self.decode_attention_fwd = torch.compiler.disable(decode_attention_fwd)
    self.extend_attention_fwd = torch.compiler.disable(extend_attention_fwd)

    # Parse args
    self.skip_prefill = skip_prefill
    max_bs = model_runner.req_to_token_pool.size
    self.sliding_window_size = model_runner.sliding_window_size
    self.req_to_token = model_runner.req_to_token_pool.req_to_token
    self.token_to_kv_pool_allocator = model_runner.token_to_kv_pool_allocator
    self.num_draft_tokens = model_runner.server_args.speculative_num_draft_tokens
    self.speculative_num_steps = model_runner.server_args.speculative_num_steps
    self.num_head = model_runner.model_config.num_attention_heads // get_attention_tp_size()
    self.num_kv_head = model_runner.model_config.get_num_kv_heads(get_attention_tp_size())
    # Modifies layer id to support pipeline parallel
    if model_runner.hybrid_gdn_config is not None:
        # For hybrid linear models, layer_id = 0 may not be full attention
        self.v_head_dim = model_runner.token_to_kv_pool.get_v_head_dim()
    else:

        ################################################################################
        ## Patch for PP: get pp_start_layer
        self.v_head_dim = model_runner.token_to_kv_pool.get_value_buffer(
            model_runner.pp_start_layer
        ).shape[-1]
        ## End of patch
        ################################################################################
    self.max_context_len = model_runner.model_config.context_len
    self.device = model_runner.device
    self.device_core_count = get_device_core_count(model_runner.gpu_id)
    self.static_kv_splits = get_bool_env_var("SGLANG_TRITON_DECODE_ATTN_STATIC_KV_SPLITS", "false")
    self.max_kv_splits = model_runner.server_args.triton_attention_num_kv_splits

    # Decide whether enable deterministic inference with batch-invariant operations
    self.enable_deterministic = model_runner.server_args.enable_deterministic_inference

    # Configure deterministic inference settings
    if self.enable_deterministic:
        # Use fixed split tile size for batch invariance
        self.split_tile_size = get_int_env_var("SGLANG_TRITON_DECODE_SPLIT_TILE_SIZE", 256)
        # Set static_kv_splits to False to use deterministic logic instead
        self.static_kv_splits = False
    else:
        self.split_tile_size = model_runner.server_args.triton_attention_split_tile_size

    if self.split_tile_size is not None:
        self.max_kv_splits = (
            self.max_context_len + self.split_tile_size - 1
        ) // self.split_tile_size
    # Check arguments
    assert not (
        model_runner.sliding_window_size is not None
        and model_runner.model_config.is_encoder_decoder
    ), "Sliding window and cross attention are not supported together"

    # Initialize buffers
    # TODO(Jianan Ji): Make sure it behaves as expected when kv_indptr_buf is provided and sliding window is enabled
    if kv_indptr_buf is None:
        self.kv_indptr = torch.zeros((max_bs + 1,), dtype=torch.int32, device=model_runner.device)
    else:
        self.kv_indptr = kv_indptr_buf

    # If sliding window is enabled, we might need two sets of buffers
    # because of interleaved attention types (e.g. for Gemma3)
    self.window_kv_indptr = None
    if self.sliding_window_size is not None and self.sliding_window_size > 0:
        if kv_indptr_buf is None:
            self.window_kv_indptr = torch.zeros(
                (max_bs + 1,), dtype=torch.int32, device=model_runner.device
            )
        else:
            # When provided a buffer, create a clone for the second buffer
            self.window_kv_indptr = torch.zeros_like(kv_indptr_buf)

    if not self.skip_prefill:
        self.qo_indptr = torch.zeros((max_bs + 1,), dtype=torch.int32, device=model_runner.device)

        self.mask_indptr = torch.zeros((max_bs + 1,), dtype=torch.int64, device=model_runner.device)

    # Initialize forward metadata
    from sglang.srt.layers.attention.triton_backend import ForwardMetadata

    self.forward_metadata: ForwardMetadata = None

    self.cuda_graph_custom_mask = None


def apply_triton_backend_init_monkey_patch():
    TritonAttnBackend.__init__ = parallax_triton_backend_init


================================================================================
File: src/parallax/sglang/monkey_patch_utils/weight_loader_filter.py
Size: 2.19 kB
================================================================================

import logging
from pathlib import Path
from typing import List

from parallax.utils.weight_filter_utils import (
    filter_weight_files_by_layer_range_for_load,
)

logger = logging.getLogger(__name__)

_layer_range_cache = {}


def set_layer_range_for_filtering(pp_start_layer: int, pp_end_layer: int, num_hidden_layers: int):
    global _layer_range_cache
    _layer_range_cache["pp_start_layer"] = pp_start_layer
    _layer_range_cache["pp_end_layer"] = pp_end_layer
    _layer_range_cache["num_hidden_layers"] = num_hidden_layers


def _filter_weight_files_by_cache(hf_weights_files: List[str]) -> List[str]:
    global _layer_range_cache

    pp_start_layer = _layer_range_cache.get("pp_start_layer")
    pp_end_layer = _layer_range_cache.get("pp_end_layer")
    num_hidden_layers = _layer_range_cache.get("num_hidden_layers")

    if pp_start_layer is None or pp_end_layer is None:
        logger.debug("No layer range set, loading all weight files")
        return hf_weights_files

    if not hf_weights_files:
        return hf_weights_files

    model_path = Path(hf_weights_files[0]).parent
    is_first_shard = pp_start_layer == 0
    is_last_shard = pp_end_layer >= num_hidden_layers

    filtered_files = filter_weight_files_by_layer_range_for_load(
        model_path=model_path,
        weight_files=hf_weights_files,
        start_layer=pp_start_layer,
        end_layer=pp_end_layer,
        is_first_shard=is_first_shard,
        is_last_shard=is_last_shard,
    )

    return filtered_files


def apply_weight_loader_filter_patch():
    import glob as glob_module

    original_glob = glob_module.glob

    def patched_glob(pathname, **kwargs):
        files = original_glob(pathname, **kwargs)
        if (
            isinstance(files, list)
            and files
            and any(f.endswith((".safetensors", ".bin", ".pt")) for f in files)
        ):

            # Filter if we have layer range set
            global _layer_range_cache
            if _layer_range_cache.get("pp_start_layer") is not None:
                filtered = _filter_weight_files_by_cache(files)
                return filtered

        return files

    glob_module.glob = patched_glob


================================================================================
File: src/parallax/utils/__init__.py
Size: 0 B
================================================================================



================================================================================
File: src/parallax/utils/selective_download.py
Size: 6.5 kB
================================================================================

import inspect
import logging
import os
from pathlib import Path
from typing import Optional

from huggingface_hub import HfApi, hf_hub_download, snapshot_download

logger = logging.getLogger(__name__)
from parallax.utils.weight_filter_utils import (
    determine_needed_weight_files_for_download,
)

# Monkey patch HfApi.repo_info to add short timeout for faster failure on network issues
# This prevents snapshot_download from hanging silently when Hugging Face Hub is unreachable
_original_repo_info = HfApi.repo_info
_REPO_INFO_TIMEOUT = float(os.environ.get("PARALLAX_HF_REPO_INFO_TIMEOUT", "5.0"))


def _repo_info_with_timeout(self, repo_id, repo_type=None, revision=None, **kwargs):
    """Wrapper for HfApi.repo_info that injects a short timeout if not provided."""
    if "timeout" not in kwargs:
        kwargs["timeout"] = _REPO_INFO_TIMEOUT
        logger.debug(f"Injecting timeout={_REPO_INFO_TIMEOUT}s for repo_info call to {repo_id}")
    return _original_repo_info(
        self, repo_id=repo_id, repo_type=repo_type, revision=revision, **kwargs
    )


# Only apply monkey patch if repo_info accepts timeout parameter
_repo_info_signature = inspect.signature(_original_repo_info)
if "timeout" in _repo_info_signature.parameters or "kwargs" in str(_repo_info_signature):
    HfApi.repo_info = _repo_info_with_timeout
    logger.debug(f"Applied monkey patch to HfApi.repo_info with timeout={_REPO_INFO_TIMEOUT}s")
else:
    logger.warning(
        "HfApi.repo_info does not accept 'timeout' parameter - monkey patch skipped. "
        "Network timeout issues may still occur."
    )

EXCLUDE_WEIGHT_PATTERNS = [
    "*.safetensors",
    "*.bin",
    "*.pt",
    "*.pth",
    "pytorch_model*.bin",
    "model*.safetensors",
    "weight*.safetensors",
]


def download_metadata_only(
    repo_id: str,
    cache_dir: Optional[str] = None,
    force_download: bool = False,
    local_files_only: bool = False,
) -> Path:
    # If a local path is provided, return it directly without contacting HF Hub
    local_path = Path(repo_id)
    if local_path.exists():
        return local_path

    path = snapshot_download(
        repo_id=repo_id,
        cache_dir=cache_dir,
        ignore_patterns=EXCLUDE_WEIGHT_PATTERNS,
        force_download=force_download,
        local_files_only=local_files_only,
    )
    return Path(path)


def selective_model_download(
    repo_id: str,
    start_layer: Optional[int] = None,
    end_layer: Optional[int] = None,
    cache_dir: Optional[str] = None,
    force_download: bool = False,
    local_files_only: bool = False,
) -> Path:
    # Handle local model directory
    local_path = Path(repo_id)
    if local_path.exists():
        model_path = local_path
        logger.debug(f"Using local model path: {model_path}")
        is_remote = False
    else:
        logger.debug(f"Downloading model metadata for {repo_id}")
        model_path = download_metadata_only(
            repo_id=repo_id,
            cache_dir=cache_dir,
            force_download=force_download,
            local_files_only=local_files_only,
        )
        logger.debug(f"Downloaded model metadata to {model_path}")
        is_remote = True

    if start_layer is not None and end_layer is not None:
        logger.debug(f"Determining required weight files for layers [{start_layer}, {end_layer})")

        needed_weight_files = determine_needed_weight_files_for_download(
            model_path=model_path,
            start_layer=start_layer,
            end_layer=end_layer,
        )

        if is_remote:
            if not needed_weight_files:
                logger.debug("Could not determine specific weight files, downloading all")
                snapshot_download(
                    repo_id=repo_id,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    local_files_only=local_files_only,
                )
            else:
                # Step 3: Download only the needed weight files
                logger.info(f"Downloading {len(needed_weight_files)} weight files")

                for weight_file in needed_weight_files:
                    # Check if file already exists in local cache before downloading
                    weight_file_path = model_path / weight_file
                    if weight_file_path.exists():
                        logger.debug(
                            f"Weight file {weight_file} already exists locally, skipping download"
                        )
                        continue

                    logger.debug(f"Downloading {weight_file}")
                    try:
                        hf_hub_download(
                            repo_id=repo_id,
                            filename=weight_file,
                            cache_dir=cache_dir,
                            force_download=force_download,
                            local_files_only=local_files_only,
                        )
                    except Exception as e:
                        logger.error(f"Failed to download {weight_file} for {repo_id}: {e}")
                        logger.error(
                            "This node cannot reach Hugging Face Hub to download weight files. "
                            "Please check network connectivity or pre-download the model."
                        )
                        raise

                logger.debug(f"Downloaded weight files for layers [{start_layer}, {end_layer})")
        else:
            # Local path: skip any downloads
            logger.debug("Local model path detected; skipping remote weight downloads")
    else:
        # No layer range specified
        if is_remote:
            logger.debug("No layer range specified, downloading all model files")
            snapshot_download(
                repo_id=repo_id,
                cache_dir=cache_dir,
                force_download=force_download,
                local_files_only=local_files_only,
            )
        else:
            logger.debug("No layer range specified and using local path; nothing to download")

    return model_path


def get_model_path_with_selective_download(
    model_path_or_repo: str,
    start_layer: Optional[int] = None,
    end_layer: Optional[int] = None,
    local_files_only: bool = False,
) -> Path:
    return selective_model_download(
        repo_id=model_path_or_repo,
        start_layer=start_layer,
        end_layer=end_layer,
        local_files_only=local_files_only,
    )


================================================================================
File: src/parallax/utils/shared_state.py
Size: 5.93 kB
================================================================================

"""
Inter-process communication utilities using multiprocessing.Manager().

Provides a clean abstraction for sharing state between processes (executor, P2P server, etc.)
with dict-like interface and get/set methods.
"""

from __future__ import annotations

import multiprocessing
import time
from typing import Any, Dict, Optional, Union


class SharedState:
    """Wrapper for multiprocessing.Manager().dict() with dict-like interface.

    Supports both dict-like access (shared_state['key']) and method access (shared_state.get('key')).
    Automatically handles conversion from dict to SharedState.
    """

    def __init__(self, manager_dict: Optional[Union[Dict[str, Any], "SharedState"]] = None):
        """Initialize SharedState with a Manager().dict(), dict, SharedState, or None.

        Args:
            manager_dict: A Manager().dict(), regular dict, SharedState instance, or None.
                         If None, creates a new Manager().dict().
                         If dict, wraps it (assumes it's a Manager().dict()).
                         If SharedState, uses its underlying dict.
        """
        if manager_dict is None:
            manager = multiprocessing.Manager()
            self._dict = manager.dict()
        elif isinstance(manager_dict, SharedState):
            self._dict = manager_dict._dict
        else:
            self._dict = manager_dict

    def get(self, key: str, default: Any = None) -> Any:
        """Get a value from shared state."""
        return self._dict.get(key, default)

    def set(self, key: str, value: Any) -> None:
        """Set a value in shared state."""
        self._dict[key] = value

    def update(self, **kwargs) -> None:
        """Batch update multiple values in shared state.

        Args:
            **kwargs: Key-value pairs to update.
        """
        for key, value in kwargs.items():
            self._dict[key] = value

    def __getitem__(self, key: str) -> Any:
        """Dict-like access: shared_state['key']"""
        return self._dict[key]

    def __setitem__(self, key: str, value: Any) -> None:
        """Dict-like access: shared_state['key'] = value"""
        self._dict[key] = value

    def __contains__(self, key: str) -> bool:
        """Check if key exists: 'key' in shared_state"""
        return key in self._dict

    @property
    def dict(self) -> Dict[str, Any]:
        """Get the underlying Manager().dict() for multiprocessing serialization."""
        return self._dict

    def get_metrics(self) -> Dict[str, Any]:
        """Get a shallow copy of current metrics suitable for JSON serialization."""
        metrics_dict = self._dict.get("metrics")
        if not metrics_dict:
            return {}
        # For Manager().dict(), create a copy by accessing each key explicitly
        return {k: metrics_dict[k] for k in metrics_dict.keys()}

    def update_metrics(
        self,
        *,
        current_requests: Optional[int] = None,
        layer_latency_ms_sample: Optional[float] = None,
        ewma_alpha: float = 0.2,
    ) -> None:
        """Update metrics with optional fields and EWMA smoothing for latency.

        Args:
            current_requests: Number of in-flight requests on this node.
            layer_latency_ms_sample: A new sample of per-layer latency in ms.
            ewma_alpha: Smoothing factor in [0, 1] for latency EWMA.
        """
        metrics_dict = self._dict.get("metrics")
        if not metrics_dict:
            raise RuntimeError("metrics not initialized in shared_state")

        # Update metrics
        if current_requests is not None:
            metrics_dict["current_requests"] = int(current_requests)
        if layer_latency_ms_sample is not None:
            prev = metrics_dict.get("layer_latency_ms")
            if prev is None:
                metrics_dict["layer_latency_ms"] = float(layer_latency_ms_sample)
            else:
                metrics_dict["layer_latency_ms"] = float(
                    (1.0 - ewma_alpha) * float(prev) + ewma_alpha * float(layer_latency_ms_sample)
                )
        metrics_dict["_last_update_ts"] = time.time()

    def get_model_info(self) -> Dict[str, Any]:
        """Get model and layer allocation information."""
        return {
            "model_name": self._dict.get("model_name"),
            "block_start_index": self._dict.get("block_start_index"),
            "block_end_index": self._dict.get("block_end_index"),
            "tp_size": self._dict.get("tp_size"),
            "_layer_allocation_changed": self._dict.get("_layer_allocation_changed", False),
        }

    def get_layer_allocation_changed(self) -> bool:
        """Check if layer allocation has changed."""
        return self._dict.get("_layer_allocation_changed", False)

    def get_status(self) -> Optional[str]:
        """Get current status."""
        return self._dict.get("status")

    def set_status(self, status: str) -> None:
        """Set current status."""
        self._dict["status"] = status

    @classmethod
    def create(cls) -> "SharedState":
        """Create a new SharedState with default initialization.

        Returns:
            A new SharedState instance with initialized default values.
        """
        manager = multiprocessing.Manager()
        shared_dict = manager.dict()

        # Initialize default values
        shared_dict["block_start_index"] = None
        shared_dict["block_end_index"] = None
        shared_dict["model_name"] = None
        shared_dict["tp_size"] = None
        shared_dict["_layer_allocation_changed"] = False
        shared_dict["status"] = None

        # Create nested shared dict for metrics
        shared_dict["metrics"] = manager.dict()
        shared_dict["metrics"]["current_requests"] = 0
        shared_dict["metrics"]["layer_latency_ms"] = None
        shared_dict["metrics"]["_last_update_ts"] = 0.0

        return cls(shared_dict)


================================================================================
File: src/parallax/utils/tokenizer_utils.py
Size: 4.46 kB
================================================================================

"""
Implements parallax detokenizers for performance.
"""

import json
from functools import partial
from json import JSONDecodeError

from mlx_lm.tokenizer_utils import (
    BPEStreamingDetokenizer,
    NaiveStreamingDetokenizer,
    SPMStreamingDetokenizer,
    _is_bpe_decoder,
    _is_spm_decoder,
    _is_spm_decoder_no_space,
)
from mlx_lm.tokenizer_utils import load_tokenizer as _mlx_load_tokenizer


class ParallaxNaiveStreamingDetokenizer(NaiveStreamingDetokenizer):
    """A custom BPE streaming detokenizer that add an argument 'tokenizer'"""

    def __init__(self, tokenizer, tokenmap):
        self._tokenizer = tokenizer
        self._tokenizer.decode([0])
        self.reset()


class ParallaxBPEStreamingDetokenizer(BPEStreamingDetokenizer):
    """A custom BPE streaming detokenizer that skips initializing tokenmap"""

    def __init__(self, tokenizer, tokenmap):
        self.clean_spaces = tokenizer.clean_up_tokenization_spaces
        self.tokenmap = tokenmap
        self.reset()
        self.make_byte_decoder()


class ParallaxSPMStreamingDetokenizer(SPMStreamingDetokenizer):
    """A custom SPM streaming detokenizer that skips initializing tokenmap"""

    def __init__(self, tokenizer, tokenmap, trim_space=True):
        self.trim_space = trim_space
        self._sep = "\u2581".encode()
        self.tokenmap = tokenmap
        self.reset()


def _get_spm_tokenmap(tokenizer):
    """Initialize spm tokenmap for reuse"""
    # Extract the tokens in a list from id to text
    tokenmap = [""] * (max(tokenizer.vocab.values()) + 1)
    for value, tokenid in tokenizer.vocab.items():
        if value.startswith("<0x"):
            # Replace bytes with their value
            tokenmap[tokenid] = bytes([int(value[3:5], 16)])
        else:
            tokenmap[tokenid] = value.encode()
    return tokenmap


def _get_bpe_tokenmap(tokenizer):
    """Initialize bpe tokenmap for reuse"""
    # Extract the tokens in a list from id to text
    tokenmap = [None] * len(tokenizer.vocab)
    for value, tokenid in tokenizer.vocab.items():
        tokenmap[tokenid] = value
    return tokenmap


def load_detokenizer(model_path, tokenizer):
    """Load a huggingface tokenizer and try to infer the type of streaming
    detokenizer to use.

    Note, to use a fast streaming tokenizer, pass a local file path rather than
    a Hugging Face repo ID.
    """
    detokenizer_class = ParallaxNaiveStreamingDetokenizer
    tokenmap = None

    tokenizer_file = model_path / "tokenizer.json"
    if tokenizer_file.exists():
        with open(tokenizer_file, "r", encoding="utf-8") as fid:
            try:
                tokenizer_content = json.load(fid)
            except JSONDecodeError as e:
                raise JSONDecodeError("Failed to parse tokenizer.json", e.doc, e.pos)

        if "decoder" in tokenizer_content:
            if _is_spm_decoder(tokenizer_content["decoder"]):
                detokenizer_class = ParallaxSPMStreamingDetokenizer
                tokenmap = _get_spm_tokenmap(tokenizer)
            elif _is_spm_decoder_no_space(tokenizer_content["decoder"]):
                detokenizer_class = partial(ParallaxSPMStreamingDetokenizer, trim_space=False)
                tokenmap = _get_spm_tokenmap(tokenizer)
            elif _is_bpe_decoder(tokenizer_content["decoder"]):
                detokenizer_class = ParallaxBPEStreamingDetokenizer
                tokenmap = _get_bpe_tokenmap(tokenizer)

    return detokenizer_class, tokenmap


def load_tokenizer(model_path, trust_remote_code=True, tokenizer_config_extra=None, **kwargs):
    """
    Wrapper function for MLX load_tokenizer that defaults trust_remote_code to True.
    This is needed for models like Kimi-K2 that contain custom code.

    Args:
        model_path: Path to the model
        trust_remote_code: Whether to trust remote code (defaults to True)
        tokenizer_config_extra: Extra config to pass to AutoTokenizer.from_pretrained
        **kwargs: Additional arguments to pass to the original load_tokenizer

    Returns:
        The loaded tokenizer
    """
    if tokenizer_config_extra is None:
        tokenizer_config_extra = {}

    # Add trust_remote_code to the tokenizer config
    if trust_remote_code:
        tokenizer_config_extra = tokenizer_config_extra.copy()
        tokenizer_config_extra["trust_remote_code"] = True

    return _mlx_load_tokenizer(model_path, tokenizer_config_extra=tokenizer_config_extra, **kwargs)


================================================================================
File: src/parallax/utils/utils.py
Size: 9.84 kB
================================================================================

"""Utility functions."""

import random
import socket
from typing import List

import mlx.core as mx
import numpy as np
import psutil
import torch
import zmq
from mlx_lm.utils import get_model_path, load_config


def is_cuda_available():
    """Check backend supports cuda"""
    return torch.cuda.is_available()


def is_mps_available():
    """Check backend supports mps"""
    return torch.mps.is_available()


def get_current_device():
    """
    Returns the backend device name.
    Parallax currently supports cuda, mlx, cpu
    """
    device = "cpu"
    if is_cuda_available():
        device = "cuda"
    if is_mps_available():
        device = "mlx"
    return device


def get_device_dtype(dtype_str: str, device: str):
    """Gets the real data type according to current device"""
    if device == "cuda":
        dtype_map = {
            "float16": torch.float16,
            "bfloat16": torch.bfloat16,
            "float32": torch.float32,
        }
    else:
        dtype_map = {
            "float16": mx.float16,
            "bfloat16": mx.bfloat16,
            "float32": mx.float32,
        }
    return dtype_map[dtype_str]


def get_zmq_socket(context: zmq.Context, socket_type: zmq.SocketType, endpoint: str, bind: bool):
    """Create and configure a ZeroMQ socket.

    Ported from SGLang.
    """
    mem = psutil.virtual_memory()
    total_mem = mem.total / 1024**3
    available_mem = mem.available / 1024**3
    if total_mem > 32 and available_mem > 16:
        buf_size = int(0.5 * 1024**3)
    else:
        buf_size = -1

    socket = context.socket(socket_type)
    if endpoint.find("[") != -1:
        socket.setsockopt(zmq.IPV6, 1)

    def set_send_opt():
        socket.setsockopt(zmq.SNDHWM, 0)
        socket.setsockopt(zmq.SNDBUF, buf_size)

    def set_recv_opt():
        socket.setsockopt(zmq.RCVHWM, 0)
        socket.setsockopt(zmq.RCVBUF, buf_size)

    if socket_type == zmq.PUSH:
        set_send_opt()
    elif socket_type == zmq.PULL:
        set_recv_opt()
    elif socket_type == zmq.DEALER:
        set_send_opt()
        set_recv_opt()
    else:
        raise ValueError(f"Unsupported socket type: {socket_type}")

    if bind:
        socket.bind(endpoint)
    else:
        socket.connect(endpoint)

    return socket


def get_infinite_value_by_dtype(dtype: mx.Dtype):
    """Returns infinite value according to mx dtype"""
    inf = 6e4
    if dtype in (mx.bfloat16, mx.float32):
        inf = 1e9
    return inf


def pad_prefix_caches(
    cache: List, input_lengths: List, dtype: mx.Dtype = mx.bfloat16
) -> tuple[mx.array, mx.array]:
    """
    Pads prefix kv caches.

    Returnas:
        - mx.array: The padded batch of caches with a shape of [B, max_input_seq_len].
        - mx.array: The corresponding 4D k mask with a shape of [B, 1, 1, max_output_seq_len].
    """
    caches_mx = [mx.array(i) if isinstance(i, np.ndarray) else i for i in cache]

    seq_len_axis = 2
    max_input_len = 0
    max_output_len = 0
    for i, tensor in enumerate(caches_mx):
        max_input_len = max(max_input_len, tensor.shape[seq_len_axis])
        max_output_len = max(max_output_len, input_lengths[i])

    padded_tensors = []
    k_masks = []
    for i, tensor in enumerate(caches_mx):
        cache_len = tensor.shape[seq_len_axis]
        num_kv_padding = max_input_len - cache_len
        input_seq_len = input_lengths[i] - 1
        num_mask_padding = max_output_len - input_seq_len - 1

        if num_kv_padding > 0:
            pad_shape = list(tensor.shape)
            pad_shape[seq_len_axis] = num_kv_padding
            padding = mx.zeros(tuple(pad_shape), dtype=tensor.dtype)
            padded_tensors.append(mx.concatenate([tensor, padding], axis=seq_len_axis))
        else:
            padded_tensors.append(tensor)

        k_masks.append([1] * (input_seq_len + 1) + [0] * num_mask_padding)

    padded_batch = mx.stack(padded_tensors, axis=0)
    attention_mask = mx.array(k_masks, dtype=dtype)[:, None, None, :]
    return padded_batch, attention_mask


def pad_inputs(
    pad_value: int, inputs: List, dtype: mx.Dtype = mx.bfloat16
) -> tuple[mx.array, mx.array]:
    """
    Pads a list of sequences (token ID lists or hidden state arrays) to the same length.
    # TODO: refactor this allow cumstomized dim.

    Args:
        pad_value: The value to use for padding. For token IDs, this should be the
                   tokenizer's pad_token_id. For hidden states, it's ignored (always 0).
        inputs: A list of sequences to pad. Each sequence can be a list of integers
                or an MLX/NumPy array of hidden states.
        dtype: The data type for the padded inputs.

    Returns:
        A tuple containing:
        - mx.array: The padded batch of inputs.
        - mx.array: The corresponding 4D attention mask.
    """
    if not inputs:
        return mx.array([]), mx.array([])

    max_len = 0
    attention_masks = []

    # Check the dimensionality of the input to handle KV cache padding
    is_kv_cache = isinstance(inputs[0], mx.array) and inputs[0].ndim == 4

    if isinstance(inputs[0], list):  # Assuming list of token IDs
        for tokens in inputs:
            max_len = max(max_len, len(tokens))

        padded_sequences = []
        for tokens in inputs:
            num_padding = max_len - len(tokens)
            padded_sequences.append(tokens + [pad_value] * num_padding)
            attention_masks.append([1] * len(tokens) + [0] * num_padding)

        padded_batch = mx.array(padded_sequences)

    elif isinstance(
        inputs[0], (mx.array, np.ndarray)
    ):  # Assuming list of hidden states or KV caches
        inputs_mx = [mx.array(i) if isinstance(i, np.ndarray) else i for i in inputs]

        # Determine sequence length axis based on input type
        # kv cache: (n_layers, n_kv_h, source_len, h_dim)
        seq_len_axis = 2 if is_kv_cache else 0
        for tensor in inputs_mx:
            max_len = max(max_len, tensor.shape[seq_len_axis])

        padded_tensors = []
        for tensor in inputs_mx:
            seq_len = tensor.shape[seq_len_axis]
            num_padding = max_len - seq_len

            if num_padding > 0:
                if is_kv_cache:
                    pad_shape = list(tensor.shape)
                    pad_shape[seq_len_axis] = num_padding
                    padding = mx.zeros(tuple(pad_shape), dtype=tensor.dtype)
                else:
                    # Hidden state shape: (seq_len, hidden_dim)
                    hidden_dim = tensor.shape[1]
                    padding = mx.zeros((num_padding, hidden_dim), dtype=tensor.dtype)
                padded_tensors.append(mx.concatenate([tensor, padding], axis=seq_len_axis))
            else:
                padded_tensors.append(tensor)
            attention_masks.append([1] * seq_len + [0] * num_padding)

        padded_batch = mx.stack(padded_tensors, axis=0)

    else:
        raise TypeError(f"Unsupported input type for padding: {type(inputs[0])}")

    # Create 4D attention mask, ensuring it's float
    attention_mask = mx.array(attention_masks, dtype=dtype)[:, None, None, :]
    return padded_batch, attention_mask


def create_causal_mask(seq_len: int, total_len: int, dtype=mx.bfloat16) -> mx.array:
    """
    Creates a causal attention mask of shape (input_seq, total_seq).

    Args:
        input_seq: The length of sequence.
        total_seq: The length of sequence + cached sequence.
        dtype: The data type for the mask.

    Returns:
        mx.array: A square matrix with -1e9 on the upper triangle (excluding the diagonal).
    """
    assert (
        total_len >= seq_len
    ), f"Total lengths {total_len} should be no less than input sequence {seq_len}."
    inf_value = get_infinite_value_by_dtype(dtype)
    mask = mx.triu(mx.full((seq_len, seq_len), -inf_value, dtype), k=1)
    if total_len == seq_len:
        return mask
    # total lengths is larger than input sequence length
    cached_zeros = mx.zeros((seq_len, total_len - seq_len), dtype)
    final_mask = mx.concatenate([cached_zeros, mask], axis=1)
    return final_mask


def combine_padding_and_causal_masks(
    padding_mask: mx.array, causal_mask: mx.array, dtype=mx.bfloat16
) -> mx.array:
    """
    Combines a padding mask and a causal mask.

    Args:
        padding_mask: A 4D padding mask of shape (B, 1, 1, total_seq)
                      where masked positions are 0 and unmasked are 1.
        causal_mask: A 2D causal mask of shape (input_seq, total_seq).
        dtype: The data type for the final mask.

    Returns:
        mx.array: A combined attention mask, typically of shape (B, 1, input_seq, total_seq).
    """
    inf_value = get_infinite_value_by_dtype(dtype)
    padding_mask_float = (padding_mask - 1) * inf_value
    padding_mask_float = padding_mask_float.astype(dtype)
    return causal_mask + padding_mask_float


def fetch_model_from_hf(name: str):
    """Fetch model from huggingface and returns model config"""
    model_path = get_model_path(name)[0]
    config = load_config(model_path)
    return config


def is_port_available(port: int):
    """
    Copied from SGLang.
    Return whether a port is available.
    """
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            s.bind(("", port))
            s.listen(1)
            return True
        except socket.error:
            return False
        except OverflowError:
            return False


def initialize_nccl_port():
    """Initialize nccl port for GPU"""
    nccl_port = random.randint(4000, 5000)
    while True:
        if is_port_available(nccl_port):
            break
        if nccl_port < 60000:
            nccl_port += 42
        else:
            nccl_port -= 43
    return nccl_port


================================================================================
File: src/parallax/utils/weight_filter_utils.py
Size: 5.29 kB
================================================================================

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Set

logger = logging.getLogger(__name__)


def should_include_weight_key(
    key: str,
    start_layer: int,
    end_layer: int,
    is_first_shard: bool,
    is_last_shard: bool,
    tie_word_embeddings: bool = False,
) -> bool:
    if is_first_shard and "embed_tokens" in key and key.startswith("model."):
        return True

    if is_last_shard:
        if "model.norm" in key or "lm_head" in key:
            return True
        if tie_word_embeddings and "embed" in key and key.startswith("model.embed_tokens"):
            return True

    if "layers." in key:
        parts = key.split(".")
        for i, part in enumerate(parts):
            if part == "layers" and i + 1 < len(parts):
                layer_idx = int(parts[i + 1])
                return start_layer <= layer_idx < end_layer

    return False


def filter_weight_files_by_layer_range_for_load(
    model_path: Path,
    weight_files: List[str],
    start_layer: int,
    end_layer: int,
    is_first_shard: bool,
    is_last_shard: bool,
    config: Optional[Dict] = None,
) -> List[str]:
    index_file = model_path / "model.safetensors.index.json"

    if not index_file.exists():
        logger.debug(f"No index file found at {index_file}, cannot filter weight files")
        return weight_files

    with open(index_file, "r") as f:
        index_data = json.load(f)

    weight_map = index_data.get("weight_map", {})
    if not weight_map:
        logger.debug("weight_map is empty in index file")
        return weight_files

    tie_word_embeddings = False
    if config:
        tie_word_embeddings = config.get("tie_word_embeddings", False)
    else:
        config_file = model_path / "config.json"
        if config_file.exists():
            with open(config_file, "r") as f:
                cfg = json.load(f)
                tie_word_embeddings = cfg.get("tie_word_embeddings", False)

    needed_files: Set[str] = set()

    for key, filename in weight_map.items():
        if filename in needed_files:
            continue
        if should_include_weight_key(
            key=key,
            start_layer=start_layer,
            end_layer=end_layer,
            is_first_shard=is_first_shard,
            is_last_shard=is_last_shard,
            tie_word_embeddings=tie_word_embeddings,
        ):
            needed_files.add(filename)

    if not needed_files:
        logger.debug(
            f"No relevant weight files found in index for layers [{start_layer}, {end_layer})"
        )
        return weight_files

    filtered_files = []
    for wf in weight_files:
        wf_name = Path(wf).name
        if wf_name in needed_files:
            filtered_files.append(wf)

    logger.debug(
        f"Filtered weight files from {len(weight_files)} to {len(filtered_files)} "
        f"for layers [{start_layer}, {end_layer})"
    )

    return filtered_files


def determine_needed_weight_files_for_download(
    model_path: Path,
    start_layer: int,
    end_layer: int,
    config: Optional[Dict] = None,
) -> List[str]:
    is_first_shard = start_layer == 0

    is_last_shard = False
    if config:
        num_hidden_layers = config.get("num_hidden_layers", 0)
        is_last_shard = end_layer >= num_hidden_layers
    else:
        config_file = model_path / "config.json"
        if config_file.exists():
            with open(config_file, "r") as f:
                cfg = json.load(f)
                num_hidden_layers = cfg.get("num_hidden_layers", 0)
                is_last_shard = end_layer >= num_hidden_layers

    index_file = model_path / "model.safetensors.index.json"

    if not index_file.exists():
        logger.debug(f"Index file not found at {index_file}, checking for single weight file")
        # For non-sharded models, look for single weight file
        single_weight_files = [
            "model.safetensors",
            "pytorch_model.bin",
            "model.bin",
        ]
        for weight_file in single_weight_files:
            if (model_path / weight_file).exists():
                logger.debug(f"Found single weight file: {weight_file}")
                return [weight_file]

        logger.debug("No weight files found (neither index nor single file)")
        return []

    with open(index_file, "r") as f:
        index_data = json.load(f)

    weight_map = index_data.get("weight_map", {})
    if not weight_map:
        logger.debug("weight_map is empty in index file")
        return []

    tie_word_embeddings = False
    if config:
        tie_word_embeddings = config.get("tie_word_embeddings", False)

    needed_files: Set[str] = set()

    for key, filename in weight_map.items():
        if filename in needed_files:
            continue
        if should_include_weight_key(
            key=key,
            start_layer=start_layer,
            end_layer=end_layer,
            is_first_shard=is_first_shard,
            is_last_shard=is_last_shard,
            tie_word_embeddings=tie_word_embeddings,
        ):
            needed_files.add(filename)

    result = sorted(list(needed_files))
    logger.debug(
        f"Determined {len(result)} weight files needed for layers [{start_layer}, {end_layer})"
    )
    return result


================================================================================
File: src/parallax/vllm/batch_info.py
Size: 14.12 kB
================================================================================

from __future__ import annotations

from typing import Any, Dict, List, Optional

import torch
from vllm.sampling_params import SamplingParams as VLLMSamplingParams
from vllm.sampling_params import StructuredOutputsParams
from vllm.sequence import IntermediateTensors
from vllm.v1.core.sched.output import CachedRequestData, NewRequestData, SchedulerOutput
from vllm.v1.request import Request as VLLMRequest

from parallax.server.request import Request
from parallax.server.sampling.sampling_params import (
    SamplingParams as ParallaxSamplingParams,
)
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


def compute_expected_intermediate_tokens(scheduler_output: Any, model_runner: Any) -> Optional[int]:
    """
    Estimate the padded token count expected by vLLM for this batch.

    This function computes the total number of tokens including padding that vLLM
    expects for data parallel processing.

    Args:
        scheduler_output: SchedulerOutput from vLLM scheduler
        model_runner: The vLLM model runner instance

    Returns:
        Expected total token count including padding, or None if unable to compute
    """
    if scheduler_output is None:
        return None

    total_tokens = getattr(scheduler_output, "total_num_scheduled_tokens", None)
    if total_tokens is None:
        return None

    try:
        total_tokens = int(total_tokens)
    except (TypeError, ValueError):
        return None

    if model_runner is None:
        return None

    get_num_input_tokens = getattr(model_runner, "_get_num_input_tokens", None)
    get_dp_padding = getattr(model_runner, "get_dp_padding", None)
    if get_num_input_tokens is None or get_dp_padding is None:
        return None

    num_input_tokens = get_num_input_tokens(total_tokens)
    num_pad, _ = get_dp_padding(num_input_tokens)
    return num_input_tokens + num_pad


def pad_or_trim_tensor(tensor: torch.Tensor, target_len: int) -> torch.Tensor:
    """
    Pad or trim a tensor to the target length along dimension 0.

    Args:
        tensor: Input tensor to pad/trim
        target_len: Target length for dimension 0. If negative, returns unchanged.

    Returns:
        Tensor with dimension 0 adjusted to target_len
    """
    if target_len < 0:
        return tensor
    current_len = tensor.shape[0]
    if current_len == target_len:
        return tensor
    if current_len > target_len:
        return tensor[:target_len]
    pad_shape = (target_len - current_len,) + tensor.shape[1:]
    pad = tensor.new_zeros(pad_shape)
    return torch.cat((tensor, pad), dim=0)


def resize_intermediate_tensors(
    intermediate_tensors: IntermediateTensors, target_len: Optional[int]
) -> IntermediateTensors:
    """
    Resize all tensors in IntermediateTensors to match the target length.

    This is needed for vLLM pipeline parallelism when the actual token count
    doesn't match the expected padded count for data parallel processing.

    Args:
        intermediate_tensors: vLLM IntermediateTensors containing hidden states
        target_len: Target token count. If None or negative, returns unchanged.

    Returns:
        IntermediateTensors with all tensors resized to target_len
    """
    if intermediate_tensors is None or target_len is None:
        return intermediate_tensors
    if target_len < 0:
        return intermediate_tensors

    # Create a list to avoid "dictionary changed size during iteration".
    for key, tensor in list(intermediate_tensors.items()):
        intermediate_tensors[key] = pad_or_trim_tensor(tensor, target_len)
    return intermediate_tensors


def transform_sampling_params_to_vllm(old_params: ParallaxSamplingParams) -> VLLMSamplingParams:
    structured = (
        StructuredOutputsParams(json=old_params.json_schema)
        if getattr(old_params, "json_schema", None) is not None
        else None
    )
    params = VLLMSamplingParams(
        max_tokens=old_params.max_new_tokens,
        min_tokens=old_params.min_new_tokens,
        temperature=old_params.temperature,
        top_p=old_params.top_p,
        min_p=old_params.min_p,
        top_k=old_params.top_k,
        stop_token_ids=(
            list(old_params.stop_token_ids)
            if getattr(old_params, "stop_token_ids", None) is not None
            else None
        ),
        ignore_eos=old_params.ignore_eos,
        stop=old_params.stop_strs,
        repetition_penalty=old_params.repetition_penalty,
        presence_penalty=old_params.presence_penalty,
        frequency_penalty=old_params.frequency_penalty,
        structured_outputs=structured,
    )
    return params


def _build_vllm_request(
    req: Request,
    sampling_params: VLLMSamplingParams,
    model_runner: Any,
    *,
    include_outputs: bool,
) -> VLLMRequest:
    block_hasher = getattr(model_runner, "request_block_hasher", None)
    vllm_req = VLLMRequest(
        request_id=req.request_id,
        prompt_token_ids=getattr(req, "input_ids", None),
        sampling_params=sampling_params,
        pooling_params=None,
        eos_token_id=getattr(req, "eos_token_id", None),
        arrival_time=getattr(req, "arrival_time", 0.0),
        block_hasher=block_hasher,
    )
    if include_outputs:
        output_ids = getattr(req, "output_ids", None) or []
        if output_ids:
            vllm_req.append_output_token_ids(output_ids)
    return vllm_req


def form_vllm_batch_prefill(
    batched_requests: List[Request],
    model_runner: Any = None,
) -> Optional[SchedulerOutput]:
    if not batched_requests:
        return None

    if not hasattr(model_runner, "kv_cache_manager"):
        raise RuntimeError(
            "model_runner must have kv_cache_manager initialized. "
            "Call model_runner.initialize_kv_cache_manager() first."
        )

    kv_cache_manager = model_runner.kv_cache_manager

    num_common_prefix_blocks = [0] * len(model_runner.kv_cache_config.kv_cache_groups)

    created_vllm_requests: List[VLLMRequest] = []

    new_request_data_list = []
    num_scheduled_tokens: Dict[str, int] = {}
    total_tokens = 0

    for req in batched_requests:
        sampling_params = transform_sampling_params_to_vllm(req.sampling_params)

        vllm_req = _build_vllm_request(req, sampling_params, model_runner, include_outputs=False)
        created_vllm_requests.append(vllm_req)

        computed_blocks, num_computed_tokens = kv_cache_manager.get_computed_blocks(vllm_req)

        prompt_token_ids = getattr(req, "input_ids", None) or []
        num_new_tokens = max(len(prompt_token_ids) - num_computed_tokens, 0)
        if num_new_tokens > 0:
            new_blocks = kv_cache_manager.allocate_slots(
                request=vllm_req,
                num_new_tokens=num_new_tokens,
                num_new_computed_tokens=num_computed_tokens,
                new_computed_blocks=computed_blocks if num_computed_tokens > 0 else None,
            )

            if new_blocks is None:
                logger.warning(f"Cannot allocate KV cache for request {req.request_id}")
                for prev_req in created_vllm_requests[:-1]:
                    kv_cache_manager.free(prev_req)
                return None

            all_blocks = computed_blocks + new_blocks if num_computed_tokens > 0 else new_blocks
        else:
            all_blocks = computed_blocks

        block_ids = all_blocks.get_block_ids()

        new_req_data = NewRequestData(
            req_id=req.request_id,
            prompt_token_ids=req.input_ids,
            mm_features=[],
            sampling_params=sampling_params,
            pooling_params=None,
            block_ids=block_ids,
            num_computed_tokens=num_computed_tokens,
            lora_request=None,
            prompt_embeds=None,
        )
        new_request_data_list.append(new_req_data)

        scheduled_tokens = len(prompt_token_ids)
        num_scheduled_tokens[req.request_id] = scheduled_tokens
        total_tokens += scheduled_tokens

    scheduler_output = SchedulerOutput(
        scheduled_new_reqs=new_request_data_list,
        scheduled_cached_reqs=CachedRequestData.make_empty(),
        num_scheduled_tokens=num_scheduled_tokens,
        total_num_scheduled_tokens=total_tokens,
        scheduled_spec_decode_tokens={},
        scheduled_encoder_inputs={},
        num_common_prefix_blocks=num_common_prefix_blocks,
        finished_req_ids=set(),
        free_encoder_mm_hashes=[],
        structured_output_request_ids=[],
        grammar_bitmask=None,
        kv_connector_metadata=None,
    )

    return scheduler_output


def form_vllm_batch_decode(
    batched_requests: List[Request],
    model_runner: Any = None,
    scheduler: Any = None,
    **kwargs,
) -> Optional[SchedulerOutput]:
    if not batched_requests:
        return None

    if not hasattr(model_runner, "kv_cache_manager"):
        raise RuntimeError(
            "model_runner must have kv_cache_manager initialized. "
            "Call model_runner.initialize_kv_cache_manager() first."
        )

    kv_cache_manager = model_runner.kv_cache_manager

    req_ids: List[str] = []
    resumed_from_preemption: List[bool] = []
    new_token_ids: List[List[int]] = []
    resumed_req_token_ids: List[List[int] | None] = []
    new_block_ids: List[tuple[List[int], ...] | None] = []
    num_computed_tokens: List[int] = []
    num_output_tokens: List[int] = []
    num_scheduled_tokens: Dict[str, int] = {}

    for req in batched_requests:
        req_ids.append(req.request_id)
        resumed_from_preemption.append(False)

        # For GPU workers (non-first peer), IntermediateRequest doesn't have output_ids
        # We need to get it from vLLM's CachedRequestState in model_runner
        output_ids = getattr(req, "output_ids", None) or []

        # If this request doesn't have output_ids (IntermediateRequest case),
        # try to get it from model_runner's cached request state (vLLM internal state)
        if not output_ids and hasattr(model_runner, "requests"):
            cached_req_state = model_runner.requests.get(req.request_id)
            if cached_req_state is not None:
                output_ids = getattr(cached_req_state, "output_token_ids", [])
                logger.debug(
                    f"[Decode] Retrieved output_token_ids from vLLM CachedRequestState for "
                    f"{req.request_id}: len={len(output_ids)}"
                )

        # Fallback: try scheduler if available
        if not output_ids and scheduler is not None:
            running_req = scheduler.get_running_request(req.request_id)
            if running_req is not None:
                output_ids = getattr(running_req, "output_ids", None) or []
                logger.debug(
                    f"[Decode] Retrieved output_ids from scheduler for {req.request_id}: "
                    f"len={len(output_ids)}"
                )

        if output_ids:
            last_token = output_ids[-1]
            new_token_ids.append([last_token])
        else:
            new_token_ids.append([])

        resumed_req_token_ids.append([])

        sampling_params = transform_sampling_params_to_vllm(req.sampling_params)
        vllm_req = _build_vllm_request(req, sampling_params, model_runner, include_outputs=True)

        prompt_ids = getattr(req, "input_ids", None) or []
        # For decode stage, computed_token_count should be the total number of tokens
        # that have been processed (including all output tokens).
        # In pipeline parallelism, this must match what GPU worker expects.
        if output_ids:
            # All tokens (prompt + all generated outputs) have been computed
            computed_token_count = len(prompt_ids) + len(output_ids) - 1
        else:
            # First decode step: only prompt has been computed
            computed_token_count = len(prompt_ids)
        vllm_req.num_computed_tokens = computed_token_count

        # Debug logging to track state synchronization
        logger.debug(
            f"[Decode] req_id={req.request_id}, prompt_len={len(prompt_ids)}, "
            f"output_len={len(output_ids)}, computed_tokens={computed_token_count}"
        )

        new_blocks = kv_cache_manager.allocate_slots(
            request=vllm_req,
            num_new_tokens=1,
            num_new_computed_tokens=0,
        )

        if new_blocks is None:
            logger.warning(f"Cannot allocate KV cache for decode request {req.request_id}")
            return None

        new_block_ids.append(new_blocks.get_block_ids(allow_none=True))
        num_computed_tokens.append(computed_token_count)
        num_output_tokens.append(len(output_ids))
        num_scheduled_tokens[req.request_id] = 1

    cached_req_data = CachedRequestData(
        req_ids=req_ids,
        resumed_from_preemption=resumed_from_preemption,
        new_token_ids=new_token_ids,
        new_block_ids=new_block_ids,
        num_computed_tokens=num_computed_tokens,
    )

    scheduler_output = SchedulerOutput(
        scheduled_new_reqs=[],
        scheduled_cached_reqs=cached_req_data,
        num_scheduled_tokens=num_scheduled_tokens,
        total_num_scheduled_tokens=sum(num_scheduled_tokens.values()),
        scheduled_spec_decode_tokens={},
        scheduled_encoder_inputs={},
        num_common_prefix_blocks=[0] * getattr(kv_cache_manager, "num_kv_cache_groups", 1),
        finished_req_ids=set(),
        free_encoder_mm_hashes=[],
        structured_output_request_ids=[],
        grammar_bitmask=None,
        kv_connector_metadata=None,
    )

    return scheduler_output


def release_vllm_request(model_runner: Any, request_id: str):
    if not hasattr(model_runner, "kv_cache_manager"):
        logger.warning(f"KV cache manager not found when releasing request {request_id}")
        return

    kv_cache_manager = model_runner.kv_cache_manager

    try:
        kv_cache_manager.coordinator.free(request_id)
        logger.debug(f"Released KV cache for request {request_id}")
    except Exception as e:
        logger.warning(f"Error releasing KV cache for request {request_id}: {e}")


================================================================================
File: src/parallax/vllm/model_runner.py
Size: 20.95 kB
================================================================================

from __future__ import annotations

import importlib
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import torch
from mlx_lm.utils import load_config
from vllm.config import (
    CacheConfig,
    CompilationConfig,
    DeviceConfig,
    LoadConfig,
    ModelConfig,
    ParallelConfig,
    SchedulerConfig,
    VllmConfig,
)
from vllm.distributed.parallel_state import GroupCoordinator as VLLMGroupCoordinator
from vllm.v1.core.kv_cache_manager import KVCacheManager
from vllm.v1.core.kv_cache_utils import (
    generate_scheduler_kv_cache_config,
    get_kv_cache_configs,
    get_request_block_hasher,
    init_none_hash,
)
from vllm.v1.kv_cache_interface import KVCacheConfig, KVCacheGroupSpec, KVCacheTensor
from vllm.v1.worker.gpu_model_runner import GPUModelRunner

from parallax.sglang.monkey_patch_utils.weight_loader_filter import (
    apply_weight_loader_filter_patch,
    set_layer_range_for_filtering,
)
from parallax.utils.tokenizer_utils import load_tokenizer
from parallax.vllm.monkey_patch import apply_parallax_vllm_monkey_patch
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


class ParallaxVLLMGroupCoordinator(VLLMGroupCoordinator):
    """
    Parallax version of vLLM's GroupCoordinator.
    Override is_first_rank and is_last_rank to use layer ranges instead of process ranks.
    """

    def __init__(
        self,
        group_ranks: List[List[int]],
        local_rank: int,
        torch_distributed_backend: Union[str, torch.distributed.Backend],
        use_device_communicator: bool,
        use_message_queue_broadcaster: bool = False,
        group_name: Optional[str] = None,
        pp_start_layer: int = 0,
        pp_end_layer: int = 0,
        num_hidden_layers: int = 0,
    ):
        super().__init__(
            group_ranks=group_ranks,
            local_rank=local_rank,
            torch_distributed_backend=torch_distributed_backend,
            use_device_communicator=use_device_communicator,
            use_message_queue_broadcaster=use_message_queue_broadcaster,
            group_name=group_name,
        )
        self.pp_start_layer = pp_start_layer
        self.pp_end_layer = pp_end_layer
        self.num_hidden_layers = num_hidden_layers

    @property
    def is_first_rank(self) -> bool:
        """Return whether this is the first pipeline stage based on layer range."""
        return self.pp_start_layer == 0

    @property
    def is_last_rank(self) -> bool:
        """Return whether this is the last pipeline stage based on layer range."""
        return self.pp_end_layer >= self.num_hidden_layers


def _create_kv_cache_config_from_specs(
    kv_cache_group: KVCacheGroupSpec,
    attn_layers: List[str],
    kv_cache_memory_fraction: float,
) -> KVCacheConfig:
    import torch

    free_memory, total_memory = torch.cuda.mem_get_info(0)
    available_memory = int(free_memory * kv_cache_memory_fraction)

    logger.info(
        f"Available GPU memory for KV cache: "
        f"{available_memory / (1024**3):.2f} GB "
        f"({kv_cache_memory_fraction:.1%} of {free_memory / (1024**3):.2f} GB)"
    )

    page_size_bytes = kv_cache_group.kv_cache_spec.page_size_bytes

    max_blocks_by_memory = available_memory // page_size_bytes

    num_blocks = max(100, min(1000, int(max_blocks_by_memory * 0.8)))

    logger.debug(f"Calculated KV cache blocks: {num_blocks} (max possible: {max_blocks_by_memory})")

    tensor_size_bytes = page_size_bytes * num_blocks

    kv_cache_config = KVCacheConfig(
        num_blocks=num_blocks,
        kv_cache_tensors=[
            KVCacheTensor(
                size=tensor_size_bytes,
                shared_by=attn_layers,
            )
        ],
        kv_cache_groups=[kv_cache_group],
    )

    return kv_cache_config


class ParallaxVLLMModelRunner(GPUModelRunner):

    def __init__(
        self,
        vllm_config: VllmConfig,
        kv_cache_config: Optional[KVCacheConfig],
        device: str,
        start_layer: int,
        end_layer: int,
        num_hidden_layers: int,
    ):
        self.start_layer = start_layer
        self.end_layer = end_layer
        self.num_hidden_layers = num_hidden_layers
        self.num_shard_layers = end_layer - start_layer

        self.is_first_peer = start_layer == 0
        self.is_last_peer = end_layer == num_hidden_layers

        self.pp_rank = 0
        self.pp_size = 1

        self.request_block_hasher: Optional[Callable[[Any], List[Any]]] = None
        self.enable_prefix_caching: bool = True

        super().__init__(vllm_config=vllm_config, device=torch.device(device))
        self.kv_cache_config = kv_cache_config

        logger.info(
            f"ParallaxVLLMModelRunner initialized: layers [{start_layer}, {end_layer}), "
            f"is_first={self.is_first_peer}, is_last={self.is_last_peer}"
        )

    def _create_kv_cache_config(self, kv_cache_memory_fraction: float = None) -> KVCacheConfig:
        logger.debug("Generating KV cache configuration from model...")

        try:
            kv_cache_specs = self.model.get_kv_cache_spec()
        except AttributeError:
            logger.warning(
                "Cannot access get_kv_cache_spec due to cudagraph wrapper, using fallback method"
            )
            kv_cache_specs = None

        import torch

        free_memory, total_memory = torch.cuda.mem_get_info(self.device.index or 0)

        memory_fraction = (
            kv_cache_memory_fraction
            if kv_cache_memory_fraction is not None
            else self.cache_config.gpu_memory_utilization
        )
        available_memory = int(free_memory * memory_fraction)

        logger.debug(
            f"Available GPU memory for KV cache: "
            f"{available_memory / (1024**3):.2f} GB "
            f"({memory_fraction:.1%} of {free_memory / (1024**3):.2f} GB)"
        )

        if kv_cache_specs is not None:
            kv_cache_configs = get_kv_cache_configs(
                vllm_config=self.vllm_config,
                kv_cache_specs=[kv_cache_specs],
                available_memory=[available_memory],
            )
            kv_cache_config = generate_scheduler_kv_cache_config(kv_cache_configs)
        else:
            logger.debug("Using fallback KV cache configuration")

            model = self.model
            hf_config = model.model.config
            num_attention_heads = getattr(hf_config, "num_attention_heads", 8)
            hidden_size = getattr(hf_config, "hidden_size", 1024)
            head_size = hidden_size // num_attention_heads

            from vllm.v1.kv_cache_interface import FullAttentionSpec, KVCacheGroupSpec

            model_dtype = self.vllm_config.model_config.dtype
            if isinstance(model_dtype, str):
                try:
                    from vllm.utils.torch_utils import (
                        STR_DTYPE_TO_TORCH_DTYPE,  # type: ignore
                    )
                except Exception:
                    # Older/newer vLLM versions may not expose torch_utils.
                    # Fall back silently and default to float16.
                    STR_DTYPE_TO_TORCH_DTYPE = {}
                model_dtype = STR_DTYPE_TO_TORCH_DTYPE.get(model_dtype, torch.float16)

            kv_cache_group = KVCacheGroupSpec(
                layer_names=[f"model.layers.{i}" for i in range(self.start_layer, self.end_layer)],
                kv_cache_spec=FullAttentionSpec(
                    block_size=self.cache_config.block_size,
                    num_kv_heads=num_attention_heads,
                    head_size=head_size,
                    dtype=model_dtype,
                ),
            )

            layer_names = [f"model.layers.{i}" for i in range(self.start_layer, self.end_layer)]

            kv_cache_config = _create_kv_cache_config_from_specs(
                kv_cache_group=kv_cache_group,
                attn_layers=layer_names,
                kv_cache_memory_fraction=memory_fraction,
            )

        logger.debug(
            f"KV cache config generated: "
            f"num_blocks={kv_cache_config.num_blocks}, "
            f"num_groups={len(kv_cache_config.kv_cache_groups)}"
        )

        return kv_cache_config

    def initialize_kv_cache_manager(self, max_model_len: int) -> KVCacheManager:
        logger.debug("Initializing vLLM KVCacheManager...")

        if self.kv_cache_config is None:
            self.kv_cache_config = self._create_kv_cache_config()

        kv_cache_manager = KVCacheManager(
            kv_cache_config=self.kv_cache_config,
            max_model_len=max_model_len,
            enable_caching=True,
            use_eagle=False,
            log_stats=True,
            enable_kv_cache_events=False,
            dcp_world_size=1,
        )

        self.kv_cache_manager = kv_cache_manager
        cache_config = self.vllm_config.cache_config
        enable_prefix = cache_config.enable_prefix_caching
        if enable_prefix is None:
            enable_prefix = True

        self.enable_prefix_caching = False

        self.request_block_hasher = None
        if enable_prefix and kv_cache_manager.block_size is not None:
            try:
                hashing_mod = importlib.import_module("vllm.utils.hashing")
                get_hash_fn_by_name: Callable[[str], Callable[[Any], bytes]] = getattr(
                    hashing_mod, "get_hash_fn_by_name"
                )
                hash_fn = get_hash_fn_by_name(cache_config.prefix_caching_hash_algo)
                init_none_hash(hash_fn)
            except (ModuleNotFoundError, AttributeError) as exc:
                logger.warning("Unable to initialize prefix cache hashing: %s", exc)

                def simple_hash_fn(obj: Any) -> bytes:
                    return str(hash(str(obj))).encode("utf-8")

                hash_fn = simple_hash_fn
                logger.info("Using simple fallback hash function for prefix caching")

            block_size = kv_cache_manager.block_size
            if block_size is None and self.kv_cache_config.kv_cache_groups:
                block_size = self.kv_cache_config.kv_cache_groups[0].kv_cache_spec.block_size
            if block_size is not None:
                self.request_block_hasher = get_request_block_hasher(block_size, hash_fn)
                logger.info("Initialized prefix cache block hasher with block_size=%d", block_size)

        logger.debug(
            f"KVCacheManager initialized: block_size={kv_cache_manager.block_size}, "
            f"usage={kv_cache_manager.usage:.2%}"
        )

        return kv_cache_manager

    def load_model(self) -> None:
        logger.debug(f"Loading vLLM model with layers [{self.start_layer}, {self.end_layer})...")

        from vllm.distributed.utils import get_pp_indices

        original_get_pp_indices = get_pp_indices

        def custom_get_pp_indices(num_layers: int, rank: int, world_size: int):
            logger.debug(
                f"custom_get_pp_indices called: num_layers={num_layers}, "
                f"returning [{self.start_layer}, {self.end_layer})"
            )
            return self.start_layer, self.end_layer

        import vllm.distributed.utils

        vllm.distributed.utils.get_pp_indices = custom_get_pp_indices

        try:
            super().load_model()
            logger.debug(
                f"Successfully loaded {self.num_shard_layers} layers "
                f"[{self.start_layer}:{self.end_layer}]"
            )

        finally:
            vllm.distributed.utils.get_pp_indices = original_get_pp_indices

    def execute_model(self, scheduler_output, intermediate_tensors=None):
        """
        Execute the model with the given scheduler output and intermediate tensors.
        If this is not the first peer, and the intermediate_tensors buffer is not initialized,
        initialize it.
        """
        if not self.is_first_peer and self.intermediate_tensors is None:
            self.intermediate_tensors = self.model.make_empty_intermediate_tensors(
                batch_size=self.max_num_tokens,
                dtype=self.model_config.dtype,
                device=self.device,
            )
            logger.debug("Successfully initialized intermediate_tensors buffer")

        return super().execute_model(scheduler_output, intermediate_tensors)


def initialize_vllm_model_runner(
    model_repo: str,
    start_layer: int,
    end_layer: int,
    kv_cache_memory_fraction: float,
    attention_backend: str,
    kv_block_size: int,
    max_num_tokens_per_batch: int = 1024,
    dtype: str = "float16",
    **kwargs,
) -> Tuple[ParallaxVLLMModelRunner, Dict, Any]:
    from parallax.utils.selective_download import get_model_path_with_selective_download

    logger.info(
        f"Initializing vLLM model runner for {model_repo}, " f"layers=[{start_layer}, {end_layer})"
    )

    model_path = get_model_path_with_selective_download(
        model_repo,
        start_layer=start_layer,
        end_layer=end_layer,
    )

    config = load_config(model_path)
    tokenizer = load_tokenizer(model_path, eos_token_ids=config.get("eos_token_id", None))
    dtype = config.get("torch_dtype", "bfloat16")

    num_hidden_layers = config.get("num_hidden_layers")
    is_first_peer = start_layer == 0
    is_last_peer = end_layer == num_hidden_layers

    # Apply Parallax vLLM monkey patches for pipeline parallelism
    try:
        apply_parallax_vllm_monkey_patch(is_first_stage=is_first_peer, is_last_stage=is_last_peer)
        logger.debug(
            f"Applied Parallax vLLM monkey patches: is_first_stage={is_first_peer}, is_last_stage={is_last_peer}"
        )
    except Exception as e:
        logger.warning("Failed to apply Parallax vLLM monkey patches: %s", e)

    # Apply layer-range-based weight file filtering before any model load.
    # Reuse the generic monkey patch used by sglang implementation to reduce
    # local weight file reads when loading a partial layer shard.
    try:
        set_layer_range_for_filtering(start_layer, end_layer, num_hidden_layers)
        apply_weight_loader_filter_patch()
        logger.debug(
            f"Applied weight loader filter monkey patch for layers [{start_layer}, {end_layer})"
        )
    except Exception as e:
        logger.warning("Failed to apply weight loader filter patch for vLLM loading: %s", e)

    # For single process, always use pp_size=1
    virtual_pp_size = 1

    import os

    import vllm.distributed.parallel_state as parallel_state

    if not parallel_state.model_parallel_is_initialized():
        logger.debug(f"Initializing vLLM distributed environment...")

        # Set environment variables for distributed initialization
        if "RANK" not in os.environ:
            os.environ["RANK"] = "0"
        if "WORLD_SIZE" not in os.environ:
            os.environ["WORLD_SIZE"] = "1"
        if "LOCAL_RANK" not in os.environ:
            os.environ["LOCAL_RANK"] = "0"
        if "MASTER_ADDR" not in os.environ:
            os.environ["MASTER_ADDR"] = "localhost"
        if "MASTER_PORT" not in os.environ:
            os.environ["MASTER_PORT"] = "12355"

        try:
            parallel_state.init_distributed_environment()

            # Initialize with pp_size=1 for single process
            parallel_state.initialize_model_parallel(
                tensor_model_parallel_size=1,
                pipeline_model_parallel_size=1,
            )

            # Monkey patch the PP group with our custom Parallax coordinator
            # that uses layer ranges to determine is_first_rank/is_last_rank
            original_pp_group = parallel_state._PP
            if original_pp_group is not None:
                # Get backend from device_group (torch is already imported at module level)
                import torch.distributed

                backend = torch.distributed.get_backend(original_pp_group.device_group)

                # Create a Parallax PP group coordinator
                # Need to wrap ranks in a list of lists for group_ranks parameter
                parallax_pp_group = ParallaxVLLMGroupCoordinator(
                    group_ranks=[original_pp_group.ranks],
                    local_rank=original_pp_group.local_rank,
                    torch_distributed_backend=backend,
                    use_device_communicator=original_pp_group.use_device_communicator,
                    use_message_queue_broadcaster=(original_pp_group.mq_broadcaster is not None),
                    group_name="pp",
                    pp_start_layer=start_layer,
                    pp_end_layer=end_layer,
                    num_hidden_layers=num_hidden_layers,
                )
                # Replace the PP group
                parallel_state._PP = parallax_pp_group
                logger.debug(
                    f"Replaced vLLM PP group with Parallax coordinator: "
                    f"is_first_rank={parallax_pp_group.is_first_rank}, "
                    f"is_last_rank={parallax_pp_group.is_last_rank}"
                )

            logger.debug(f"vLLM distributed environment initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize distributed environment: {e}")
            logger.error(f"vLLM distributed initialization failed. Error: {e}")
            raise

    if end_layer > num_hidden_layers:
        raise ValueError(
            f"end_layer ({end_layer}) cannot be greater than "
            f"num_hidden_layers ({num_hidden_layers})"
        )

    model_config = ModelConfig(
        model=str(model_path),
        tokenizer=str(model_path),
        tokenizer_mode="auto",
        trust_remote_code=True,
        dtype=dtype,
        seed=0,
        max_model_len=getattr(config, "max_position_embeddings", 4096),
    )

    cache_config = CacheConfig(
        block_size=kv_block_size,
        gpu_memory_utilization=kv_cache_memory_fraction,
        swap_space=0,
        cache_dtype="auto",
    )

    parallel_config = ParallelConfig(
        pipeline_parallel_size=virtual_pp_size,
        tensor_parallel_size=1,
        distributed_executor_backend=None,
    )

    device_config = DeviceConfig(device="cuda")
    load_config_for_config = LoadConfig(load_format="auto")

    max_batched_tokens = max(max_num_tokens_per_batch, model_config.max_model_len)
    scheduler_config = SchedulerConfig(
        max_num_batched_tokens=max_batched_tokens,
        max_num_seqs=256,
        max_model_len=model_config.max_model_len,
    )

    vllm_config = VllmConfig(
        model_config=model_config,
        cache_config=cache_config,
        parallel_config=parallel_config,
        scheduler_config=scheduler_config,
        device_config=device_config,
        load_config=load_config_for_config,
        lora_config=None,
        speculative_config=None,
        observability_config=None,
        prompt_adapter_config=None,
        quant_config=None,
        compilation_config=CompilationConfig(),
        kv_transfer_config=None,
        kv_events_config=None,
        additional_config={},
        instance_id="",
    )

    model_runner = ParallaxVLLMModelRunner(
        vllm_config=vllm_config,
        kv_cache_config=None,
        device="cuda",
        start_layer=start_layer,
        end_layer=end_layer,
        num_hidden_layers=num_hidden_layers,
    )

    logger.info("Loading vLLM model (partial layers)...")
    model_runner.load_model()
    logger.info("vLLM model loaded successfully")

    logger.debug("Letting vLLM automatically generate KV cache configuration...")

    kv_cache_specs = model_runner.get_kv_cache_spec()

    if not kv_cache_specs:
        raise RuntimeError("No KV cache specs found in the loaded model")

    import torch

    free_memory, total_memory = torch.cuda.mem_get_info(0)
    available_memory = int(free_memory * kv_cache_memory_fraction)

    logger.info(
        f"Available GPU memory for KV cache: "
        f"{available_memory / (1024**3):.2f} GB "
        f"({kv_cache_memory_fraction:.1%} of {free_memory / (1024**3):.2f} GB)"
    )

    from vllm.v1.core.kv_cache_utils import (
        generate_scheduler_kv_cache_config,
        get_kv_cache_configs,
    )

    kv_cache_configs = get_kv_cache_configs(
        vllm_config=model_runner.vllm_config,
        kv_cache_specs=[kv_cache_specs],
        available_memory=[available_memory],
    )

    kv_cache_config = generate_scheduler_kv_cache_config(kv_cache_configs)

    model_runner.kv_cache_config = kv_cache_config

    logger.info("Initializing GPUModelRunner KV cache...")
    model_runner.initialize_kv_cache(kv_cache_config)
    logger.info("GPUModelRunner KV cache initialized successfully")

    logger.info("Initializing KV Cache Manager...")
    model_runner.initialize_kv_cache_manager(max_model_len=model_config.max_model_len)
    logger.info("KV Cache Manager initialized successfully")

    return model_runner, config, tokenizer


================================================================================
File: src/parallax/vllm/monkey_patch.py
Size: 986 B
================================================================================

"""
Monkey patches for vLLM to support Parallax pipeline parallelism.

This module provides a unified entry point for applying all vLLM-related monkey patches
required for Parallax's distributed inference with pipeline parallelism.
"""

from parallax.vllm.monkey_patch_utils.weight_loader import (
    apply_vllm_weight_loader_patch,
    set_vllm_pipeline_stage,
)


## Here are patch functions for vLLM
## Hopefully, when vLLM supports pipeline parallelism natively in the way we need,
## we can remove these patches
def apply_parallax_vllm_monkey_patch(is_first_stage: bool, is_last_stage: bool):
    """
    Apply all Parallax monkey patches for vLLM.

    Args:
        is_first_stage: Whether this is the first pipeline stage.
        is_last_stage: Whether this is the last pipeline stage. This affects
                      whether lm_head weights are expected to be loaded.
    """
    set_vllm_pipeline_stage(is_first_stage, is_last_stage)
    apply_vllm_weight_loader_patch()


================================================================================
File: src/parallax/vllm/monkey_patch_utils/weight_loader.py
Size: 3.92 kB
================================================================================

"""
Monkey patch for vLLM weight loading to skip non-existent weights on different pipeline stages.
This is similar to the approach used in sglang monkey patches.
"""

import logging
from typing import Any

logger = logging.getLogger(__name__)

_vllm_patch_applied = False
_is_first_stage = False  # Default to False
_is_last_stage = True  # Default to True for safety


def set_vllm_pipeline_stage(is_first_stage: bool, is_last_stage: bool):
    """Set whether this is the first and/or last pipeline stage."""
    global _is_first_stage, _is_last_stage
    _is_first_stage = is_first_stage
    _is_last_stage = is_last_stage
    logger.debug(
        f"Set vLLM pipeline stage: is_first_stage={_is_first_stage}, is_last_stage={_is_last_stage}"
    )


def apply_vllm_weight_loader_patch():
    """
    Apply monkey patch to vLLM's default loader to skip initialization checks
    for weights that are not expected on certain pipeline stages.

    - Skips `embed_tokens` check on non-first stages.
    - Skips `lm_head` check on non-last stages.
    """
    global _vllm_patch_applied

    if _vllm_patch_applied:
        logger.debug("vLLM weight loader patch already applied, skipping")
        return

    try:
        from vllm.model_executor.model_loader import default_loader

        original_load_weights = default_loader.DefaultModelLoader.load_weights

        def patched_load_weights(self, model: Any, model_config: Any):
            """Patched load_weights that handles embed_tokens and lm_head for pipeline parallelism."""
            global _is_first_stage, _is_last_stage

            try:
                # Call original load_weights
                original_load_weights(self, model, model_config)
            except ValueError as e:
                error_msg = str(e)
                uninitialized_weights = "not initialized from checkpoint" in error_msg

                # Case 1: embed_tokens.weight not found
                if "model.embed_tokens.weight" in error_msg and uninitialized_weights:
                    if not _is_first_stage:
                        # Expected behavior for non-first pipeline stages
                        logger.info(
                            "Skipping embed_tokens.weight initialization check on non-first pipeline stage"
                        )
                    else:
                        # This is the first stage, embed_tokens should be initialized
                        logger.error(
                            "embed_tokens.weight not initialized on first pipeline stage, this is an error"
                        )
                        raise

                # Case 2: lm_head.weight not found
                elif "lm_head.weight" in error_msg and uninitialized_weights:
                    if not _is_last_stage:
                        # Expected behavior for non-last pipeline stages
                        logger.info(
                            "Skipping lm_head.weight initialization check on non-last pipeline stage"
                        )
                    else:
                        # This is the last stage, lm_head should be initialized
                        logger.error(
                            "lm_head.weight not initialized on last pipeline stage, this is an error"
                        )
                        raise

                # Case 3: Other errors
                else:
                    # Different error, re-raise
                    raise

        # Apply the patch
        default_loader.DefaultModelLoader.load_weights = patched_load_weights
        _vllm_patch_applied = True
        logger.info("Successfully applied vLLM weight loader patch for pipeline parallelism")

    except ImportError as e:
        logger.warning(f"Could not apply vLLM weight loader patch: {e}")
    except Exception as e:
        logger.error(f"Error applying vLLM weight loader patch: {e}")
        raise


================================================================================
File: src/parallax_utils/__init__.py
Size: 0 B
================================================================================



================================================================================
File: src/parallax_utils/ascii_anime.py
Size: 7.81 kB
================================================================================

import json
import math
import os

from parallax_utils.file_util import get_project_root


class HexColorPrinter:
    COLOR_MAP = {
        "#000000": ("\033[30m", (0, 0, 0)),
        "#800000": ("\033[31m", (128, 0, 0)),
        "#008000": ("\033[32m", (0, 128, 0)),
        "#808000": ("\033[33m", (128, 128, 0)),
        "#000080": ("\033[34m", (0, 0, 128)),
        "#800080": ("\033[35m", (128, 0, 128)),
        "#008080": ("\033[36m", (0, 128, 128)),
        "#c0c0c0": ("\033[37m", (192, 192, 192)),
        "#808080": ("\033[90m", (128, 128, 128)),
        "#ff0000": ("\033[91m", (255, 0, 0)),
        "#00ff00": ("\033[92m", (0, 255, 0)),
        "#ffff00": ("\033[93m", (255, 255, 0)),
        "#0000ff": ("\033[94m", (0, 0, 255)),
        "#ff00ff": ("\033[95m", (255, 0, 255)),
        "#00ffff": ("\033[96m", (0, 255, 255)),
        "#ffffff": ("\033[97m", (255, 255, 255)),
    }

    RESET = "\033[0m"
    SHOW = "\033[97m"
    WHITE = "\033[97m"

    @classmethod
    def hex_to_rgb(cls, hex_color):
        hex_color = hex_color.lstrip("#")
        return tuple(int(hex_color[i : i + 2], 16) for i in (0, 2, 4))

    @classmethod
    def color_distance(cls, rgb1, rgb2):
        return math.sqrt(sum((c1 - c2) ** 2 for c1, c2 in zip(rgb1, rgb2)))

    @classmethod
    def find_closest_color(cls, target_hex):
        target_rgb = cls.hex_to_rgb(target_hex)
        min_distance = float("inf")
        closest_color = "\033[97m"

        for _, (ansi_code, rgb) in cls.COLOR_MAP.items():
            distance = cls.color_distance(target_rgb, rgb)
            if distance < min_distance:
                min_distance = distance
                closest_color = ansi_code
        if closest_color == "\033[37m":
            closest_color = "\033[35m"
        if closest_color == "\033[90m":
            closest_color = "\033[95m"

        return closest_color


def clear_screen():
    # Clear screen command for different operating systems
    os.system("cls" if os.name == "nt" else "clear")


def handle_colors_data(raw_data):
    color_dict = {}
    if raw_data is not None:
        config = json.loads(raw_data)
        for key, value in config.items():
            if isinstance(value, str) and value.startswith("#"):
                color_dict[key] = value
    return color_dict


def process_context_color_run(content, colors):
    res = []
    for row, row_str in enumerate(content):
        processed_row = ""
        for column, text in enumerate(row_str):
            position_str = str(column) + "," + str(row)
            hex_color = colors.get(position_str, None)
            if text in ("â–", "#", ".") and hex_color == "#000000":
                text = " "
            elif row == 11 and text not in ("â–", "#", " "):
                color = HexColorPrinter.WHITE
                processed_row += color
            else:
                if hex_color:
                    color = HexColorPrinter.find_closest_color(hex_color)
                    processed_row += color
            processed_row += text
        processed_row += HexColorPrinter.RESET
        res.append(processed_row)
    return res


def process_context_color_join(content, colors, model_name):
    res = []
    if len(model_name) > 30:
        model_name = model_name[:30]
    name_len = len(model_name)
    for row, row_str in enumerate(content):
        processed_row = ""
        for column, text in enumerate(row_str):
            position_str = str(column) + "," + str(row)
            hex_color = colors.get(position_str, None)
            if text in ("â–", "#", ".") and hex_color == "#000000":
                if hex_color == "#000000":
                    text = " "
            elif row == 7 and 9 <= column <= 38:
                pos = column - 9
                if pos < name_len:
                    text = model_name[pos]
                    processed_row += HexColorPrinter.RESET
                else:
                    text = " "
                    if hex_color:
                        color = HexColorPrinter.find_closest_color(hex_color)
                        processed_row += color
            else:
                if hex_color:
                    color = HexColorPrinter.find_closest_color(hex_color)
                    processed_row += color
            processed_row += text
        processed_row += HexColorPrinter.RESET
        res.append(processed_row)
    return res


def display_ascii_animation_run(animation_data):
    frames = animation_data.get("frames", [])
    # loop = animation_data.get('loop', False)

    if not frames:
        print("No animation frames found in the JSON data.")
        return

    if len(frames) > 0:
        last_frame = frames[-1]
        content = last_frame.get("content", None)
        colors_data = last_frame.get("colors", None)
        foreground = colors_data.get("foreground", None)
        colors = handle_colors_data(foreground)

        if content:
            res = process_context_color_run(content, colors)
            res = "\n".join(res)
            clear_screen()
            print(res)

    # for frame_data in frames:
    #     content = frame_data.get("content", None)
    #     delay = frame_data.get("duration", 30) / 1000.0
    #     colors_data = frame_data.get("colors", None)
    #     foreground = colors_data.get("foreground", None)
    #     colors = handle_colors_data(foreground)

    #     if content:
    #         res = process_context_color_run(content, colors)
    #         res = "\n".join(res)
    #         clear_screen()
    #         print(res)
    #         time.sleep(delay)


def display_ascii_animation_join(animation_data, model_name):
    frames = animation_data.get("frames", [])
    # loop = animation_data.get('loop', False)

    if not frames:
        print("No animation frames found in the JSON data.")
        return

    if len(frames) > 0:
        last_frame = frames[-1]
        content = last_frame.get("content", None)
        colors_data = last_frame.get("colors", None)
        foreground = colors_data.get("foreground", None)
        colors = handle_colors_data(foreground)

        if content:
            res = process_context_color_join(content, colors, model_name)
            res = "\n".join(res)
            clear_screen()
            print(res)

    # for frame_data in frames:
    #     content = frame_data.get("content", None)
    #     delay = frame_data.get("duration", 30) / 1000.0
    #     colors_data = frame_data.get("colors", None)
    #     foreground = colors_data.get("foreground", None)
    #     colors = handle_colors_data(foreground)

    #     if content:
    #         res = process_context_color_join(content, colors, model_name)
    #         res = "\n".join(res)
    #         clear_screen()
    #         print(res)
    #         time.sleep(delay)


def display_parallax_run():
    file_path = str(get_project_root()) + "/src/parallax_utils/anime/parallax_run.json"
    try:
        with open(file_path, "r") as f:
            animation_data = json.load(f)
    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found.")
        return
    except json.JSONDecodeError:
        print(f"Error: The file '{file_path}' contains invalid JSON.")
        return
    display_ascii_animation_run(animation_data)


def display_parallax_join(model_name):
    file_path = str(get_project_root()) + "/src/parallax_utils/anime/parallax_join.json"
    try:
        with open(file_path, "r") as f:
            animation_data = json.load(f)
    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found.")
        return
    except json.JSONDecodeError:
        print(f"Error: The file '{file_path}' contains invalid JSON.")
        return
    display_ascii_animation_join(animation_data, model_name)


================================================================================
File: src/parallax_utils/file_util.py
Size: 471 B
================================================================================

from pathlib import Path


def get_project_root():
    """Get the project root directory."""
    # Search for the project root by looking for pyproject.toml in parent directories
    current_dir = Path(__file__).parent
    while current_dir != current_dir.parent:
        if (current_dir / "pyproject.toml").exists():
            return current_dir
        current_dir = current_dir.parent

    # If not found, fallback to current working directory
    return Path.cwd()


================================================================================
File: src/parallax_utils/logging_config.py
Size: 4.26 kB
================================================================================

"""Logging configuration for Parallax."""

import logging
import os
import sys
import threading
from typing import Optional

__all__ = ["get_logger", "use_parallax_log_handler", "set_log_level"]

_init_lock = threading.Lock()
_default_handler: logging.Handler | None = None


class _Ansi:
    RESET = "\033[0m"
    BOLD = "\033[1m"
    RED = "\033[31m"
    YELLOW = "\033[33m"
    GREEN = "\033[32m"
    CYAN = "\033[36m"
    MAGENTA = "\033[35m"


_LEVEL_COLOR = {
    "DEBUG": _Ansi.CYAN,
    "INFO": _Ansi.GREEN,
    "WARNING": _Ansi.YELLOW,
    "ERROR": _Ansi.RED,
    "CRITICAL": _Ansi.MAGENTA,
}

_PACKAGE_COLOR = {
    "parallax": _Ansi.CYAN,
    "scheduling": _Ansi.GREEN,
    "backend": _Ansi.YELLOW,
    "sglang": _Ansi.MAGENTA,
}


class CustomFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:  # noqa: D401
        levelname = record.levelname.upper()
        levelcolor = _LEVEL_COLOR.get(levelname, "")
        record.levelcolor = levelcolor
        record.bold = _Ansi.BOLD
        record.reset = _Ansi.RESET

        # caller_block: last path component + line no
        pathname = record.pathname.rsplit("/", 1)[-1]
        record.caller_block = f"{pathname}:{record.lineno}"
        record.package = record.name.split(".")[0]
        record.packagecolor = _PACKAGE_COLOR.get(record.package, "")
        return super().format(record)


def _enable_default_handler(target_module_prefix):
    """Attach the default handler to the root logger with a name-prefix filter.

    Accepts either a single string prefix or an iterable of prefixes; a record
    passes the filter if its logger name starts with any provided prefix.
    """
    root = logging.getLogger()

    # attach the handler only to loggers that start with any of target prefixes
    class _ModuleFilter(logging.Filter):
        def __init__(self, prefixes):
            super().__init__()
            if isinstance(prefixes, str):
                self._prefixes = (prefixes,)
            else:
                try:
                    self._prefixes = tuple(prefixes)
                except TypeError:
                    self._prefixes = (str(prefixes),)

        def filter(self, rec: logging.LogRecord) -> bool:
            return any(rec.name.startswith(p) for p in self._prefixes)

    _default_handler.addFilter(_ModuleFilter(target_module_prefix))
    root.addHandler(_default_handler)


def _initialize_if_necessary():
    global _default_handler

    with _init_lock:
        if _default_handler is not None:
            return

        fmt = (
            "{asctime}.{msecs:03.0f} "
            "[{bold}{packagecolor}{package:<10}{reset}] "
            "[{bold}{levelcolor}{levelname:<8}{reset}] "
            "{caller_block:<25} {message}"
        )
        formatter = CustomFormatter(fmt=fmt, style="{", datefmt="%b %d %H:%M:%S")
        _default_handler = logging.StreamHandler(stream=sys.stdout)
        _default_handler.setFormatter(formatter)

        # root level from env or INFO
        logging.getLogger().setLevel("INFO")

        # Allow logs from our main packages by default
        _enable_default_handler(("parallax", "scheduling", "backend", "sglang"))


def set_log_level(level_name: str):
    """Set the root logger level."""
    _initialize_if_necessary()
    logging.getLogger().setLevel(level_name.upper())
    if level_name.upper() == "DEBUG":
        os.environ["RUST_LOG"] = "info"


def get_logger(name: Optional[str] = None) -> logging.Logger:
    """
    Grab a logger with parallaxâ€™s default handler attached.
    Call this in every module instead of logging.getLogger().
    """
    _initialize_if_necessary()
    return logging.getLogger(name)


def use_parallax_log_handler(for_root: bool = True):
    """
    Extend the custom handler to the root logger (so *all* libraries print
    with the same style) or to any logger you call this on.

    Example
    -------
        from parallax.logging import use_parallax_log_handler
        use_parallax_log_handler()            # now requests, hivemind, etc. share the style
    """
    del for_root
    _initialize_if_necessary()
    root = logging.getLogger()
    if _default_handler not in root.handlers:
        root.addHandler(_default_handler)


================================================================================
File: src/parallax_utils/prepare_adapter.py
Size: 5.62 kB
================================================================================

import glob
import json
import os
from pathlib import Path

import mlx.core as mx
import transformers
from huggingface_hub import snapshot_download


def process_adapter_config(model_id):
    """
    Process the adapter_config.json file associated with a Hugging Face model ID.
    """
    # Check if the model directory already exists locally.
    # Note: The local directory uses the raw `model_id` (slashes are not replaced).
    local_dir = model_id

    # Check if the local directory exists
    if not os.path.exists(local_dir):
        print(f"Model directory does not exist: {local_dir}")
        print(f"Downloading model from Hugging Face: {model_id} -> {local_dir}")
        try:
            snapshot_download(
                repo_id=model_id, local_dir=local_dir, local_dir_use_symlinks=False, revision="main"
            )
            print(f"Model downloaded to: {local_dir}")
        except Exception as e:
            raise RuntimeError(f"Model download failed: {str(e)}")

    # Check if adapter_config.json exists
    adapter_config_path = os.path.join(local_dir, "adapter_config.json")
    if not os.path.exists(adapter_config_path):
        raise FileNotFoundError(f"adapter_config.json not found at: {adapter_config_path}")

    # Load adapter_config.json
    with open(adapter_config_path, "r") as f:
        config = json.load(f)

    # 1. Handle the 'fine_tune_type' field
    if "fine_tune_type" not in config:
        peft_type = config.get("peft_type", "lora").lower()
        config["fine_tune_type"] = peft_type
        print(f"Added fine_tune_type field: {peft_type}")

    # 2. Handle the 'num_layers' field
    if "num_layers" not in config:
        config_path = os.path.join(local_dir, "config.json")
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"config.json not found at: {config_path}")

        with open(config_path, "r") as f:
            model_config = json.load(f)

        if "num_hidden_layers" not in model_config:
            raise ValueError("config.json is missing the 'num_hidden_layers' field")

        config["num_layers"] = model_config["num_hidden_layers"]
        print(f"Added num_layers field: {model_config['num_hidden_layers']}")

    # 3. Handle the 'lora_parameters' field
    if "lora_parameters" not in config:
        # Extract LoRA parameters from config
        r = config.get("r", 8)
        lora_alpha = config.get("lora_alpha", 20.0)
        lora_dropout = config.get("lora_dropout", 0.0)

        config["lora_parameters"] = {
            "rank": int(r),
            "scale": float(lora_alpha),
            "dropout": float(lora_dropout),
        }
        print(f"Added lora_parameters field: {config['lora_parameters']}")

    # Save the updated config to the current working directory
    output_path = os.path.join(os.getcwd(), "adapter_config.json")
    with open(output_path, "w") as f:
        json.dump(config, f, indent=2)

    print(f"Processing complete! Updated adapter_config.json saved to: {output_path}")
    return output_path


def trans_adapter_config(model_id):
    """
    Process and generate the adapter_config.json file (wrapper function).
    """
    process_adapter_config(model_id)


# Step 2: Convert safetensors files


def fetch_from_hub(path_or_hf_repo: str):
    """
    Load model weights, config, and tokenizer from a local path or Hugging Face Hub.
    If the path does not exist locally, download it from HF first.
    """
    model_path = Path(path_or_hf_repo)

    # Check if it's a local directory
    if not model_path.exists():
        print(f"[INFO] Downloading {path_or_hf_repo} from Hugging Face...")
        model_path = Path(
            snapshot_download(
                repo_id=path_or_hf_repo,
                allow_patterns=["*.json", "*.safetensors", "tokenizer.model"],
            )
        )
    else:
        print(f"[INFO] Using local model from {model_path}")

    # Load weight files
    weight_files = glob.glob(f"{model_path}/*.safetensors")
    if len(weight_files) == 0:
        raise FileNotFoundError(f"No safetensors files found in {model_path}")

    weights = {}
    for wf in weight_files:
        weights.update(mx.load(wf).items())

    # Load config and tokenizer
    config = transformers.AutoConfig.from_pretrained(path_or_hf_repo)
    tokenizer = transformers.AutoTokenizer.from_pretrained(path_or_hf_repo)

    return weights, config.to_dict(), tokenizer, model_path


def save_adapter(weights, tokenizer, config):
    """Save the adapter weights into a single adapters.safetensors file."""
    # Save all weights into one file
    mx.save_safetensors("adapters.safetensors", weights, metadata={"format": "mlx"})
    print("[INFO] Saved adapters to adapters.safetensors")


def trans_safetensors(model_path: str):
    print("[INFO] Loading model...")
    weights, config, tokenizer, model_path = fetch_from_hub(model_path)

    # Set data type
    dtype = mx.float16
    weights = {k: v.astype(dtype) for k, v in weights.items()}

    # Save adapter to a single file
    print("[INFO] Saving adapter...")
    save_adapter(weights, tokenizer, config)

    print("[INFO] Conversion complete!")


if __name__ == "__main__":
    import sys

    if len(sys.argv) != 2:
        print("Usage: python process_adapter.py <Hugging Face Model ID> or model_path")
        print("Example: python process_adapter.py Qwen/Qwen3-0.6B or ./xxx")
        sys.exit(1)

    model_id = sys.argv[1]
    try:
        trans_adapter_config(model_id)
    except Exception as e:
        print(f"Error: {str(e)}")
        sys.exit(1)

    trans_safetensors(model_id)


================================================================================
File: src/parallax_utils/request_metrics.py
Size: 726 B
================================================================================

import json


def get_request_metrics(chunk, start_time, first_token_time, last_token_time):
    try:
        if isinstance(chunk, bytes):
            chunk = chunk.decode("utf-8")
        if isinstance(chunk, str):
            chunk = chunk.removeprefix("data: ").lstrip()
            chunk = json.loads(chunk)
        usage = chunk.get("usage")
        input_tokens = usage.get("prompt_tokens")
        output_tokens = usage.get("completion_tokens")
        usage.get("total_tokens")
        tps = output_tokens / (last_token_time - first_token_time)
        ttft = int((first_token_time - start_time) * 1000)
        return tps, ttft, input_tokens, output_tokens
    except Exception:
        return None, None, None, None


================================================================================
File: src/parallax_utils/utils.py
Size: 4.62 kB
================================================================================

from typing import Optional

import torch

from parallax.server.server_info import HardwareInfo
from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


def bytes_per_element(dtype) -> int:
    """Return element size in bytes for supported torch/MLX dtypes."""
    try:
        import mlx.core as mx  # type: ignore
    except Exception:
        mx = None

    if dtype is None:
        return 2
    if dtype in (
        getattr(torch, "float32", None),
        getattr(torch, "bfloat16", None),
        getattr(torch, "float16", None),
        getattr(torch, "half", None),
        getattr(torch, "int8", None),
    ):
        if dtype == torch.float32:
            return 4
        if dtype in (torch.bfloat16, torch.float16, torch.half):
            return 2
        if dtype == torch.int8:
            return 1
    if mx is not None and dtype in (
        getattr(mx, "float32", None),
        getattr(mx, "bfloat16", None),
        getattr(mx, "float16", None),
    ):
        if dtype == mx.float32:
            return 4
        return 2
    return 2


def compute_max_tokens_in_cache(
    *,
    device: str,
    kv_cache_memory_fraction: float,
    num_shard_layers: int,
    num_key_value_heads: int,
    head_dim_k: int,
    head_dim_v: int,
    elem_bytes: int,
    available_cache_bytes: Optional[int] = None,
) -> int:
    """Estimate max tokens storable in KV cache given current free memory and fraction."""
    if available_cache_bytes is not None:
        available_cache_size = int(available_cache_bytes)
    elif device == "cuda":
        free_bytes, _ = torch.cuda.mem_get_info(torch.cuda.current_device())
        available_cache_size = int(free_bytes * kv_cache_memory_fraction)
    else:
        try:
            import mlx.core as mx
        except Exception:
            mx = None
        hw = HardwareInfo.detect()
        used = mx.get_active_memory() if mx is not None else 0
        available_cache_size = int((hw.total_ram_gb * 1024**3 - used) * kv_cache_memory_fraction)
    per_token_cache_size = (
        num_shard_layers * num_key_value_heads * (head_dim_k + head_dim_v) * elem_bytes
    )
    return max(0, available_cache_size // per_token_cache_size)


def derive_max_batch_size(
    *,
    requested_max_batch_size: Optional[int],
    max_sequence_len: Optional[int],
    max_tokens_in_cache: Optional[int],
) -> int:
    """Derive final max_batch_size clamped by KV capacity if sequence length known."""
    max_batch_capacity: Optional[int] = None
    if max_sequence_len and max_tokens_in_cache:
        max_batch_capacity = max(1, max_tokens_in_cache // int(max_sequence_len))
    if requested_max_batch_size is None:
        if max_batch_capacity is None:
            logger.warning("Overriding max_batch_size to 16 due to no max_sequence_len provided")
            return 16
        return max_batch_capacity
    if max_batch_capacity is not None:
        return min(requested_max_batch_size, max_batch_capacity)
    return requested_max_batch_size


def compute_max_batch_size(
    *,
    requested_max_batch_size: Optional[int],
    max_sequence_len: Optional[int],
    device: Optional[str],
    kv_cache_memory_fraction: float,
    num_shard_layers: int,
    num_key_value_heads: int,
    head_dim: int,
    dtype=None,
    elem_bytes: Optional[int] = None,
    memory_gb: Optional[float] = None,
    head_dim_k: Optional[int] = None,
    head_dim_v: Optional[int] = None,
) -> int:
    """Compute final max_batch_size by chaining dtype->elem_bytes, KV capacity, and clamping.

    If memory_gb is provided, we compute available_cache_bytes from it; otherwise we use device heuristics.
    """
    eb = elem_bytes if elem_bytes is not None else bytes_per_element(dtype)
    available_cache_bytes = None
    if memory_gb is not None:
        available_cache_bytes = int(memory_gb * 1024**3 * kv_cache_memory_fraction)
    ## This is an Error due to kv may have different head_dim
    max_tokens = compute_max_tokens_in_cache(
        device=device or "",  # empty means non-cuda path
        kv_cache_memory_fraction=kv_cache_memory_fraction,
        num_shard_layers=num_shard_layers,
        num_key_value_heads=num_key_value_heads,
        head_dim_k=head_dim_k if head_dim_k is not None else head_dim,
        head_dim_v=head_dim_v if head_dim_v is not None else head_dim,
        elem_bytes=eb,
        available_cache_bytes=available_cache_bytes,
    )
    return derive_max_batch_size(
        requested_max_batch_size=requested_max_batch_size,
        max_sequence_len=max_sequence_len,
        max_tokens_in_cache=max_tokens,
    )


================================================================================
File: src/parallax_utils/version_check.py
Size: 1.43 kB
================================================================================

import importlib.metadata
import json
import urllib.request


def get_current_version():
    version = "unknown"
    try:
        version = importlib.metadata.version("parallax")
    except Exception:
        try:
            import parallax

            version = getattr(parallax, "__version__", "unknown")
        except Exception:
            pass
    return version


def check_latest_release():
    """
    Check if the current version matches the latest GitHub release.
    If not, print an update notification.
    """
    version = get_current_version()
    GITHUB_API = "https://api.github.com/repos/GradientHQ/parallax/releases/latest"
    try:
        with urllib.request.urlopen(GITHUB_API, timeout=4) as response:
            data = json.loads(response.read())
            latest = data.get("tag_name") or data.get("name")
            if latest:
                latest = latest.lstrip("vV")
                ver = version.lstrip("vV")
                if latest != ver:
                    print(
                        f"\033[93m[Parallax] New version available: {latest} (current: {ver})\033[0m\n"
                        f"For macOS, run `git pull && pip install -e '.[mac]'` to update.\n"
                        f"For Linux, run `git pull && pip install -e '.[gpu]'` to update.\n"
                        f"For Windows, run `parallax install` to update\n"
                    )
    except Exception:
        pass


================================================================================
File: src/scheduling/README.md
Size: 7.86 kB
================================================================================

## Scheduling subsystem

This directory implements a two-phase scheduler for distributed LLM inference:

### Phase 1 â€” Layer allocation

Assign contiguous decoder layer ranges to nodes and rebalance in place, as illustrated below:

<img width="1874" height="852" alt="parallax_1" src="https://github.com/user-attachments/assets/c57cde77-0cda-48fc-b1ad-6d4aa1b1787b" />

### Phase 2 â€” Request routing

Compute an end-to-end, minimum-latency path across the assigned node ranges, as illustrated below:

<img width="1828" height="705" alt="parallax_2" src="https://github.com/user-attachments/assets/8a6b4d8f-8d97-402b-ba84-3ce61e4ee313" />

The main entrypoint is `scheduling.scheduler.Scheduler`, which orchestrates allocation, dynamic joins/leaves, health checks, and routing.

### Key concepts
- **`ModelInfo`**: model-level shapes and per-layer cost/IO; used by capacity and latency estimators.
- **`NodeHardwareInfo`**: static hardware facts (TFLOPS, memory size/bandwidth).
- **`Node`**: live worker state. Tracks allocated `[start_layer, end_layer)` range, load (`current_requests`), RTTs to peers, and exposes helpers:
  - `get_decoder_layer_capacity(...)`: parameter-memory-bounded layer capacity.
  - `layer_latency_ms`: effective per-node latency (overload-aware, roofline fallback).
  - `hosts_layer(layer_id)`; allocators provide `has_full_pipeline()` across nodes.
- **Pipeline**: a chain of nodes whose ranges cover `[0, L)` without gaps (L = `ModelInfo.num_layers`).

## Phase 1: Layer allocation

Implemented in `scheduling.layer_allocation`.

- **`BaseLayerAllocator`**: shared utilities and state
  - Tracks per-layer hosting power via `LayerLoad` (min-heap of KV cache hosting power).
  - Assign/deassign helpers: `allocate(...)`, `deallocate(...)`, `declare(...)`.
  - Dynamic events: `join(node)`, `leave(node_id)`; best-effort placement on lightest layers.
  - Global health: `has_full_pipeline()`, `layer_replication_stats()` and `should_global_rebalance()`.
  - In-place rebalancing: `adjust_pipeline_layers(pipeline_nodes, power_type)` uses a water-filling algorithm to split decoder layers proportional to compute/bandwidth while respecting per-node capacity. Endpoints (embedding/LM head) are reserved on the first/last nodes.
  - Leftovers: `allocate_left_over_nodes()` uses the dynamic policy to replicate lightest layers when full pipelines are already formed.

- **`GreedyLayerAllocator`**
  - Goal: maximize number of pipelines while minimizing stages per pipeline.
  - Strategy: iteratively build pipelines using capacity-sorted nodes with a simple look-ahead; after each pipeline, call `adjust_pipeline_layers`.
  - API: `global_allocation() -> bool` returns whether at least one full pipeline exists; unassigned nodes are handled by `allocate_left_over_nodes()`.

- **`DynamicProgrammingLayerAllocator`**
  - Goal: jointly optimize concurrency (pipelines) and latency (stages) via DP.
  - Scoring: chooses `k` to maximize `k^2 / s*(k)`, where `s*(k)` = minimal total stages to realize `k` pipelines.
  - Produces disjoint pipelines and then rebalances each via `adjust_pipeline_layers`.

### Important knobs
- **`rebalance_threshold`**: coefficient-of-variation threshold of layer loads to trigger global rebalance.
- **`water_filling_max_iterations`**: iterations for the water-filling search.

## Phase 2: Request routing

Implemented in `scheduling.request_routing`.

- **`RequestRoutingStrategy`**: interface with
  - `find_turning_points(nodes, num_layers)` for optional warm-up truncations.
  - `find_optimal_path(nodes, num_layers)` to return `(node_ids, latency)`.

- **`DynamicProgrammingRouting`**
  - Warm-up: layer-level DP over hosts of each layer to detect turning points:
    - `(node_id, l, "tail")`: node still hosts `l` but optimal path switches away â†’ drop `[l, end)` on that node.
    - `(node_id, l, "head")`: path first uses node at `l > start` â†’ drop `[start, l)`.
  - Routing: shard-level DP over the assigned contiguous ranges; edge cost is RTT via `Node.get_rtt_to`, vertex cost is `Node.layer_latency_ms`.

## Orchestration: `Scheduler`

Implemented in `scheduling.scheduler`.

- Coordinates layer allocation, node join/leave, periodic heartbeat checks, and request dispatch.
- Supports either `GreedyLayerAllocator` or `DynamicProgrammingLayerAllocator` via `strategy`.
- Maintains thread-safe queues for events and a background dispatcher for requests.
- Bootstrapping:
  - Waits for `min_nodes_bootstrapping` nodes, runs `global_allocation()`, and optional warm-up truncation via `request_warm_up_for_reshard` and `find_turning_points`.
- Dynamic events (non-blocking enqueuers):
  - `enqueue_join(node)`, `enqueue_leave(node_id)`, `enqueue_node_update(...)`.
- Heartbeats: `checking_node_heartbeat()` evicts nodes inactive for `heartbeat_timeout` seconds and can trigger a global rebalance.
- Dispatching: `dispatch_next_request()` or background `_dispatch_loop` compute routes via `RequestRoutingStrategy` and increment per-node load counters.

### Scheduler configuration
Constructor signature (selected arguments):

```python
Scheduler(
  model_info: ModelInfo,
  nodes: List[Node],
  min_nodes_bootstrapping: int = 1,
  strategy: Literal["greedy", "dp"] = "dp",
  request_arrival_horizon_sec: float = 600.0,
  rebalance_threshold: float = float("inf"),
  water_filling_max_iterations: int = 40,
  request_warm_up_for_reshard: int = 0,
  heartbeat_timeout: float = 60.0,
)
```

Notes:
- Setting `rebalance_threshold` low enables auto global rebalancing based on layer load imbalance; `float("inf")` disables it.
- `request_warm_up_for_reshard > 0` runs warm-up routing passes and applies consistent turning points to shrink shards before serving.

## Usage example

A minimal construction using the DP allocator and router:

```python
from scheduling.model_info import ModelInfo
from scheduling.node import Node, NodeHardwareInfo
from scheduling.scheduler import Scheduler

# Define model and two nodes
model = ModelInfo(  # instantiate with your model's parameters
    num_layers=40,
    # ... other fields ...
)

n0 = Node(
    node_id="node-0",
    hardware=NodeHardwareInfo(node_id="node-0", tflops_fp16=180.0, num_gpus=1, gpu_name="", memory_gb=80.0, memory_bandwidth_gbps=2039.0),
    model_info=model,
)

n1 = Node(
    node_id="node-1",
    hardware=NodeHardwareInfo(node_id="node-1", tflops_fp16=180.0, num_gpus=1, gpu_name="", memory_gb=80.0, memory_bandwidth_gbps=2039.0),
    model_info=model,
)

sched = Scheduler(model_info=model, nodes=[n0, n1], strategy="dp")

# Bootstrapping
sched.bootstrap()  # or sched.run() to start background threads

# List assigned ranges
print(sched.list_node_allocations())

# Enqueue a request signal (example)
from scheduling.node import RequestSignal
sched.receive_request(RequestSignal(request_id="req-1"))
print(sched.dispatch_next_request())  # (request_id, [node_id, ...], latency_ms)
```

## Extensibility
- Add a new allocator: subclass `BaseLayerAllocator` and implement `global_allocation()`; reuse `adjust_pipeline_layers()` if applicable.
- Add a new router: implement `RequestRoutingStrategy` with the two abstract methods and plug it into `Scheduler`.
- Custom RTT measurement: set `Node.rtt_getter: Callable[[Node, Node], float]` to lazily populate `rtt_to_nodes`.

## Guarantees and assumptions
- Allocations are contiguous `[start, end)` per node; endpoints are reserved at pipeline edges.
- A request runs all layers hosted on a node before transitioning to the next node in the path.
- Bootstrapping success implies at least one full pipeline covering `[0, L)`.

## Tests
Relevant tests live under `tests/scheduler_tests/` and cover allocation, routing, and scheduling:
- `test_layer_allocation.py`
- `test_request_routing.py`
- `test_scheduler.py`
- `test_utils.py`

Run your projectâ€™s test runner to validate changes.


================================================================================
File: src/scheduling/__init__.py
Size: 0 B
================================================================================



================================================================================
File: src/scheduling/layer_allocation.py
Size: 44.43 kB
================================================================================

"""
Layer allocation and dynamic rebalancing primitives (Phase 1 of scheduling).

Key components:
- LayerLoad: per-layer hosting power (combined KV memory + FLOPs) tracked in a min-heap;
- BaseLayerAllocator: shared utilities for static global_allocation(), dynamic join/leave, and
  in-place pipeline rebalancing using a water-filling algorithm;
- GreedyLayerAllocator: builds pipelines greedily to minimize stages and maximize the
  number of pipelines, then rebalances each pipeline in-place;
- DynamicProgrammingLayerAllocator: explores pipeline construction via DP to balance
  concurrency (pipelines) and latency (stages per pipeline), then rebalances each pipeline
  in-place.

All allocators assign contiguous layer ranges directly on `Node` instances and maintain
per-layer load state. Water-filling allocates decoder layers proportional to node compute
power (TFLOPS or memory bandwidth), respecting per-node parameter capacity and reserving
embedding/LM head endpoints on the first/last nodes of a pipeline.
"""

import heapq
from dataclasses import dataclass, field
from functools import lru_cache
from math import floor
from typing import Dict, List, Literal, Optional, Set, Tuple

from parallax_utils.logging_config import get_logger
from scheduling.model_info import ModelInfo
from scheduling.node import Node

logger = get_logger(__name__)


@dataclass
class LayerLoad:
    """Tracks the load and hosting power for a specific layer."""

    layer_id: int
    current_kv_size: int
    hosting_nodes: Set[str] = field(default_factory=set)

    def add_node(self, node: Node) -> None:
        """Add a node's contribution to this layer's load."""
        self.hosting_nodes.add(node.node_id)
        if node.per_decoder_layer_kv_cache_memory is None:
            raise ValueError("Node must have per_decoder_layer_kv_cache_memory")
        self.current_kv_size += node.per_decoder_layer_kv_cache_memory

    def remove_node(self, node: Node):
        """Remove a node from the layer load."""
        if node.node_id in self.hosting_nodes:
            self.hosting_nodes.remove(node.node_id)
            if node.per_decoder_layer_kv_cache_memory is None:
                raise ValueError("Node must have per_decoder_layer_kv_cache_memory")

            self.current_kv_size -= node.per_decoder_layer_kv_cache_memory

    def __lt__(self, other):
        """For heap ordering: prioritize layers with lower hosting power.

        Primary: less memory (less hosting power)
        Secondary: lower layer ID (less layers hosted)
        """
        return (self.current_kv_size, self.layer_id) < (
            other.current_kv_size,
            other.layer_id,
        )


class BaseLayerAllocator:
    """Base class for layer allocation and rebalancing.

    There are two types of allocations:
    1. Static: initialization, global rebalancing.
       - happens globally, not per-node;
       - more optimal since it considers the entire cluster;
    2. Dynamic: nodes are joining / leaving dynamically
       - assignment done per-node based on layer load heap;
       - less optimal compared to static allocation (global knowledge);
       - necessary because we don't want to
         - re-start the whole server to
         - each worker node gives up running requests
         - each worker node re-load a different shard.

    Global rebalancing, i.e. re-`global_allocation` is needed when:
     - When we find some layers are not hosted by any node,
     - Loads are too imbalanced.

    Pipeline: nodes hosting the first layer to the last layer (inclusive).
    In-place adjustment:
        After each pipeline is formed, the layers within
        it are immediately adjusted to be proportional to the computing power
        (TFLOPS or bandwidth) of the nodes forming the pipeline. Capped by capacity.
        This ensures that the stages are balanced and system throughput is maximized.
        We use water-filling algorithm.
    """

    def __init__(
        self,
        model_info: ModelInfo,
        nodes: List[Node],
        *,
        rebalance_threshold: float = 0.25,
        water_filling_max_iterations: int = 40,
        assign_left_over_nodes: bool = True,
    ) -> None:
        self.model_info = model_info
        self.num_total_layers = model_info.num_layers
        # Use the caller-provided list object to keep a single authoritative list.
        # Sort in-place to preserve shared reference with scheduler.
        self.nodes = nodes
        self.nodes.sort(key=lambda node: node.get_decoder_layer_capacity(), reverse=True)

        self.layer_to_load: Dict[int, LayerLoad] = {}
        self.node_id_to_node: Dict[str, Node] = {}
        # Sync dict with initial nodes; prevents declare() from adding duplicates
        # when allocate_left_over_nodes() processes unallocated nodes
        for node in self.nodes:
            self.node_id_to_node[node.node_id] = node

        # Pipeline endpoints for routing
        self.embedding_node_ids: List[str] = []
        self.lm_head_node_ids: List[str] = []
        # How much we value memory vs. FLOPs for hosting power (sum to 1)
        # Threshold for layer hosting power imbalance to trigger global rebalance
        self.rebalance_threshold = rebalance_threshold
        # Maximum number of iterations to run the water-filling algorithm
        self.water_filling_max_iterations = water_filling_max_iterations
        # Whether to assign left-over nodes using dynamic policy for
        # static allocation's leftover nodes
        self.assign_left_over_nodes = assign_left_over_nodes

        # Node allocation
        self.node_allocation: Dict[str, Tuple[int, int]] = {}

        # Heapify Layer Loads
        self.layer_loads_heap: List[LayerLoad] = []
        for layer_id in range(self.num_total_layers):
            layer_load = LayerLoad(layer_id=layer_id, current_kv_size=0)
            self.layer_to_load[layer_id] = layer_load
        self._update_layer_loads_heap()
        logger.debug(
            "Initialized LayerAllocator with %d nodes for %d total layers",
            len(self.nodes),
            self.num_total_layers,
        )

    @property
    def num_nodes(self) -> int:
        """Number of nodes in the allocator."""
        return len(self.nodes)

    def validate_allocation(self, start_layer: int, end_layer: int):
        """Validate the allocation."""
        if start_layer < 0 or end_layer > self.num_total_layers:
            return False
        if start_layer >= end_layer:
            return False
        return True

    def global_allocation(self) -> bool:
        """Static assignment based on existing nodes. For cold-start and global rebalancing.

        Returns:
            True if at least one full pipeline (covering [0, num_total_layers)) was allocated.
        """
        return False

    def allocate(self, node: Node, start_layer: int, end_layer: int) -> None:
        """Allocate a node to a specific layer range."""
        if end_layer <= start_layer:
            raise ValueError(
                f"Invalid allocation: start_layer {start_layer} >= end_layer {end_layer}"
            )
        self.node_id_to_node[node.node_id] = node
        node.set_layer_allocation(start_layer, end_layer)
        self.node_allocation[node.node_id] = (start_layer, end_layer)
        logger.debug("Allocated node %s to layers [%d, %d)", node.node_id, start_layer, end_layer)
        if start_layer == 0:
            self.embedding_node_ids.append(node.node_id)
        if end_layer == self.num_total_layers:
            self.lm_head_node_ids.append(node.node_id)
        for layer_id in range(start_layer, end_layer):
            if layer_id not in self.layer_to_load:
                raise ValueError(f"Layer {layer_id} not found in layer_to_load")
            self.layer_to_load[layer_id].add_node(node)
        self._update_layer_loads_heap()

    def deallocate(self, node: Node) -> None:
        """Deallocate a node from its assigned layers."""
        if node.start_layer is None or node.end_layer is None:
            logger.info("Node must have start_layer and end_layer")
            return
        start_layer, end_layer = node.start_layer, node.end_layer
        logger.debug(
            "Deallocating node %s from layers [%d, %d)", node.node_id, start_layer, end_layer
        )
        if node.node_id in self.node_allocation:
            del self.node_allocation[node.node_id]
        if node.node_id in self.embedding_node_ids:
            self.embedding_node_ids.remove(node.node_id)
        if node.node_id in self.lm_head_node_ids:
            self.lm_head_node_ids.remove(node.node_id)
        for layer_id in range(start_layer, end_layer):
            if layer_id in self.layer_to_load:
                self.layer_to_load[layer_id].remove_node(node)
        node.clear_layer_allocation()
        node.is_active = False
        self._update_layer_loads_heap()

    def reallocate(self, node: Node, start_layer: int, end_layer: int) -> None:
        """Reallocate a node to a specific layer range."""
        self.deallocate(node)
        self.allocate(node, start_layer, end_layer)

    def declare(self, node: Node) -> None:
        """Declare a node to the allocator."""
        if node.node_id not in self.node_id_to_node:
            self.nodes.append(node)
            self.node_id_to_node[node.node_id] = node
        # Keep order deterministic without rebinding the list reference
        self.nodes.sort(key=lambda node: node.get_decoder_layer_capacity(), reverse=True)
        logger.debug("Declared node %s (total declared: %d)", node.node_id, len(self.nodes))

    def join(self, node: Node) -> None:
        """Dynamically assign a new node based on lightest layers."""
        logger.debug("Joining node dynamically: %s", node.node_id)
        self.declare(node)
        lightest_layer = self.get_lightest_layer()
        logger.debug("Lightest layer: %s", lightest_layer)
        if lightest_layer is None:
            raise ValueError("No layers to assign")

        # Assign consecutive layers starting from the lightest layer
        start_layer = lightest_layer.layer_id
        # Greedily assign layers that the node can host
        end_layer = self._adjust_end_layer_for_tail(node, start_layer)
        logger.debug(
            "Dynamic assignment candidate for %s: start=%d end=%d",
            node.node_id,
            start_layer,
            end_layer,
        )
        self.allocate(node, start_layer, end_layer)

    def leave(self, node_id: str) -> None:
        """Dynamically remove a node, update layer loads and pipeline endpoints."""
        node = self.node_id_to_node.get(node_id)
        if node is None:
            raise ValueError(f"Node {node_id} not found in allocation")
        logger.debug("Node leaving allocator: %s", node_id)
        self.deallocate(node)
        del self.node_id_to_node[node_id]
        # Ensure the shared nodes list is updated
        for node in self.nodes:
            if node.node_id == node_id:
                self.nodes.remove(node)
                break

    def allocate_left_over_nodes(self) -> None:
        """Assign any nodes without allocations by treating them as dynamic joins.

        During bootstrapping or after a global allocation, some nodes may remain
        unassigned because they cannot contribute to a full pipeline. This method
        assigns such nodes one-by-one using the same policy as dynamic `join`:
        repeatedly host the lightest layers to improve replication and balance.
        """
        logger.debug("Allocating left-over nodes (unassigned after global allocation)")
        # Iterate in capacity order for determinism and better packing
        for node in sorted(self.nodes, key=lambda n: n.get_decoder_layer_capacity(), reverse=True):
            if node.node_id not in self.node_allocation:
                try:
                    logger.debug("Attempting left-over allocation for %s", node.node_id)
                    self.join(node)
                except Exception:
                    # Best-effort: if no layers can be assigned, skip
                    logger.debug(
                        "Left-over allocation skipped for %s (no assignable layers)", node.node_id
                    )
                    continue

    def should_global_rebalance(self) -> bool:
        """Trigger global rebalance, i.e. re-run `initialize`  if load imbalance is too high.

        The method calculates a combined, normalized load for each layer based
        on its memory and FLOPs usage relative to the total cluster capacity.
        It then computes the coefficient of variation (std_dev / mean) of these
        loads. If this value exceeds a configurable threshold, it indicates
        significant imbalance and returns True.
        """

        # If we don't currently have a full pipeline covering [0, L), force rebalance
        if not self.has_full_pipeline():
            return True

        layer_heap = self.layer_loads_heap
        if len(layer_heap) < 2:
            return False

        total_cluster_memory = sum(
            (node.hardware.num_gpus * node.hardware.memory_gb) for node in self.nodes
        )

        if total_cluster_memory == 0:
            raise ValueError("Total cluster memory is zero")

        loads = [layer.current_kv_size / total_cluster_memory for layer in layer_heap]

        if not loads:
            return False

        avg_load = sum(loads) / len(loads)
        if avg_load == 0:
            return False

        variance = sum((load - avg_load) ** 2 for load in loads) / len(loads)
        std_dev = variance**0.5

        coefficient_of_variation = std_dev / avg_load

        decision = coefficient_of_variation > self.rebalance_threshold
        logger.debug(
            "Global rebalance check: cv=%.4f threshold=%.4f -> %s",
            coefficient_of_variation,
            self.rebalance_threshold,
            decision,
        )
        return decision

    def adjust_pipeline_layers(
        self,
        pipeline_nodes: List[Node],
        assume_sorted: bool = False,
        power_type: Literal["flops", "bandwidth"] = "flops",
    ) -> None:
        """Rebalance a single pipeline in-place using water-filling and set allocations on nodes.

        Adjusts `start_layer`/`end_layer` on the given `pipeline_nodes` so that:
        - Decoder layers are split proportional to node compute (TFLOPS or bandwidth),
          capped by each node's parameter capacity;
        - The first node reserves input embedding capacity; the last reserves LM head;
        - Assigned stages are contiguous from layer 0 to the final layer.

        Args:
            pipeline_nodes: Nodes that form a full pipeline from embedding to LM head.
            assume_sorted: If True, nodes are assumed sorted by decoder-layer capacity (desc).
            power_type: Type of compute power to use for rebalancing.

        Returns:
            None. Nodes are updated in-place via `set_layer_allocation`.
        """

        total_layers = self.num_total_layers
        if not pipeline_nodes or total_layers <= 0:
            raise ValueError("No nodes or total layers is non-positive")

        nodes = (
            pipeline_nodes
            if assume_sorted
            else sorted(pipeline_nodes, key=lambda n: n.get_decoder_layer_capacity(), reverse=True)
        )
        n = len(nodes)

        # Clear previous allocations for participating nodes (avoid double counting loads)
        for node in nodes:
            if node.start_layer is not None and node.end_layer is not None:
                self.deallocate(node)

        caps: List[int] = []
        compute_powers: List[float] = []
        for i, node in enumerate(nodes):
            if i == 0:
                cap = node.get_decoder_layer_capacity(include_input_embed=True)
            elif i == n - 1:
                cap = node.get_decoder_layer_capacity(include_lm_head=True)
            else:
                cap = node.get_decoder_layer_capacity()
            if cap <= 0:
                raise ValueError(f"Node {node.node_id} has non-positive capacity: {cap}")
            caps.append(cap)
            compute_powers.append(
                node.hardware.tflops_fp16
                if power_type == "flops"
                else node.hardware.memory_bandwidth_gbps
            )

        if sum(caps) < total_layers:
            raise ValueError(f"Total capacity {sum(caps)} is less than total layers {total_layers}")

        # Water-filling: find lambda s.t. sum_i min(c_i, Î» F_i) == L
        def total_at(lmbd: float) -> float:
            return sum(min(caps[i], lmbd * compute_powers[i]) for i in range(n))

        lo, hi = 0.0, max((caps[i] / compute_powers[i]) for i in range(n))
        for _ in range(self.water_filling_max_iterations):
            mid = 0.5 * (lo + hi)
            if total_at(mid) >= total_layers:
                hi = mid
            else:
                lo = mid
        lam = hi

        target = [min(caps[i], lam * compute_powers[i]) for i in range(n)]

        # Integerization: floor + largest remainders (respect caps)
        stage_layer_counts = [min(caps[i], int(floor(target[i]))) for i in range(n)]
        assigned = sum(stage_layer_counts)
        remaining = total_layers - assigned
        if remaining > 0:
            frac = [(target[i] - stage_layer_counts[i], -i) for i in range(n)]
            for _, negi in sorted(frac, reverse=True):
                i = -negi
                if remaining == 0:
                    break
                room = caps[i] - stage_layer_counts[i]
                if room > 0:
                    stage_layer_counts[i] += 1
                    remaining -= 1
        elif remaining < 0:
            raise ValueError(f"Remaining {remaining} is negative")

        # Final clamp (safety) and residual distribute, if any
        extra = 0
        for i in range(n):
            if stage_layer_counts[i] > caps[i]:
                extra += stage_layer_counts[i] - caps[i]
                stage_layer_counts[i] = caps[i]
        if extra > 0:
            for i in range(n):
                if extra == 0:
                    break
                room = caps[i] - stage_layer_counts[i]
                take = min(room, extra)
                stage_layer_counts[i] += take
                extra -= take

        # Apply contiguous assignments in stage order directly to nodes
        start_layer = 0
        for idx, node in enumerate(nodes):
            layers = stage_layer_counts[idx]
            if layers <= 0:
                # TODO(chris-t): should we deallocate the node?
                continue
            end_layer = start_layer + layers
            self.allocate(node, start_layer, end_layer)
            start_layer = end_layer

        # Sanity check: ensure coverage from 0..num_total_layers
        if start_layer != total_layers:
            raise ValueError(
                f"Assignment did not cover all layers: assigned {start_layer} of {total_layers}"
            )

    def adjust_pipeline_layers_greedy(self, pipeline_nodes: List[Node]) -> None:
        """Greedily assign contiguous layers to `pipeline_nodes` from 0 to L.

        This simpler alternative to `adjust_pipeline_layers` walks nodes in the given
        order and assigns as many layers as each node can host based on its capacity.
        The first node includes input embedding allowance; the final closing node
        includes LM head allowance. Previous allocations for participating nodes are
        cleared to avoid double counting load.

        Args:
            pipeline_nodes: Nodes that form a full pipeline in stage order.

        Raises:
            ValueError: If total capacity is insufficient to cover all layers.
        """

        total_layers = self.num_total_layers
        if not pipeline_nodes or total_layers <= 0:
            raise ValueError("No nodes or total layers is non-positive")

        # Clear previous allocations for participating nodes (avoid double counting loads)
        for node in pipeline_nodes:
            if node.start_layer is not None and node.end_layer is not None:
                self.deallocate(node)

        start_layer = 0
        remaining_layers = total_layers

        for idx, node in enumerate(pipeline_nodes):
            include_input_embed = start_layer == 0

            # Base capacity without LM head
            base_cap = node.get_decoder_layer_capacity(include_input_embed=include_input_embed)

            # If this node will be the tail that closes the pipeline, allow LM head
            if base_cap >= remaining_layers:
                tail_cap = node.get_decoder_layer_capacity(
                    include_input_embed=include_input_embed, include_lm_head=True
                )
                assign_layers = min(tail_cap, remaining_layers)
            else:
                assign_layers = min(base_cap, remaining_layers)

            if assign_layers <= 0:
                continue

            end_layer = start_layer + assign_layers
            self.allocate(node, start_layer, end_layer)
            start_layer = end_layer
            remaining_layers -= assign_layers

            if remaining_layers == 0:
                break

        if remaining_layers != 0:
            raise ValueError(
                f"Greedy assignment did not cover all layers: remaining {remaining_layers}"
            )

    def list_node_allocations(self) -> List[Tuple[str, int, int]]:
        """List current per-node layer allocations as (node_id, start_layer, end_layer).

        Nodes without an allocation are omitted. Results are sorted by start_layer.
        """
        items = [(node_id, se[0], se[1]) for node_id, se in self.node_allocation.items()]
        items.sort(key=lambda x: (x[1], x[2], x[0]))
        return items

    def get_lightest_layer(self) -> Optional[LayerLoad]:
        """Return the current lightest-hosted layer from the heap, if any."""
        if not self.layer_loads_heap:
            return None
        return self.layer_loads_heap[0]

    def _update_layer_loads_heap(self):
        """Rebuild the layer loads heap."""
        self.layer_loads_heap = list(self.layer_to_load.values())
        heapq.heapify(self.layer_loads_heap)

    def _adjust_end_layer_for_tail(self, node: Node, proposed_start_layer: int) -> int:
        """Adjust the number of layers to host for tail nodes."""
        include_input_embed = proposed_start_layer == 0
        node_capacity = node.get_decoder_layer_capacity(include_input_embed=include_input_embed)
        end_layer = min(proposed_start_layer + node_capacity, self.num_total_layers)
        if end_layer == self.num_total_layers:
            adjusted_capacity = node.get_decoder_layer_capacity(
                include_lm_head=True, include_input_embed=include_input_embed
            )
            end_layer = min(proposed_start_layer + adjusted_capacity, self.num_total_layers)

        return end_layer

    def has_full_pipeline(self, active_only: bool = False) -> bool:
        """Return True if there exists at least one pipeline covering [0, num_total_layers).

        Checks whether we can chain contiguous node allocations starting at 0 to reach L.
        This requires that there exists at least one node starting at layer 0 and a chain
        of contiguous node ranges that reaches num_total_layers.
        """
        total_layers = self.num_total_layers

        # Build index of nodes by start_layer
        start_to_nodes: Dict[int, List[Node]] = {}
        for node_id, (s, e) in self.node_allocation.items():
            if s is None or e is None:
                continue
            node = self.node_id_to_node.get(node_id)
            if node is None or (active_only and not node.is_active):
                continue
            start_to_nodes.setdefault(s, []).append(node)

        # Must have at least one node starting at layer 0
        if not start_to_nodes.get(0):
            return False

        # DFS to check if we can reach total_layers from any head node
        def can_reach_target(current_end: int) -> bool:
            if current_end >= total_layers:
                return current_end == total_layers

            for nxt in start_to_nodes.get(current_end, []):
                if nxt.end_layer and nxt.end_layer > current_end:
                    if can_reach_target(nxt.end_layer):
                        return True
            return False

        return any(
            head.end_layer and can_reach_target(head.end_layer)
            for head in start_to_nodes.get(0, [])
        )

    def layer_replication_stats(self) -> Tuple[int, int, float]:
        """Return (min, max, avg) number of nodes hosting each layer.

        Counts the number of hosting nodes per layer from `layer_to_load` and
        aggregates basic statistics. If there are no layers, returns (0, 0, 0.0).
        """
        if not self.layer_to_load:
            return 0, 0, 0.0
        counts = [len(layer.hosting_nodes) for layer in self.layer_to_load.values()]
        if not counts:
            return 0, 0, 0.0
        min_hosts = min(counts)
        max_hosts = max(counts)
        avg_hosts = float(sum(counts)) / float(len(counts))
        return min_hosts, max_hosts, avg_hosts


class GreedyLayerAllocator(BaseLayerAllocator):
    """
    Greedy layer allocator that assigns layers to nodes trying to
    1. Minimize number of stages in each pipeline;
    2. Maximize number of pipelines.

    The algorithm proceeds as follows:
    1.  Initialization: Nodes are sorted by their layer capacity in descending
        order, so nodes with more memory are considered first.

    2.  Iterative Pipeline Construction: The allocator attempts to build one
        pipeline at a time. It iterates through the sorted nodes, adding them
        to the current pipeline until the total number of layers required by
        the model is met. Greedy in the sense that it always assigns the max
        layers allowed by capacity of the current node to the pipeline.
        So to minimize number of stages.

    3.  Look-Ahead Optimization: Before selecting a node, the algorithm "looks ahead"
        to see if the remaining nodes have enough capacity to form at least one more
        full pipeline.
        - If so, it chooses the *smallest* node that can complete the current
          pipeline. This preserves the larger nodes for future pipelines.
        - If not, it falls back to the default greedy behavior and picks the
          *largest* available node to complete the pipeline as efficiently as
          possible.
        So to maximize number of pipelines.

    4.  In-Place Rebalancing: After each pipeline is formed, the layers within
        it are immediately rebalanced to be proportional to the computing power
        (TFLOPS) of the nodes. Capped by capacity. This ensures that the stages
        are balanced and system throughput is maximized.
        We use water-filling algorithm.


    This process repeats until no more complete pipelines can be formed from the
    remaining nodes.
    """

    def init(
        self,
        *,
        look_ahead_enable: bool = True,
        pipeline_rebalance_strategy: Literal["greedy", "water_filling"] = "water_filling",
    ) -> None:
        """Initialize Greedy allocator runtime knobs.

        Args:
            look_ahead_enable: Toggle the look-ahead optimization when selecting
                the tail node to close a pipeline.
            pipeline_rebalance_strategy: Strategy for in-place per-pipeline
                rebalancing after a pipeline is formed. "greedy" assigns
                contiguous layers in capacity order; "water_filling" uses
                proportional water-filling based on compute power.

        Returns:
            None. Sets internal flags `_look_ahead_enable` and
            `_pipeline_rebalance_strategy`.
        """
        self._look_ahead_enable = look_ahead_enable
        self._pipeline_rebalance_strategy = pipeline_rebalance_strategy

    def global_allocation(self) -> bool:
        """
        Allocate layers to nodes greedily to maximize the number of pipelines.

        Builds pipelines from the sorted nodes and uses `adjust_pipeline_layers`
        to assign contiguous layer ranges on each pipeline.
        """
        logger.debug(
            "[Greedy] Starting global_allocation with %d nodes for %d layers",
            len(self.nodes),
            self.model_info.num_layers,
        )
        num_total_layers = self.model_info.num_layers

        available_nodes = self.nodes.copy()
        for n in available_nodes:
            logger.warning(f"Node {n.node_id} has capacity {n.get_decoder_layer_capacity()}")
        any_assigned = False

        # Read runtime knobs with sensible defaults if `init` wasn't called
        look_ahead_enabled = getattr(self, "_look_ahead_enable", True)
        rebalance_strategy = getattr(self, "_pipeline_rebalance_strategy", "water_filling")

        while available_nodes:
            total_remaining_capacity = sum(
                node.get_decoder_layer_capacity() for node in available_nodes
            )
            if total_remaining_capacity < num_total_layers:
                logger.debug(
                    "[Greedy] Remaining capacity %d < total layers %d; stop",
                    total_remaining_capacity,
                    num_total_layers,
                )
                break

            pipeline_nodes: List[Node] = []
            remaining_layers = num_total_layers
            current_pipeline_total_capacity = total_remaining_capacity

            while remaining_layers > 0 and available_nodes:
                is_start = len(pipeline_nodes) == 0
                # Look-ahead optimization (only for picking the last node to finish a pipeline)
                look_ahead_possible = (
                    look_ahead_enabled
                    and is_start
                    and current_pipeline_total_capacity - remaining_layers >= num_total_layers + 1
                )
                best_fit_idx = -1
                if look_ahead_possible:
                    # Find smallest node that can complete the pipeline while leaving enough for another full pipeline
                    for i, node in enumerate(available_nodes):
                        node_i_capacity = node.get_decoder_layer_capacity(include_lm_head=True)
                        if node_i_capacity >= remaining_layers:
                            remaining_nodes_capacity = (
                                current_pipeline_total_capacity - node_i_capacity
                            )
                            if remaining_nodes_capacity >= num_total_layers:
                                best_fit_idx = (
                                    i  # choose the last matching (smallest due to sorting)
                                )

                node_to_add = (
                    available_nodes.pop(best_fit_idx)
                    if best_fit_idx != -1
                    else available_nodes.pop(0)
                )

                pipeline_nodes.append(node_to_add)
                # Update running totals with appropriate capacity at this position
                node_capacity = node_to_add.get_decoder_layer_capacity(include_input_embed=is_start)
                remaining_layers -= node_capacity
                if remaining_layers <= 0:
                    # Tail node can include LM head allowance
                    remaining_layers += node_capacity
                    node_capacity = node_to_add.get_decoder_layer_capacity(
                        include_input_embed=is_start, include_lm_head=True
                    )
                    remaining_layers -= node_capacity

                current_pipeline_total_capacity -= node_capacity

            if remaining_layers <= 0 and pipeline_nodes:
                # Assign layers within this pipeline in-place
                logger.debug(
                    "[Greedy] Built pipeline with %d nodes; adjusting layers",
                    len(pipeline_nodes),
                )
                if rebalance_strategy == "greedy":
                    self.adjust_pipeline_layers_greedy(pipeline_nodes)
                else:
                    self.adjust_pipeline_layers(pipeline_nodes, assume_sorted=False)
                any_assigned = True
            else:
                # Cannot form a complete pipeline with remaining nodes
                logger.debug("[Greedy] Unable to form complete pipeline; stopping")
                break

        if not any_assigned or not self.has_full_pipeline():
            logger.debug("[Greedy] global_allocation produced no full pipeline")
            return False
        # Assign any nodes that were left unallocated using dynamic policy
        if self.assign_left_over_nodes:
            logger.debug("[Greedy] Assigning left-over nodes")
            self.allocate_left_over_nodes()
        logger.debug("[Greedy] global_allocation completed successfully")
        return True


class DynamicProgrammingLayerAllocator(BaseLayerAllocator):
    """
    Dynamic programming based allocator that balances two objectives:
     - Concurrency (number of pipelines)
     - Latency (number of stages per pipeline).

    Why DP (vs greedy): interleaving constructions of multiple pipelines
    so cases like capacities (40, 40, 20, 20, 10, 10) with total 70 layers
    yields (40, 20, 10) + (40, 20, 10) instead of a single 2-stage pipe line (40 + 30).

    We want to maximize a simple scalar objective function:
        Z(k) = k / (s*(k) / k) = k^2 / s*(k)
    where:
        k: number of pipelines
        s*(k): minimum total number of stages realizing k pipelines

    DP State:
        dp(i, open_residuals, finshed_pipes) := min total stages needed using GPUs with index >= i;
        where:
            i: GPU index, in [0, N]
            open_residuals: sorted tuple of remaining layers for all open pipelines (values in 1..L-1)
            finished_pipes: number of already fully closed pipelines
        Transitions (for node i with capacity c_i):
            1. Skip node: dp(i + 1, open_residuals, finished_pipes)
            2. Assign to an existing open pipeline j:
               r' = r_j - c_i. If r' <= 0, try closing with LM head; if still <= 0 -> close (remove j, finished+1),
               else keep open with updated residual r'.
            3. Start a new pipeline (if finished + len(open_residuals) < k_target):
               r = L - c_i (with input embedding). If r <= 0, it closes immediately (finished+1), else append r.

    Finally:
        Compute objective Z(k) = (k**alpha) / (T_comp + (total_stages/k)*r_RTT)
        Choose the best k and backtrack to recover assignments
    """

    def __init__(
        self,
        model_info: ModelInfo,
        nodes: List[Node],
        alpha: float = 2.0,
        *,
        assign_left_over_nodes: bool = True,
        rebalance_threshold: float = 0.25,
        water_filling_max_iterations: int = 40,
    ) -> None:
        super().__init__(
            model_info,
            nodes,
            rebalance_threshold=rebalance_threshold,
            water_filling_max_iterations=water_filling_max_iterations,
        )
        # Sort GPUs by layer capacity descending for stronger pruning
        self.alpha = alpha
        self._path: Dict[Tuple[int, Tuple[int, ...], int], Tuple] = {}

    def global_allocation(self) -> bool:
        logger.debug(
            "[DP] Starting global_allocation with %d nodes for %d layers",
            len(self.nodes),
            self.model_info.num_layers,
        )
        num_nodes = len(self.nodes)
        num_layers = int(self.model_info.num_layers)
        total_cap = sum(node.get_decoder_layer_capacity() for node in self.nodes)

        if num_layers <= 0 or num_nodes == 0 or total_cap < num_layers:
            logger.warning(
                "[DP] Insufficient resources: nodes=%d, layers=%d, total_cap=%d",
                num_nodes,
                num_layers,
                total_cap,
            )
            return False
        else:
            logger.debug(
                "[DP] Sufficient resources: nodes=%d, layers=%d, total_cap=%d",
                num_nodes,
                num_layers,
                total_cap,
            )
        # used for pruning
        suffix_sum = [0] * (num_nodes + 1)
        for i in range(num_nodes - 1, -1, -1):
            suffix_sum[i] = suffix_sum[i + 1] + self.nodes[i].get_decoder_layer_capacity()

        max_num_pipes = min(num_nodes, total_cap // num_layers)
        best_num_pipes = 0
        best_score: float = float("-inf")

        best_path: Dict[Tuple[int, Tuple[int, ...], int], Tuple] = {}
        for k_target in range(1, max_num_pipes + 1):
            path: Dict[Tuple[int, Tuple[int, ...], int], Tuple] = {}

            @lru_cache(maxsize=None)
            def dp(i: int, open_residuals: Tuple[int, ...], finished_pipes: int) -> int:
                # Completed target with no open pipelines
                if finished_pipes == k_target and len(open_residuals) == 0:
                    path[(i, open_residuals, finished_pipes)] = ("done",)
                    return 0
                if i == num_nodes:
                    return float("inf")

                new_needed = k_target - finished_pipes - len(open_residuals)
                need_open = sum(open_residuals)
                remaining_cap = suffix_sum[i]

                # Pruning
                # 1. already have more (finished + open) than target;
                # 2. remaining capacity is not enough to close ongoing pipelines
                #    and unfulfilled new pipelines
                # 3. remaining nodes are not enough to fulfill new pipelines
                if (
                    new_needed < 0
                    or remaining_cap < need_open + max(0, new_needed) * num_layers
                    or finished_pipes + len(open_residuals) + (num_nodes - i) < k_target
                ):
                    return float("inf")

                # Option 1: Skip this node
                best_cost = dp(i + 1, open_residuals, finished_pipes)
                best_action: Tuple = ("skip",)

                # Option 2: Assign to existing open pipeline
                for j, rj in enumerate(open_residuals):
                    c_norm = self.nodes[i].get_decoder_layer_capacity()
                    r_after = rj - c_norm
                    if r_after <= 0:
                        # try closing with LM head allowance
                        c_close = self.nodes[i].get_decoder_layer_capacity(include_lm_head=True)
                        r_after_close = rj - c_close
                        if r_after_close <= 0:
                            new_open = list(open_residuals)
                            new_open.pop(j)
                            cost = 1 + dp(i + 1, tuple(new_open), finished_pipes + 1)
                            if cost < best_cost:
                                best_cost = cost
                                best_action = ("assign", j, True)
                        else:
                            new_open = list(open_residuals)
                            new_open[j] = r_after_close
                            new_open.sort()
                            cost = 1 + dp(i + 1, tuple(new_open), finished_pipes)
                            if cost < best_cost:
                                best_cost = cost
                                best_action = ("assign", j, False)
                    else:
                        new_open = list(open_residuals)
                        new_open[j] = r_after
                        new_open.sort()
                        cost = 1 + dp(i + 1, tuple(new_open), finished_pipes)
                        if cost < best_cost:
                            best_cost = cost
                            best_action = ("assign", j, False)

                # Option 3: start a new pipeline (if we still need more)
                if new_needed > 0:
                    c_start = self.nodes[i].get_decoder_layer_capacity(include_input_embed=True)
                    r_new = num_layers - c_start
                    if r_new <= 0:
                        cost = 1 + dp(i + 1, open_residuals, finished_pipes + 1)
                        if cost < best_cost:
                            best_cost = cost
                            best_action = ("start", 0, True)
                    else:
                        new_open = list(open_residuals) + [r_new]
                        new_open.sort()
                        cost = 1 + dp(i + 1, tuple(new_open), finished_pipes)
                        if cost < best_cost:
                            best_cost = cost
                            best_action = ("start", r_new, False)

                path[(i, open_residuals, finished_pipes)] = best_action
                return best_cost

            s_star = dp(0, tuple(), 0)
            if s_star < float("inf"):
                score = (k_target * k_target) / s_star  # Z(k) = k^2 / s*(k)
                if score > best_score:
                    best_score, best_num_pipes = score, k_target
                    best_path = dict(path)

        if best_num_pipes is None or best_num_pipes == 0:
            logger.debug("[DP] Could not find a feasible number of pipelines")
            return False
        self._path = best_path
        pipelines = self._backtrack(best_num_pipes, num_nodes)

        # Assign layers for each pipeline via in-place rebalancing
        for pl_nodes in pipelines:
            if not pl_nodes:
                continue
            logger.debug("[DP] Adjusting pipeline with %d nodes", len(pl_nodes))
            self.adjust_pipeline_layers(pl_nodes, assume_sorted=False)
        # Assign any nodes that were left unallocated using dynamic policy
        if self.assign_left_over_nodes:
            logger.debug("[DP] Assigning left-over nodes")
            self.allocate_left_over_nodes()
        if not self.has_full_pipeline():
            logger.debug("[DP] Allocation did not produce a full pipeline")
            return False
        logger.debug("[DP] global_allocation completed successfully")
        return True

    def _backtrack(self, best_num_pipes: int, num_nodes: int) -> List[List[Node]]:
        # Reconstruct pipelines
        logger.debug("[DP] Backtracking to construct %d pipelines", best_num_pipes)
        pipelines: List[List[Node]] = [[] for _ in range(best_num_pipes)]
        # (residual, nodes list)
        open_list: List[Tuple[int, List[Node]]] = []
        i = 0
        finished = 0
        while i < num_nodes and finished < best_num_pipes:
            open_tuple = tuple(sorted(r for r, _ in open_list))
            action = self._path.get((i, open_tuple, finished))
            if action is None:
                break
            kind = action[0]
            if kind in ("skip", "done"):
                i += 1
                continue
            node = self.nodes[i]
            if kind == "assign":
                j, closed = action[1], action[2]
                # ensure open_list sorted like open_tuple
                open_list.sort(key=lambda x: x[0])
                rj, nodes_seq = open_list[j]
                c_norm = node.get_decoder_layer_capacity()
                r_after = rj - c_norm
                if r_after <= 0:
                    c_close = node.get_decoder_layer_capacity(include_lm_head=True)
                    r_after = rj - c_close
                nodes_seq.append(node)
                if r_after <= 0 or closed:
                    # pipeline closes here
                    pipelines[finished].extend(nodes_seq)
                    open_list.pop(j)
                    finished += 1
                else:
                    open_list[j] = (r_after, nodes_seq)
                i += 1
            elif kind == "start":
                r_new, closed = action[1], action[2]
                if closed:
                    pipelines[finished].append(node)
                    finished += 1
                else:
                    open_list.append((r_new, [node]))
                i += 1
            else:
                # Unknown action; advance to avoid infinite loop
                i += 1

        return pipelines


================================================================================
File: src/scheduling/model_info.py
Size: 7.01 kB
================================================================================

"""
ModelInfo class for scheduling.

Abstraction of transformer models containing key architectural configurations
used for FLOPs and I/O estimation. This information is later consumed by
nodes within the scheduling system to make informed layer allocation
and performance estimation decisions.
"""

from dataclasses import dataclass
from typing import Optional

from parallax_utils.logging_config import get_logger

logger = get_logger(__name__)


@dataclass
class ModelInfo:
    """
    Abstraction of transformer model architecture for scheduling decisions.
    Assuming all decoder layers have uniform computational and memory requirements
    """

    model_name: str
    mlx_model_name: str
    head_size: int
    hidden_dim: int
    intermediate_dim: int
    num_attention_heads: int
    num_kv_heads: int
    vocab_size: int
    num_layers: int
    ffn_num_projections: int = 3
    num_local_experts: Optional[int] = None
    num_experts_per_tok: Optional[int] = None
    moe_intermediate_dim: Optional[int] = None
    tie_embedding: bool = False
    # Default int8
    param_bytes_per_element: float = 1
    mlx_param_bytes_per_element: float = 1
    cache_bytes_per_element: int = 1
    embedding_bytes_per_element: int = 1

    qk_nope_head_dim: Optional[int] = None
    qk_rope_head_dim: Optional[int] = None
    head_size_k: int = None
    head_size_v: int = None

    def __init__(self, **kwargs):
        for key, value in kwargs.items():
            setattr(self, key, value)

        if self.qk_nope_head_dim is not None and self.qk_rope_head_dim is not None:
            self.head_size_k = self.qk_nope_head_dim + self.qk_rope_head_dim
        else:
            self.head_size_k = self.head_size
        self.head_size_v = self.head_size

    @property
    def q_dim(self) -> int:
        """Return query head dim."""
        return self.num_attention_heads * self.head_size

    @property
    def v_dim(self) -> int:
        """Return key and value head dim."""
        return self.num_kv_heads * self.head_size_v

    @property
    def k_dim(self) -> int:
        """Return key head dim."""
        return self.num_kv_heads * self.head_size_k

    @property
    def mlx_bit_factor(self) -> float:
        return self.mlx_param_bytes_per_element / self.param_bytes_per_element

    @property
    def embedding_io_bytes(self) -> int:
        """Estimate memory for input_embeddings / or lm_head."""
        return self.embedding_bytes_per_element * self.vocab_size * self.hidden_dim

    @property
    def per_token_per_layer_kv_size(self) -> int:
        """Return bytes per token for KV cache."""
        return self.cache_bytes_per_element * (self.k_dim + self.v_dim)

    def per_layer_kv_cache_size(self, *, batch_size: int = 1, source_seq_len: int = 256) -> int:
        """Return size of KV cache in bytes for given request dimensions."""
        return self.per_token_per_layer_kv_size * batch_size * source_seq_len

    def expected_num_activated_experts(
        self, *, batch_size: int = 1, target_seq_len: int = 1
    ) -> Optional[int]:
        """Return expected number of activated experts for a request size."""
        num_tokens = batch_size * target_seq_len
        if self.num_local_experts is not None and self.num_experts_per_tok is not None:
            return int(
                self.num_local_experts
                * (1 - (1 - self.num_experts_per_tok / self.num_local_experts) ** num_tokens)
            )
        return None

    def decoder_layer_flops(
        self,
        *,
        batch_size: int = 1,
        target_seq_len: int = 1,
        source_seq_len: int = 256,
    ) -> int:
        """
        Estimate FLOPs per decoder layer including attention and FFN components.
        For GEMM: (M, K) @ (K, N) = (M, N), flops would be 2 * M * K * N;
        """
        # Attention
        # Q/O projections: (T, hidden_dim) @ (hidden_dim, hidden_dim)
        qo_flops = 2 * 2 * target_seq_len * self.hidden_dim * self.hidden_dim
        # K/V projections: (T, hidden_dim) @ (hidden_dim, kv_dim)
        kv_flops = 2 * target_seq_len * self.hidden_dim * (self.k_dim + self.v_dim)
        projection_flops = qo_flops + kv_flops

        # 'roof' estimation for GQA
        # score: (T, hidden_dim) @ (hidden_dim, S)
        score_flops = 2 * self.hidden_dim * target_seq_len * source_seq_len
        # weight: (T, S) @ (S, hidden_dim)
        attn_v_flops = 2 * self.hidden_dim * target_seq_len * source_seq_len

        attention_flops = projection_flops + score_flops + attn_v_flops

        # Dense FFN
        ffn_flops = (
            2 * self.ffn_num_projections * target_seq_len * self.hidden_dim * self.intermediate_dim
        )
        # Sparse MoE FFN, if applicable
        expected_experts = self.expected_num_activated_experts(
            batch_size=batch_size, target_seq_len=target_seq_len
        )
        if expected_experts is not None:
            ffn_flops *= expected_experts

        return batch_size * (attention_flops + ffn_flops)

    def decoder_layer_io_bytes(
        self,
        roofline: Optional[bool] = None,
        *,
        batch_size: int = 1,
        target_seq_len: int = 1,
        source_seq_len: int = 256,
    ) -> int:
        """
        Estimate memory per decoder layer in bytes including params and kv cache.

        Args:
            roofline: True if calculation is for roofline io latency, otherwise for param size estimation.
            batch_size: Request batch size
            target_seq_len: Target sequence length (tokens to generate)
            source_seq_len: Source sequence length (prompt tokens)
        """
        # Attention params
        qo_params = self.param_bytes_per_element * self.hidden_dim * self.q_dim * 2
        kv_params = self.param_bytes_per_element * self.hidden_dim * (self.k_dim + self.v_dim)
        attention_params = qo_params + kv_params

        # FFN params
        ffn_params = self.param_bytes_per_element * self.ffn_num_projections * self.hidden_dim
        if self.moe_intermediate_dim is not None:
            ffn_params *= self.moe_intermediate_dim
        else:
            ffn_params *= self.intermediate_dim

        if roofline:
            expected_experts = self.expected_num_activated_experts(
                batch_size=batch_size, target_seq_len=target_seq_len
            )
            if expected_experts is not None:
                ffn_params *= expected_experts
            kv_cache_size = self.per_layer_kv_cache_size(
                batch_size=batch_size, source_seq_len=source_seq_len
            )
        else:
            if self.num_local_experts is not None:
                ffn_params *= self.num_local_experts
            kv_cache_size = 0

        return round(ffn_params + kv_cache_size + attention_params)

    def lm_head_flops(self, target_seq_len: int = 1) -> int:
        """Estimate FLOPs for lm_head (last layer GEMM) for a sequence length."""
        return 2 * target_seq_len * self.hidden_dim * self.vocab_size


================================================================================
File: src/scheduling/node.py
Size: 14.57 kB
================================================================================

"""
Scheduling primitives for distributed LLM inference.

- `NodeHardwareInfo`: static hardware properties
- `RequestSignal`: minimal request envelope (id, received timestamp)
- `RooflinePerformanceModel`: compute/IO roofline estimator with configurable
  sequence/batch shape
- `Node`: worker serving state; manages layer allocation, capacity helpers,
  latency tracking, and RTT cache for network-aware request routing
"""

import time
from dataclasses import dataclass, field
from math import floor
from typing import Dict, List, Optional

from parallax_utils.logging_config import get_logger
from parallax_utils.utils import bytes_per_element, compute_max_batch_size
from scheduling.model_info import ModelInfo

logger = get_logger(__name__)


@dataclass
class NodeHardwareInfo:
    """
    Hardware-only description of a node.

    Contains static properties that do not depend on a specific model, and
    optionally cached RTTs to other nodes for network-aware decisions.
    """

    node_id: str
    num_gpus: int
    tflops_fp16: float
    gpu_name: str
    memory_gb: float
    memory_bandwidth_gbps: float
    device: str


@dataclass
class RequestSignal:
    """
    Minimal request signal container for scheduling.

    - request_id: Unique identifier (hash) for the request
    - received_ts: UNIX timestamp (seconds) when the request was received
    - routing_table: Set by the scheduler when a path is assigned. Semantics:
        None -> not assigned yet; [] -> all pipelines full at the moment; [..] -> route
    """

    request_id: str
    received_ts: float = field(default_factory=time.time)
    routing_table: Optional[List[str]] = None


class RooflinePerformanceModel:
    """
    Lightweight roofline-based performance estimator.

    Encapsulates compute- and IO-bound latency estimations for a given
    `(hardware, model_info)` pair. Sequence/batch shape can be updated to
    reflect current request context.
    """

    def __init__(
        self,
        hardware: NodeHardwareInfo,
        model_info: ModelInfo,
        quantization_speedup: float = 1.0,
        *,
        batch_size: int = 1,
        target_seq_len: int = 1,
        source_seq_len: int = 256,
        using_mlx: bool = False,
    ) -> None:
        self.tflops = hardware.tflops_fp16
        self.io_bandwidth = hardware.memory_bandwidth_gbps
        self.model_info = model_info
        self.quantization_speedup = quantization_speedup
        self.batch_size = batch_size
        self.target_seq_len = target_seq_len
        self.source_seq_len = source_seq_len
        self.using_mlx = using_mlx

    def get_compute_roofline_latency_ms(self, flops: int) -> float:
        """Compute-bound latency in milliseconds for the given floating-point ops."""
        return flops / (self.quantization_speedup * self.tflops * 1e9)

    def get_io_roofline_latency_ms(self, io_bytes: int) -> float:
        """Memory/IO-bound latency in milliseconds for the given data transfer size."""
        return io_bytes / (self.io_bandwidth * 1e6)

    def set_sequence_shape(
        self,
        *,
        batch_size: Optional[int] = None,
        target_seq_len: Optional[int] = None,
        source_seq_len: Optional[int] = None,
    ) -> None:
        """Convenience setter to update any of batch/target/source sequence sizes."""
        if batch_size is not None:
            self.batch_size = batch_size
        if target_seq_len is not None:
            self.target_seq_len = target_seq_len
        if source_seq_len is not None:
            self.source_seq_len = source_seq_len

    def roofline_layer_latency_ms(
        self,
        include_input_embed: bool = False,
        include_lm_head: bool = False,
        num_current_layers: int = 1,
    ) -> float:
        """Estimate latency to execute the specified layer set on this node.

        Args:
            include_input_embed: Whether to include input embedding I/O
            include_lm_head: Whether to include LM head compute and I/O
            num_current_layers: Number of decoder layers included

        Returns:
            Total latency (ms) combining decoder layers and optional endpoints.
        """
        decoder_layer_compute_latency = self.get_compute_roofline_latency_ms(
            self.model_info.decoder_layer_flops(
                batch_size=self.batch_size,
                target_seq_len=self.target_seq_len,
                source_seq_len=self.source_seq_len,
            )
        )
        model_btyes = self.model_info.decoder_layer_io_bytes(
            roofline=True,
            batch_size=self.batch_size,
            target_seq_len=self.target_seq_len,
            source_seq_len=self.source_seq_len,
        )
        if self.using_mlx:
            model_btyes *= self.model_info.mlx_bit_factor
        decoder_layer_io_latency = self.get_io_roofline_latency_ms(model_btyes)

        # For first / last layers
        flops, io_bytes = 0, 0
        if include_input_embed:
            # Embedding lookup is I/O-dominant
            io_bytes += self.model_info.embedding_io_bytes

        if include_lm_head:
            flops += self.model_info.lm_head_flops(self.target_seq_len)
            io_bytes += self.model_info.embedding_io_bytes

        compute_time_ms = self.get_compute_roofline_latency_ms(flops)
        io_time_ms = self.get_io_roofline_latency_ms(io_bytes)
        return (
            num_current_layers * max(decoder_layer_compute_latency, decoder_layer_io_latency)
            + max(compute_time_ms, io_time_ms)
        ) / num_current_layers


@dataclass
class Node:
    """
    Dynamic worker node's serving state and network-aware routing hooks.

    - Tracks layer allocation and request load;
    - Capacity helpers for layer allocation;
    - Latency tracking and estimation if not available from node broadcasting;
    - Networking: optional RTT cache and getter for on-demand RTT measurement.

    """

    node_id: str
    hardware: NodeHardwareInfo
    model_info: ModelInfo

    kvcache_mem_ratio: float = 0.3
    param_mem_ratio: float = 0.5

    max_concurrent_requests: int = 16
    max_sequence_length: int = 4096

    manual_layer_assignment: bool = False
    start_layer: Optional[int] = None  # inclusive
    end_layer: Optional[int] = None  # exclusive
    current_requests: int = 0

    # todo upload is_active
    is_active: bool = True
    last_heartbeat: float = 0.0
    # Will be updated by node broadcasting
    # otherwise, use roofline performance model to estimate
    avg_layer_latency_ms: Optional[float] = None
    load_compensator: float = 0.05

    rtt_to_nodes: Optional[Dict[str, float]] = None

    _force_max_concurrent_requests: bool = False

    def __post_init__(self):
        if self.last_heartbeat == 0.0:
            self.last_heartbeat = time.time()
        if self.rtt_to_nodes is None:
            self.rtt_to_nodes = {}

    @property
    def max_requests(self) -> int:
        """Max concurrent requests bounded by KV budget using sequence length."""
        if self._force_max_concurrent_requests:
            return self.max_concurrent_requests

        if self.start_layer is None or self.end_layer is None:
            return self.max_concurrent_requests
        try:
            elem_bytes = bytes_per_element(
                getattr(self.model_info, "cache_bytes_per_element", None)
            )
        except Exception:
            elem_bytes = 2
        derived_max = compute_max_batch_size(
            requested_max_batch_size=self.max_concurrent_requests,
            max_sequence_len=self.max_sequence_length,
            device=None,
            kv_cache_memory_fraction=self.kvcache_mem_ratio,
            num_shard_layers=self.num_current_layers,
            num_key_value_heads=self.model_info.num_kv_heads,
            head_dim=self.model_info.head_size,
            elem_bytes=elem_bytes,
            memory_gb=self.hardware.memory_gb,
            head_dim_k=self.model_info.head_size_k,
            head_dim_v=self.model_info.head_size_v,
        )
        if derived_max <= 0:
            raise ValueError(
                f"Node {self.node_id} has invalid max concurrent requests: {derived_max}"
            )
        if self.max_concurrent_requests is None:
            return derived_max
        else:
            return min(self.max_concurrent_requests, derived_max)

    @property
    def num_current_layers(self) -> int:
        """Number of currently allocated layers."""
        if self.start_layer is None or self.end_layer is None:
            return 0
        return self.end_layer - self.start_layer

    @property
    def has_embedding(self) -> bool:
        """Check if this node hosts the embedding layer (layer 0)."""
        if self.start_layer is None:
            return False
        return self.start_layer == 0

    @property
    def has_lm_head(self) -> bool:
        """Check if this node hosts the LM head layer (last layer)."""
        if self.end_layer is None:
            return False
        return self.end_layer == self.model_info.num_layers

    @property
    def is_overloaded(self) -> bool:
        """Check if node is at capacity for requests."""
        return self.current_requests >= self.max_requests

    def get_decoder_layer_capacity(
        self, include_input_embed: bool = False, include_lm_head: bool = False
    ) -> int:
        """Return how many decoder layers this node can store for parameters.

        Capacity is measured using the parameter memory budget on the device.
        """
        available_memory_bytes = floor(
            self.hardware.num_gpus
            * self.hardware.memory_gb
            * 1024
            * 1024
            * 1024
            * self.param_mem_ratio
        )
        if include_input_embed:
            available_memory_bytes -= self.model_info.embedding_io_bytes
        if include_lm_head:
            if not (include_input_embed and self.model_info.tie_embedding):
                available_memory_bytes -= self.model_info.embedding_io_bytes

        if self.hardware.device == "mlx":
            # For mlx, consider mlx bit factor
            return floor(
                available_memory_bytes
                / (
                    self.model_info.decoder_layer_io_bytes(roofline=False)
                    * self.model_info.mlx_bit_factor
                )
            )
        else:
            return floor(
                available_memory_bytes / self.model_info.decoder_layer_io_bytes(roofline=False)
            )

    @property
    def per_decoder_layer_kv_cache_memory(self) -> Optional[int]:
        """Return the available memory for kv cache per layer."""
        if self.num_current_layers == 0:
            return None
        return floor(
            (
                self.hardware.num_gpus
                * self.hardware.memory_gb
                * 1024
                * 1024
                * 1024
                * self.kvcache_mem_ratio
            )
            / self.num_current_layers
        )

    def set_layer_allocation(self, start_layer: int, end_layer: int) -> None:
        """Set the layer range allocated to this node."""
        self.start_layer = start_layer
        self.end_layer = end_layer

    def clear_layer_allocation(self) -> None:
        """Clear the layer allocation for this node."""
        self.start_layer = None
        self.end_layer = None

    def set_layer_latency_ms(self, latency_ms: float) -> None:
        """Update the layer latency for this node."""
        self.avg_layer_latency_ms = latency_ms

    def roofline_layer_latency_ms(self) -> float:
        """Get the roofline layer latency for this node."""
        # Compute an effective compute speedup due to quantization.
        bytes_per_elem = float(self.model_info.param_bytes_per_element)
        # bf16/fp16 baseline ~2 bytes
        base = 1.0 if bytes_per_elem <= 0 else 2.0 / bytes_per_elem
        # Empirical efficiency factor: int8 often achieves ~80% of theoretical 2x
        efficiency = 0.8 if bytes_per_elem < 2.0 else 1.0
        quantization_speedup = max(0.1, base * efficiency)
        perf_model = RooflinePerformanceModel(
            hardware=self.hardware,
            model_info=self.model_info,
            quantization_speedup=quantization_speedup,
            batch_size=self.current_requests,
            target_seq_len=1,
            source_seq_len=self.max_sequence_length,
            using_mlx=self.hardware.device == "mlx",
        )
        return perf_model.roofline_layer_latency_ms(
            include_input_embed=self.has_embedding,
            include_lm_head=self.has_lm_head,
            num_current_layers=self.num_current_layers,
        )

    @property
    def layer_latency_ms(self) -> float:
        """Get effective layer latency considering both roofline and load."""
        if self.is_overloaded:
            logger.warning(
                f"Node {self.node_id} is overloaded: {self.current_requests} >= {self.max_requests}"
            )
            return float("inf")
        if self.avg_layer_latency_ms is None:
            return self.roofline_layer_latency_ms()
        return self.avg_layer_latency_ms + self.load_compensator * (
            1.0 * self.current_requests / self.max_requests
        )

    def update_rtt(self, target_node_id: str, rtt_ms: float):
        """Update RTT measurement to another node."""
        self.rtt_to_nodes[target_node_id] = rtt_ms

    def get_rtt_to(self, other: "Node") -> float:
        """Get RTT to another node from cached RTTs.

        Returns:
            RTT in milliseconds, or float("inf") if no cached RTT exists.
        """
        if self == other:
            return 0.0
        if self.rtt_to_nodes is None:
            return float("inf")
        if other.node_id not in self.rtt_to_nodes:
            logger.warning("Cannot find RTT from node %s to node %s", self.node_id, other.node_id)
            return float("inf")
        return self.rtt_to_nodes[other.node_id]

    def hosts_layer(self, layer_id: int) -> bool:
        """Return True if this node hosts the given layer id.

        Interprets `current_layers` as a half-open interval [start, end).
        """
        if self.start_layer is None or self.end_layer is None:
            return False
        return self.start_layer <= layer_id < self.end_layer

    def add_request(self):
        """Add a request to this node."""
        self.current_requests += 1

    def remove_request(self):
        """Remove a request from this node."""
        self.current_requests -= 1


================================================================================
File: src/scheduling/request_routing.py
Size: 19.27 kB
================================================================================

"""
Phase 2: Request routing.

Provides:
- A base strategy interface.
- A dynamic-programming router that minimizes end-to-end latency across nodes.
- A round-robin router that uses round-robin over complete pipelines.

Routing is at node granularity: once a request enters a node, it runs all layers
hosted by that node. We can optionally compute layer-level turning points for a
warm-up phase (to inform rebalancing), then perform shard-level DP to produce the
final node path and total latency.
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Tuple

from parallax_utils.logging_config import get_logger
from scheduling.node import Node

logger = get_logger(__name__)


class RequestRoutingStrategy(ABC):
    """Base abstract class for request routing strategies."""

    @abstractmethod
    def find_turning_points(self, nodes: List[Node], num_layers: int) -> List[Tuple[str, int, str]]:
        """Find truncation points.

        Turning points mark where shards can be trimmed based on optimal routing.
        Returns a list of (node_id, layer_index, kind), where kind in {"head", "tail"}:
        - (node, l, "tail"): the route switches away at layer l although node still
          hosts l, so drop [l, end) on that node.
        - (node, l, "head"): the route first uses this node at layer l (> start),
          so drop [start, l) on that node.
        """

    @abstractmethod
    def find_optimal_path(self, nodes: List[Node], num_layers: int) -> Tuple[List[str], float]:
        """Shard-level DP path across nodes. Returns (node_ids, latency)."""


class DynamicProgrammingRouting(RequestRoutingStrategy):
    """
    Dynamic-programming router.

    - Warm-up: run a layer-level DP to identify turning points (where the optimal
      path switches nodes even if the current node still hosts the next layer).
    - Routing: run a shard-level DP over node assignments (contiguous layer ranges),
      using per-node execution latency and RTT via `Node.get_rtt_to`, to obtain a
      minimum-latency node sequence and total latency.
    """

    @staticmethod
    def find_turning_points(nodes: List[Node], num_layers: int) -> List[Tuple[str, int, str]]:
        """Find shard truncation points via layer-level DP.

        DP state is (layer l, node i that hosts l). Node cost uses the node's
        per-layer latency proxy; edge cost uses RTT between nodes.

        This is a static method that can be called directly without creating an instance:
        DynamicProgrammingRouting.find_turning_points(nodes, num_layers)

        It can also be called via an instance, which will work due to Python's method resolution.
        """
        if num_layers <= 0 or not nodes:
            return []

        # Build host lists per layer using start/end layer ranges
        layer_hosts: List[List[int]] = []
        for l in range(num_layers):
            hosts = [i for i, n in enumerate(nodes) if n.hosts_layer(l)]
            layer_hosts.append(hosts)

        # If any layer lacks a host, return empty
        if any(len(h) == 0 for h in layer_hosts):
            return []

        # layer_id: node_id -> cost
        dp: List[Dict[int, float]] = [{i: float("inf") for i in layer_hosts[0]}]
        back: List[Dict[int, Optional[int]]] = [{i: None for i in layer_hosts[0]}]

        # Init layer 0
        for i in layer_hosts[0]:
            dp[0][i] = nodes[i].layer_latency_ms

        # Recurrrence: dp[l+1][g] = min_g' (dp[l][g] + rtt(g,g') + latency(g'))
        for l in range(1, num_layers):
            curr: Dict[int, float] = {i: float("inf") for i in layer_hosts[l]}
            prev_back: Dict[int, Optional[int]] = {i: None for i in layer_hosts[l]}
            for i in layer_hosts[l]:
                node_i = nodes[i]
                best_cost = float("inf")
                best_j: Optional[int] = None
                for j, prev_cost in dp[l - 1].items():
                    if prev_cost == float("inf"):
                        continue
                    node_j = nodes[j]
                    trans = 0.0 if i == j else node_j.get_rtt_to(node_i)
                    total = prev_cost + trans + node_i.layer_latency_ms
                    if total < best_cost:
                        best_cost = total
                        best_j = j
                curr[i] = best_cost
                prev_back[i] = best_j
            dp.append(curr)
            back.append(prev_back)

        # Backtrack optimal node index per layer
        last = dp[-1]
        end_i = min(last, key=lambda k: last[k])
        path_idx: List[int] = [end_i]
        for l in range(num_layers - 1, 0, -1):
            prev_i = back[l][path_idx[-1]]
            if prev_i is None:
                break
            path_idx.append(prev_i)
        path_idx.reverse()

        # Identify turning points: tail truncations when switching away
        turning: List[Tuple[str, int, str]] = []
        for l in range(1, len(path_idx)):
            prev_i = path_idx[l - 1]
            cur_i = path_idx[l]
            if prev_i == cur_i:
                continue
            prev_node = nodes[prev_i]
            if prev_node.hosts_layer(l):
                turning.append((nodes[prev_i].node_id, l, "tail"))
        # Identify front truncations: for each node on the path, if the first
        # layer used is greater than its hosted start, we can drop the prefix
        # [start, first_used_layer)
        first_used: Dict[int, int] = {}
        for l, idx in enumerate(path_idx):
            if idx not in first_used:
                first_used[idx] = l
        for idx, l0 in first_used.items():
            n = nodes[idx]
            if n.start_layer is None:
                continue
            if l0 > n.start_layer:
                turning.append((n.node_id, l0, "head"))
        return turning

    def find_optimal_path(self, nodes: List[Node], num_layers: int) -> Tuple[List[str], float]:
        """Shard-level DP path across node ranges using `Node` APIs."""
        if num_layers <= 0 or not nodes:
            return [], 0.0

        # Collect vertices from nodes with valid layer ranges
        starts: Dict[int, List[int]] = {}
        ends: Dict[int, List[int]] = {}
        for idx, n in enumerate(nodes):
            if n.start_layer is None or n.end_layer is None or n.is_active is False:
                continue
            starts.setdefault(n.start_layer, []).append(idx)
            ends.setdefault(n.end_layer, []).append(idx)

        # DP over vertices sorted by (start, end)
        order = [
            i
            for i, n in sorted(
                [
                    (i, n)
                    for i, n in enumerate(nodes)
                    if n.start_layer is not None and n.end_layer is not None
                ],
                key=lambda p: (p[1].start_layer, p[1].end_layer),
            )
        ]

        dp: Dict[int, float] = {i: float("inf") for i in order}
        parent: Dict[int, Optional[int]] = {i: None for i in order}

        # Initialize with nodes starting at layer 0
        for i in starts.get(0, []):
            dp[i] = float(nodes[i].layer_latency_ms)
            parent[i] = None

        # Transitions: j -> i if end(j) == start(i)
        for i in order:
            if dp[i] == float("inf"):
                # Not reachable yet; still try to relax successors using INF + ... won't help
                pass
            n_i = nodes[i]
            if n_i.start_layer is None:
                continue
            for j in ends.get(n_i.start_layer, []):
                if dp[j] == float("inf"):
                    continue
                n_j = nodes[j]
                trans = 0.0 if n_j.node_id == n_i.node_id else float(n_j.get_rtt_to(n_i))
                cand = dp[j] + trans + float(n_i.layer_latency_ms)
                if cand < dp[i]:
                    dp[i] = cand
                    parent[i] = j

        # Pick best terminal node that ends at num_layers
        terminals = ends.get(num_layers, [])
        if not terminals:
            return [], float("inf")
        end_idx = min(terminals, key=lambda k: dp.get(k, float("inf")))
        if dp.get(end_idx, float("inf")) == float("inf"):
            return [], float("inf")

        # Reconstruct path
        path_indices: List[int] = []
        cur: Optional[int] = end_idx
        while cur is not None:
            path_indices.append(cur)
            cur = parent[cur]
        path_indices.reverse()
        return [nodes[i].node_id for i in path_indices], dp[end_idx]


class RoundRobinPipelineRouting(RequestRoutingStrategy):
    """
    Baseline routing strategy using round-robin over complete pipelines.

    A complete pipeline is a sequence of nodes with contiguous layer ranges that
    exactly covers [0, num_layers). We enumerate all such pipelines from current
    node allocations, skip any pipeline that contains an overloaded node, and
    dispatch requests by rotating among the remaining pipelines.

    This implementation discovers all complete pipelines once, caches them as
    node-id sequences sorted by their estimated end-to-end latency (including
    RTT), and then round-robins over that cached list. Use `reset_pipelines()`
    to force rediscovery if allocations change.
    """

    def __init__(self) -> None:
        self._rr_cursor: int = 0
        self._pipelines: Optional[List[List[str]]] = None

    def pipeline_discovery(self, nodes: List[Node], num_layers: int) -> List[List[str]]:
        """Discover and return all complete pipelines via DFS backtracking.

        Robust enumeration procedure:
        - Build a mapping from start_layer to nodes starting there.
        - For each head node with start_layer == 0, perform a depth-first search
          from its end_layer, trying all candidate next nodes whose start equals
          the current end and whose end strictly increases.
        - Record any chain that reaches exactly `num_layers` as a complete pipeline.

        This approach backtracks when a candidate cannot lead to completion,
        avoiding the brittleness of a single greedy choice and ensuring that
        overlapping heads/tails yield all valid pipelines.

        Returns:
            A list of pipelines as node-id sequences. Does not cache.
        """
        if not nodes or num_layers <= 0:
            return []

        # Index nodes by start layer
        start_to_nodes: Dict[int, List[Node]] = {}
        for n in nodes:
            if n.start_layer is None or n.end_layer is None:
                continue
            start_to_nodes.setdefault(n.start_layer, []).append(n)

        heads = start_to_nodes.get(0, [])
        pipelines: List[List[str]] = []

        def dfs(current_end: Optional[int], path_ids: List[str]) -> None:
            if current_end is None:
                return
            if current_end == num_layers:
                pipelines.append(list(path_ids))
                return
            candidates = [
                n
                for n in start_to_nodes.get(int(current_end), [])
                if n.end_layer is not None and n.end_layer > current_end
            ]
            # Deterministic order: try shorter segments first
            candidates.sort(key=lambda n: n.end_layer)  # type: ignore[arg-type]
            for nxt in candidates:
                path_ids.append(nxt.node_id)
                dfs(int(nxt.end_layer), path_ids)  # type: ignore[arg-type]
                path_ids.pop()

        for head in heads:
            if head.end_layer is None:
                continue
            path_ids: List[str] = [head.node_id]
            dfs(int(head.end_layer), path_ids)  # type: ignore[arg-type]

        logger.debug(f"Discovered {len(pipelines)} pipelines")
        logger.debug(f"Pipelines: {pipelines}")
        return pipelines

    def find_turning_points(self, nodes: List[Node], num_layers: int) -> List[Tuple[str, int, str]]:
        """No warm-up/truncation in the baseline; return no turning points."""
        return []

    def _ensure_pipelines(self, nodes: List[Node], num_layers: int) -> None:
        """Ensure cached pipelines exist; discover and cache if missing."""
        if self._pipelines is None:
            self._pipelines = self.pipeline_discovery(nodes, num_layers)

    def _build_start_index(self, nodes: List[Node]) -> Dict[int, List[Node]]:
        """Build an index of nodes by their `start_layer` for fast lookups.

        Only nodes with both `start_layer` and `end_layer` set are included.
        """
        index: Dict[int, List[Node]] = {}
        for n in nodes:
            if n.start_layer is None or n.end_layer is None:
                continue
            index.setdefault(n.start_layer, []).append(n)
        return index

    def _attempt_repair_pipeline(
        self, candidate_ids: List[str], nodes: List[Node], num_layers: int
    ) -> Optional[List[str]]:
        """Best-effort repair of an overloaded pipeline by backtracking from the tail.

        Starting from the end of the proposed pipeline, keep the longest viable
        prefix (no missing/overloaded nodes) and search for an alternative suffix
        that completes coverage to `num_layers`. The search explores all nodes that
        start at the split layer and are not overloaded, performing DFS until a
        complete chain is found or possibilities are exhausted.

        Returns:
            A repaired pipeline (list of node_ids) or None if not found.
        """
        id_to_node: Dict[str, Node] = {n.node_id: n for n in nodes}
        start_to_nodes = self._build_start_index(nodes)

        # Identify which positions in the original pipeline are viable
        def is_viable_node_id(nid: str) -> bool:
            node = id_to_node.get(nid)
            return node is not None and not node.is_overloaded

        # Try backtracking from the tail to earlier split points
        for split_idx in range(len(candidate_ids) - 1, -1, -1):
            # Check that prefix [0, split_idx) remains viable
            prefix_ok = True
            for i in range(split_idx):
                if not is_viable_node_id(candidate_ids[i]):
                    prefix_ok = False
                    break
            if not prefix_ok:
                continue

            # Determine split layer where we start reconstructing the suffix
            if split_idx == 0:
                split_layer = 0
            else:
                prev_node = id_to_node.get(candidate_ids[split_idx - 1])
                if prev_node is None or prev_node.end_layer is None:
                    continue
                split_layer = int(prev_node.end_layer)

            # Depth-first search to build a non-overloaded suffix covering [split_layer, L)
            repaired_suffix: Optional[List[str]] = None

            def dfs(layer: int, acc: List[str]) -> bool:
                nonlocal repaired_suffix
                if layer == num_layers:
                    repaired_suffix = list(acc)
                    return True
                candidates = [
                    n
                    for n in start_to_nodes.get(layer, [])
                    if n.end_layer is not None and n.end_layer > layer and not n.is_overloaded
                ]
                # Prefer shorter segments first for responsiveness
                candidates.sort(key=lambda n: n.end_layer)  # type: ignore[arg-type]
                for nxt in candidates:
                    acc.append(nxt.node_id)
                    if dfs(int(nxt.end_layer), acc):  # type: ignore[arg-type]
                        return True
                    acc.pop()
                return False

            if dfs(split_layer, []):
                new_pipeline = candidate_ids[:split_idx] + (repaired_suffix or [])
                # Sanity check: ensure coverage starts from 0 and ends at L
                # (prefix guarantees contiguous coverage up to split_layer)
                return new_pipeline if new_pipeline else None

        return None

    def find_optimal_path(self, nodes: List[Node], num_layers: int) -> Tuple[List[str], float]:
        """Round-robin among cached pipelines, skipping overloaded ones.

        Selection procedure:
        - On first use, greedily discover and cache all full pipelines.
        - Pick the pipeline at `rr_cursor % len(pipelines)`.
        - If any node in that pipeline is overloaded or missing, advance cursor
          and try the next one, up to the number of pipelines. Additionally, if
          the selected pipeline contains overloaded nodes, attempt a best-effort
          repair by backtracking from the tail to find an alternative suffix that
          completes coverage without overloaded nodes.
        - Return the first viable pipeline and its latency estimate using
          current per-node stats and RTTs. If none are viable, return empty.
        """
        if not nodes or num_layers <= 0:
            return [], float("inf")

        self._ensure_pipelines(nodes, num_layers)
        if not self._pipelines:
            return [], float("inf")

        id_to_node: Dict[str, Node] = {n.node_id: n for n in nodes}

        attempts = 0
        total_pipelines = len(self._pipelines)
        self._rr_cursor %= total_pipelines
        while attempts < total_pipelines:
            idx = self._rr_cursor % total_pipelines
            candidate_ids = self._pipelines[idx]
            # Check overloaded / presence
            viable = True
            prev: Optional[Node] = None
            total_latency = 0.0
            for nid in candidate_ids:
                node = id_to_node.get(nid)
                if node is None or node.is_overloaded:
                    viable = False
                    break
                total_latency += float(node.layer_latency_ms)
                if prev is not None:
                    total_latency += (
                        0.0 if prev.node_id == node.node_id else float(prev.get_rtt_to(node))
                    )
                prev = node
            self._rr_cursor += 1
            attempts += 1
            if viable and total_latency != float("inf"):
                return candidate_ids, total_latency
            # Attempt a one-shot repair if the selected pipeline is not viable
            repaired = self._attempt_repair_pipeline(candidate_ids, nodes, num_layers)
            if repaired:
                # Compute latency for the repaired path
                total_latency = 0.0
                prev = None
                for nid in repaired:
                    node = id_to_node.get(nid)
                    # If any node is missing/overloaded, skip this repair
                    if node is None or node.is_overloaded:
                        total_latency = float("inf")
                        break
                    total_latency += float(node.layer_latency_ms)
                    if prev is not None:
                        total_latency += (
                            0.0 if prev.node_id == node.node_id else float(prev.get_rtt_to(node))
                        )
                    prev = node
                if total_latency != float("inf"):
                    return repaired, total_latency

        return [], float("inf")


================================================================================
File: src/scheduling/scheduler.py
Size: 28.25 kB
================================================================================

"""
Scheduler for Layer Allocation and Request Routing.
"""

from __future__ import annotations

import queue
import threading
import time
from collections import deque
from typing import Deque, Dict, List, Literal, Optional, Tuple

from parallax_utils.logging_config import get_logger
from scheduling.layer_allocation import (
    DynamicProgrammingLayerAllocator,
    GreedyLayerAllocator,
)
from scheduling.model_info import ModelInfo
from scheduling.node import Node, RequestSignal
from scheduling.request_routing import (
    DynamicProgrammingRouting,
    RoundRobinPipelineRouting,
)

logger = get_logger(__name__)


class Scheduler:
    """Coordinates allocation, node materialization, and request routing."""

    def __init__(
        self,
        model_info: ModelInfo,
        nodes: List[Node],
        min_nodes_bootstrapping: int = 1,
        strategy: Literal["greedy", "dp"] = "dp",
        routing_strategy: Literal["rr", "dp"] = "rr",
        *,
        request_arrival_horizon_sec: float = 600.0,
        rebalance_threshold: float = float("inf"),
        water_filling_max_iterations: int = 40,
        request_warm_up_for_reshard: int = 0,
        heartbeat_timeout: float = 60.0,
    ) -> None:
        """Initialize the scheduler.

        Args:
            model_info: Model architecture information used by allocators and routers.
            nodes: Initial list of candidate nodes.
            min_nodes_bootstrapping: Minimum nodes required to attempt initial allocation.
            strategy: Layer allocation strategy ("dp" or "greedy").
            routing_strategy: Request routing strategy ("dp" for dynamic programming, or
                "greedy" for round-robin over complete pipelines skipping overloaded ones).
            request_arrival_horizon_sec: Sliding window horizon for arrival-rate tracking.
            rebalance_threshold: Threshold for triggering rebalancing in allocation.
            water_filling_max_iterations: Max iterations for water-filling allocation.
            request_warm_up_for_reshard: Number of warm-up requests to detect truncation.
            heartbeat_timeout: Time in seconds to consider node heartbeat stale.
        """
        self.model_info = model_info
        self.num_layers = model_info.num_layers

        allocator_class = (
            GreedyLayerAllocator if strategy == "greedy" else DynamicProgrammingLayerAllocator
        )
        self.layer_allocator = allocator_class(
            model_info,
            nodes,
            rebalance_threshold=rebalance_threshold,
            water_filling_max_iterations=water_filling_max_iterations,
        )
        # Ensure Scheduler and allocator share the same node list to avoid divergence.
        self.nodes = self.layer_allocator.nodes
        self.node_id_to_node: Dict[str, Node] = self.layer_allocator.node_id_to_node
        self.min_nodes_bootstrapping = min_nodes_bootstrapping

        self.request_router = (
            DynamicProgrammingRouting() if routing_strategy == "dp" else RoundRobinPipelineRouting()
        )
        self.request_warm_up_for_reshard = request_warm_up_for_reshard

        self._request_queue: "queue.Queue[RequestSignal]" = queue.Queue()
        self.request_arrival_horizon_sec = request_arrival_horizon_sec
        self.heartbeat_timeout = heartbeat_timeout
        self._arrival_ts: Deque[float] = deque()

        # Event queues for main loop orchestration (thread-safe)
        self._pending_joins: "queue.Queue[Node]" = queue.Queue()
        self._pending_leaves: "queue.Queue[str]" = queue.Queue()
        self._pending_node_updates: "queue.Queue[Tuple[str, Optional[int], Optional[float], Optional[Dict[str, float]], Optional[bool]]]" = (queue.Queue())

        # Concurrency controls
        self._stop_event: threading.Event = threading.Event()
        self._wake_event: threading.Event = threading.Event()
        self._node_count_cv: threading.Condition = threading.Condition()
        self._event_thread: Optional[threading.Thread] = None
        self._dispatch_thread: Optional[threading.Thread] = None
        self._alloc_log_thread: Optional[threading.Thread] = None
        # Thread-safe bootstrap state
        self._bootstrapped: bool = False
        self._bootstrapped_event: threading.Event = threading.Event()
        logger.debug(
            f"Scheduler initialized, min_nodes_bootstrapping {self.min_nodes_bootstrapping}, "
            f"strategy {strategy}, rebalance threshold {rebalance_threshold}"
        )
        self._node_assigned_request_count: Dict[str, int] = {}

        # Eager bootstrap for initial allocation if enough nodes are present
        try:
            if len(self.nodes) >= self.min_nodes_bootstrapping:
                logger.debug(
                    f"Eager allocation attempt with {len(self.nodes)} nodes (min required: {self.min_nodes_bootstrapping})"
                )
                self.layer_allocator.global_allocation()
        except Exception:  # best-effort eager allocation
            pass

    # Orchestration helpers
    def bootstrap(self, *, clear_existing: bool = False, skip_warmup: bool = False) -> bool:
        """Bootstrapping:
        This method can be used for both initial bootstrapping and global rebalancing.
        When clear_existing=True, it first deallocates all existing allocations before
        performing global allocation (rebalancing behavior). When clear_existing=False,
        it performs allocation on top of existing state (initial bootstrapping behavior).

        Args:
            clear_existing: If True, deallocate all existing allocations before reallocating.
                This is used for global rebalancing. Default is False.
            skip_warmup: If True, skip the warm-up and truncate step. Default is False.

        Returns:
            True if a full pipeline was established; False otherwise.
        """
        # Check node count only for initial bootstrapping (not rebalancing)
        if not clear_existing and len(self.nodes) < self.min_nodes_bootstrapping:
            logger.debug(
                f"Bootstrapping deferred: have {len(self.nodes)} nodes; need >= {self.min_nodes_bootstrapping}"
            )
            return False

        # Clear existing allocations if this is a rebalance
        if clear_existing:
            logger.debug("Performing global rebalance (clearing existing allocations)")
            self._bootstrapped = False
            self._bootstrapped_event.clear()
            for n in self.nodes:
                if n.start_layer is not None and n.end_layer is not None:
                    self.layer_allocator.deallocate(n)
        else:
            logger.debug("Bootstrapping layer allocator")

        # Perform global allocation
        success = self.layer_allocator.global_allocation()
        if not success:
            logger.warning("Global allocation failed to produce a full pipeline")
            return False

        assignments = self.list_node_allocations()
        logger.debug(f"Layer allocator assignments: {assignments}")

        # Optional warm-up to find turning points and truncate node ranges
        # Skip warmup for rebalancing scenarios (can be overridden with skip_warmup=False)
        if not skip_warmup and self.request_warm_up_for_reshard > 0:
            self._run_warmup_and_truncate()
            assignments = self.list_node_allocations()
            logger.debug(f"Layer allocator assignments after turn-point warm-up: {assignments}")

        if not self.layer_allocator.has_full_pipeline():
            logger.warning("Bootstrapping failed to produce a full pipeline")
            return False

        self._bootstrapped = True
        self._bootstrapped_event.set()
        action = "rebalance" if clear_existing else "bootstrapping"
        logger.debug(f"{action.capitalize()} completed successfully; full pipeline established")
        return True

    def list_node_allocations(self) -> List[Tuple[str, int, int]]:
        """List the allocations of all nodes."""
        return self.layer_allocator.list_node_allocations()

    # Warm-up and re-shard
    def _run_warmup_and_truncate(self, override_warmup_count: int = 0) -> None:
        """Run a brief warm-up to detect truncation points and shrink shards.

        Uses layer-level DP turning points (node_id, layer_idx, kind):
        - kind == "tail": drop [layer_idx, end) on that node
        - kind == "head": drop [start, layer_idx) on that node

        Note: Always uses DynamicProgrammingRouting for finding turning points,
        regardless of the current request_router type, since turning points
        detection requires layer-level DP analysis.

        Args:
            override_warmup_count: If > 0, use this value instead of request_warm_up_for_reshard.
                Default is 0, which means use request_warm_up_for_reshard.
        """
        nodes_list = list(self.nodes)
        if not nodes_list:
            return
        num_layers = self.model_info.num_layers

        # The number of warm-up requests can be used to repeat detection, but a
        # single pass is sufficient with our DP model; we repeat to smooth noise.
        warmup_count = (
            override_warmup_count if override_warmup_count > 0 else self.request_warm_up_for_reshard
        )

        agg_turns: Dict[Tuple[str, int, str], int] = {}
        for _ in range(warmup_count):
            turns = DynamicProgrammingRouting.find_turning_points(nodes_list, num_layers)
            for t in turns:
                agg_turns[t] = agg_turns.get(t, 0) + 1

        # Apply truncation for consistently observed turning points
        # Note: Must use layer_allocator.allocate/deallocate to properly update
        # internal state (node_allocation dict and layer_to_load)
        for node_id, layer_idx, kind in agg_turns:
            node = next((n for n in self.nodes if n.node_id == node_id), None)
            if node is None or node.start_layer is None or node.end_layer is None:
                continue
            start, end = node.start_layer, node.end_layer
            if kind == "tail":
                if layer_idx < end:
                    self.layer_allocator.reallocate(node, start, layer_idx)
            elif kind == "head":
                if layer_idx > start:
                    self.layer_allocator.reallocate(node, layer_idx, end)

    def update_node_info(
        self,
        node: Node,
        *,
        current_requests: Optional[int] = None,
        layer_latency_ms: Optional[float] = None,
        new_rtt_to_nodes: Optional[Dict[str, float]] = None,
        is_active: Optional[bool] = None,
    ) -> None:
        """Update the info of a node."""
        if current_requests is not None:
            node.current_requests = current_requests
        if layer_latency_ms is not None:
            node.set_layer_latency_ms(layer_latency_ms)
        if new_rtt_to_nodes is not None:
            node.rtt_to_nodes = new_rtt_to_nodes
        if is_active is not None:
            node.is_active = is_active
        node.last_heartbeat = time.time()
        # logger.debug(
        #     "Node updated: %s (requests=%s, latency_ms=%s, rtt_updates=%s)",
        #     node.node_id,
        #     current_requests if current_requests is not None else node.current_requests,
        #     layer_latency_ms if layer_latency_ms is not None else node.avg_layer_latency_ms,
        #     0 if new_rtt_to_nodes is None else len(new_rtt_to_nodes),
        # )

    # Async-style event enqueuers for main loop
    def enqueue_join(self, node: Node) -> None:
        """Enqueue a join event."""
        logger.debug(f"Enqueueing join event for node {node.node_id}")
        self._pending_joins.put(node)
        self._wake_event.set()

    def enqueue_leave(self, node_id: str) -> None:
        """Enqueue a leave event."""
        self._pending_leaves.put(node_id)
        self._wake_event.set()

    def enqueue_node_update(
        self,
        node_id: str,
        *,
        current_requests: Optional[int] = None,
        layer_latency_ms: Optional[float] = None,
        new_rtt_to_nodes: Optional[Dict[str, float]] = None,
        is_active: Optional[bool] = None,
    ) -> None:
        """Enqueue a node update event."""
        self._pending_node_updates.put(
            (node_id, current_requests, layer_latency_ms, new_rtt_to_nodes, is_active)
        )
        self._wake_event.set()

    def checking_node_heartbeat(self) -> None:
        """Check the heartbeat of all nodes."""
        for node in self.nodes:
            if not node.is_active:
                continue
            if time.time() - node.last_heartbeat > self.heartbeat_timeout:
                logger.debug(f"Node {node.node_id} heartbeat timeout")
                self.leave(node.node_id)

    # Dynamic node management
    def join(self, node: Node, bootstrap: bool = False) -> None:
        """Add a node to allocation and refresh plan and materialized nodes."""
        logger.debug(
            "Joining node %s (kv_ratio=%.2f, param_ratio=%.2f, manual_assignment=%s)",
            node.node_id,
            node.kvcache_mem_ratio,
            node.param_mem_ratio,
            node.manual_layer_assignment,
        )
        self.layer_allocator.declare(node)

        # Manual layer assignment bypasses bootstrap waiting
        if node.manual_layer_assignment:
            # Manual layer assignment: use the layers specified by the node
            if node.start_layer is None or node.end_layer is None:
                raise ValueError(
                    f"Node {node.node_id} has manual_layer_assignment=True "
                    f"but start_layer ({node.start_layer}) or end_layer ({node.end_layer}) is None"
                )
            logger.info(
                f"Manual layer assignment for node {node.node_id}: "
                f"layers [{node.start_layer}, {node.end_layer})"
            )
            # Directly allocate the specified layers without automatic assignment
            self.layer_allocator.allocate(node, node.start_layer, node.end_layer)

            # Check if manual allocations now cover the full pipeline
            if self.layer_allocator.has_full_pipeline():
                if not self._bootstrapped:
                    logger.info(
                        "Manual layer assignments have established a full pipeline; "
                        "marking scheduler as bootstrapped"
                    )
                    self._bootstrapped = True
                    self._bootstrapped_event.set()
        elif not bootstrap:
            # Automatic layer assignment (only after bootstrap)
            self.layer_allocator.join(node)
        # If bootstrap=True and not manual, node is only declared (allocation deferred to bootstrap())

        # Notify waiters that node count changed
        with self._node_count_cv:
            self._node_count_cv.notify_all()

    def leave(self, node_id: str) -> None:
        """Remove a node from allocation and refresh plan and materialized nodes."""
        if node_id not in self.layer_allocator.node_id_to_node:
            raise ValueError(f"Node {node_id} not found in nodes")
        node = self.node_id_to_node[node_id]
        logger.debug(
            "Leaving node %s (start=%s, end=%s)", node_id, node.start_layer, node.end_layer
        )
        self.layer_allocator.leave(node_id)
        if self.layer_allocator.should_global_rebalance():
            logger.debug("Global rebalance triggered due to node leave")

            # Count manual vs automatic nodes
            manual_count = sum(1 for n in self.nodes if n.manual_layer_assignment)
            total_count = len(self.nodes)
            logger.debug(
                f"Node count: {manual_count} manual, {total_count - manual_count} automatic"
            )
            if manual_count == total_count:
                logger.debug("All nodes are manual assignment, skipping global rebalance")
            elif manual_count > 0:
                logger.error(
                    f"Mixed assignment detected ({manual_count} manual, {total_count - manual_count} automatic); skipping rebalance"
                )
            else:
                # All nodes are automatic, try adjustment first, then rebalance if needed
                if not self.layer_allocator.has_full_pipeline():
                    logger.debug(
                        "No full pipeline after node leave, attempting warmup and truncate"
                    )
                    self._run_warmup_and_truncate(override_warmup_count=1)
                    if not self.layer_allocator.has_full_pipeline():
                        self.bootstrap(clear_existing=True, skip_warmup=True)
                    else:
                        logger.debug(
                            "Pipeline recovered through warmup and truncate, skipping global rebalance"
                        )
                else:
                    self.bootstrap(clear_existing=True, skip_warmup=True)

        with self._node_count_cv:
            self._node_count_cv.notify_all()

    def receive_request(self, request: RequestSignal) -> None:
        """Add a request to the wait pool."""
        self._request_queue.put(request)
        self._wake_event.set()
        now = time.time()
        self._arrival_ts.append(now)
        logger.debug(
            "Received request %s (queue_size=%d)", request.request_id, self._request_queue.qsize()
        )
        # Trim old timestamps to keep arrival-rate window bounded
        horizon = self.request_arrival_horizon_sec
        while self._arrival_ts and now - self._arrival_ts[0] > horizon:
            self._arrival_ts.popleft()

    def dispatch_next_request(self) -> Optional[Tuple[str, List[str], float]]:
        """Route the next request in the wait pool; returns (request_id, path, latency)."""
        try:
            req = self._request_queue.get_nowait()
        except queue.Empty:
            req = None
        if req is None:
            return None
        path, latency = self.request_router.find_optimal_path(self.nodes, self.num_layers)
        req.routing_table = path
        # Update simple load counters
        for node_id in path:
            n = self.node_id_to_node[node_id]
            if n is not None:
                self._node_assigned_request_count[node_id] = (
                    self._node_assigned_request_count.get(node_id, 0) + 1
                )
                n.add_request()
        logger.debug(
            "Dispatched request %s via path %s (est_lat=%.2fms)", req.request_id, path, latency
        )
        return req.request_id, path, latency

    def run(self, *, poll_interval: float = 0.05, allocation_log_interval: float = 5.0) -> None:
        """Run the scheduler concurrently until `stop()` is called.

        Starts background threads for event processing (joins/leaves/updates/heartbeats)
        and request dispatching. At startup, waits until at least
        `min_nodes_bootstrapping` nodes are present, then runs `bootstrap()`.
        """
        logger.debug("Running scheduler")
        self._stop_event.clear()

        # Start event thread first so joins can be processed while we wait to bootstrap
        self._event_thread = threading.Thread(
            target=self._event_loop, args=(poll_interval,), name="SchedulerEventLoop", daemon=True
        )
        self._event_thread.start()

        # Bootstrap gating
        if not self._wait_for_bootstrap(poll_interval):
            return

        # Start dispatcher only after successful bootstrap
        self._dispatch_thread = threading.Thread(
            target=self._dispatch_loop,
            args=(poll_interval,),
            name="SchedulerDispatcher",
            daemon=True,
        )
        self._dispatch_thread.start()

        # Start periodic allocation logger thread
        def _alloc_log_loop() -> None:
            """Periodically log current layer allocations."""
            while not self._stop_event.is_set():
                try:
                    assignments = self.list_node_allocations()
                    header = f"Current allocations ({len(assignments)} nodes)"
                    sep = "-" * len(header)
                    logger.debug("%s\n%s", header, sep)
                    for node_id, start_layer, end_layer in assignments:
                        node = self.node_id_to_node[node_id]
                        # Snapshot values to avoid recomputing/logging side-effects twice
                        capacity = node.max_requests
                        current = node.current_requests
                        latency = node.layer_latency_ms
                        latency_str = "inf" if latency == float("inf") else f"{latency:.2f}"
                        n_hosted_requests = 0
                        if node_id in self._node_assigned_request_count:
                            n_hosted_requests = self._node_assigned_request_count[node_id]
                        logger.debug(
                            "  %-16s layers [%3d, %3d) | load %3d/%-3d | latency %7s ms | assigned request count %3d",
                            node_id,
                            start_layer,
                            end_layer,
                            current,
                            capacity,
                            latency_str,
                            n_hosted_requests,
                        )
                except Exception as exc:
                    logger.warning(f"Allocation logger error: {exc}")
                time.sleep(max(1.0, allocation_log_interval))

        self._alloc_log_thread = threading.Thread(
            target=_alloc_log_loop, name="SchedulerAllocLogger", daemon=True
        )
        self._alloc_log_thread.start()

        # Block until stop is requested
        try:
            while not self._stop_event.is_set():
                time.sleep(max(0.5, poll_interval))
        finally:
            if self._event_thread is not None:
                self._event_thread.join(timeout=2.0)
            if self._dispatch_thread is not None:
                self._dispatch_thread.join(timeout=2.0)
            if self._alloc_log_thread is not None:
                self._alloc_log_thread.join(timeout=2.0)

    # === Modularized worker loops ===
    def _event_loop(self, poll_interval: float) -> None:
        """Process joins/leaves/updates and perform heartbeat checks."""
        last_hb_check = 0.0
        while not self._stop_event.is_set():
            self._process_node_updates()
            self._process_joins()
            self._process_leaves()
            now = time.time()
            if now - last_hb_check >= max(0.5, poll_interval):
                self.checking_node_heartbeat()
                last_hb_check = now
            self._wake_event.wait(timeout=poll_interval)
            self._wake_event.clear()

    def _dispatch_loop(self, poll_interval: float) -> None:
        """Continuously dispatch incoming requests while running."""
        while not self._stop_event.is_set():
            try:
                req = self._request_queue.get(timeout=poll_interval)
                if req is None:
                    continue
                path, path_rtt = self.request_router.find_optimal_path(self.nodes, self.num_layers)
                logger.debug(f"Path RTT: {path_rtt}")
                req.routing_table = path
                for node_id in path:
                    n = self.node_id_to_node[node_id]
                    if n is not None:
                        self._node_assigned_request_count[node_id] = (
                            self._node_assigned_request_count.get(node_id, 0) + 1
                        )
                        n.add_request()
                logger.debug(
                    "Dispatched request %s via path %s", getattr(req, "request_id", "?"), path
                )
            except queue.Empty:
                continue

    def _wait_for_bootstrap(self, poll_interval: float) -> bool:
        """Wait until enough nodes then run bootstrap. Returns False if stopped."""
        logger.debug("Waiting for bootstrap")
        while not self._stop_event.is_set() and not self._bootstrapped_event.is_set():
            with self._node_count_cv:
                if len(self.nodes) < self.min_nodes_bootstrapping:
                    self._node_count_cv.wait(timeout=max(0.5, poll_interval))
                    continue
            boot_ok = self.bootstrap()
            if boot_ok:
                break
            with self._node_count_cv:
                self._node_count_cv.wait(timeout=max(1.0, poll_interval))
        return not self._stop_event.is_set()

    def _process_node_updates(self) -> None:
        """Apply pending node stats updates from the queue."""
        while True:
            try:
                node_id, cur, lat, rtts, is_active = self._pending_node_updates.get_nowait()
            except queue.Empty:
                break
            if node_id not in self.node_id_to_node:
                logger.warning(f"Node {node_id} not found in node list, ignore the update")
                continue
            self.update_node_info(
                self.node_id_to_node[node_id],
                current_requests=cur,
                layer_latency_ms=lat,
                new_rtt_to_nodes=rtts,
                is_active=is_active,
            )

    def _process_joins(self) -> None:
        """Handle pending join events, honoring bootstrap state for assignment."""
        joined_any = False
        had_manual_assignment = False
        while True:
            try:
                node = self._pending_joins.get_nowait()
            except queue.Empty:
                break
            # During bootstrap (no full pipeline yet), only declare nodes; no dynamic assignment.
            # After bootstrap, allow dynamic light-weight joins.
            # Exception: manual layer assignments are processed immediately regardless of bootstrap state.
            self.join(node, bootstrap=not self._bootstrapped_event.is_set())
            joined_any = True
            if node.manual_layer_assignment:
                had_manual_assignment = True

        # If we are not bootstrapped (e.g., after a leave-triggered rebalance) and
        # new nodes just joined, attempt a greedy bootstrap immediately when we have
        # enough nodes. If it doesn't produce a full pipeline, we'll try again on
        # subsequent joins.
        # Skip bootstrap if manual assignments were used (they handle bootstrapping internally).
        if joined_any and not self._bootstrapped_event.is_set() and not had_manual_assignment:
            if len(self.nodes) >= self.min_nodes_bootstrapping:
                try:
                    ok = self.bootstrap()
                    if not ok:
                        logger.debug(
                            "Bootstrap attempt after join did not produce a full pipeline; will retry on future joins"
                        )
                except Exception as exc:
                    logger.debug(
                        f"Bootstrap attempt after join failed: {exc}; will retry on future joins"
                    )
            else:
                logger.debug(
                    "Deferring bootstrap: have %d nodes; need >= %d",
                    len(self.nodes),
                    self.min_nodes_bootstrapping,
                )

    def _process_leaves(self) -> None:
        """Handle pending leave events safely."""
        while True:
            try:
                node_id = self._pending_leaves.get_nowait()
            except queue.Empty:
                break
            try:
                self.leave(node_id)
            except Exception as exc:
                logger.warning(f"Leave failed for {node_id}: {exc}")

    def stop(self) -> None:
        """Signal background threads to stop and wake any waiters."""
        self._stop_event.set()
        self._wake_event.set()
        with self._node_count_cv:
            self._node_count_cv.notify_all()

    def need_more_nodes(self):
        return not self._bootstrapped and len(self.nodes) >= self.min_nodes_bootstrapping


================================================================================
File: tests/__init__.py
Size: 0 B
================================================================================



================================================================================
File: tests/scheduler_tests/__init__.py
Size: 0 B
================================================================================



================================================================================
File: tests/scheduler_tests/test_layer_allocation.py
Size: 12.4 kB
================================================================================

"""
Tests for Phase 1: layer allocation and rebalancing.

Covers:
- Capacity sanity for `Node`
- Water-filling rebalancing within a pipeline (stage splits across heterogeneous nodes)
- Gap-patch dynamic rebalancing (join/leave behavior)
- Greedy and DP allocators producing contiguous [start, end) layer ranges
"""

from collections import Counter
from typing import Literal

import pytest

from scheduling.layer_allocation import (
    BaseLayerAllocator,
    DynamicProgrammingLayerAllocator,
    GreedyLayerAllocator,
)
from scheduling.model_info import ModelInfo
from scheduling.node import Node, NodeHardwareInfo

from .test_utils import build_model_info


def _build_node(gpu_type: str, model: ModelInfo, id_suffix: str = "") -> Node:
    hw_map = {
        "a100-80g": NodeHardwareInfo("a100-80g" + id_suffix, 1, 312.0, "", 80.0, 2039.0, "cuda"),
        "a100-40g": NodeHardwareInfo("a100-40g" + id_suffix, 1, 312.0, "", 40.0, 1935.0, "cuda"),
        "rtx5090": NodeHardwareInfo("rtx5090" + id_suffix, 1, 165, "", 32.0, 1792.0, "cuda"),
        "rtx4090": NodeHardwareInfo("rtx4090" + id_suffix, 1, 82.6, "", 24.0, 1008.0, "cuda"),
    }
    hw = hw_map[gpu_type]
    return Node(node_id=hw.node_id, hardware=hw, model_info=model)


def test_capacity_sanity_check():
    """Sanity check for capacity calculations."""
    # 36 layers for actual GPT-OSS 120B
    model = build_model_info(36)
    print(f"decoder layer flops: {model.decoder_layer_flops}")
    print(f"lm head flops: {model.lm_head_flops}")
    print(f"decoder layer io in GB: {model.decoder_layer_io_bytes(roofline=False) / (1024 ** 3)}")
    print(f"embedding table in GB: {model.embedding_io_bytes / (1024 ** 3)}")

    for gpu_type in ["a100-80g", "a100-40g", "rtx5090", "rtx4090"]:
        # (capacity, with embed) -> (13, 13), (6, 6), (5, 5), (4, 3)
        node = _build_node(gpu_type, model)
        capacity = node.get_decoder_layer_capacity()
        capacity_with_embed = node.get_decoder_layer_capacity(include_input_embed=True)
        assert capacity_with_embed <= capacity


@pytest.mark.parametrize(
    "num_layers,gpu_types,expected_layers",
    [
        (21, ["a100-80g", "rtx5090", "rtx4090"], [13, 5, 3]),
        (15, ["a100-80g", "rtx5090"], [11, 4]),
        # (20 * 312 : 20 * 165 : 20 * 82.6) / 559.6 = 11.1 : 5.8 : 2.9 -> 12 : 5 : 3
        (20, ["a100-80g", "rtx5090", "rtx4090"], [12, 5, 3]),
        (25, ["a100-80g", "rtx5090", "rtx4090", "rtx4090"], [13, 5, 4, 3]),
        (29, ["rtx4090", "a100-80g", "rtx5090", "rtx5090", "rtx4090"], [3, 13, 5, 5, 3]),
        (8, ["rtx5090", "rtx5090"], [4, 4]),
        (7, ["a100-40g", "rtx5090"], [5, 2]),
    ],
)
def test_water_filling_rebalance(num_layers: int, gpu_types: list[str], expected_layers: list[int]):
    """Test water-filling rebalancer with various GPU configurations."""
    model = build_model_info(num_layers)

    nodes = [_build_node(g, model, id_suffix=f"-{i}") for i, g in enumerate(gpu_types)]

    allocator = GreedyLayerAllocator(model, nodes)
    allocator.adjust_pipeline_layers(nodes, assume_sorted=False)

    actual_layers = []
    for node in nodes:
        assert node.start_layer is not None and node.end_layer is not None
        actual_layers.append(node.end_layer - node.start_layer)

    assert sum(actual_layers) == num_layers
    # Order-insensitive: only the multiset of stage sizes must match
    assert Counter(actual_layers) == Counter(expected_layers)

    for i, node in enumerate(nodes):
        cap = node.get_decoder_layer_capacity(
            include_input_embed=(i == 0), include_lm_head=(i == len(nodes) - 1)
        )
        assert actual_layers[i] <= cap


def _test_gap_patch_rebalance(allocator: BaseLayerAllocator):
    """Sanity checks for gap-patch dynamic rebalancing using allocator state."""
    assert allocator.layer_loads_heap, "Layer loads heap should not be empty"
    assert allocator.node_id_to_node, "Allocator should have node mappings"
    model_info = allocator.model_info

    # Heap top should include a host with minimal per-layer KV memory
    per_node_mem = {
        nid: (node.per_decoder_layer_kv_cache_memory or 0)
        for nid, node in allocator.node_id_to_node.items()
        if nid in allocator.node_allocation
    }
    min_mem = min(per_node_mem.values()) if per_node_mem else 0

    top_load = allocator.layer_loads_heap[0]
    assert top_load.hosting_nodes
    top_hosts_min = min(per_node_mem.get(h, float("inf")) for h in top_load.hosting_nodes)
    assert top_hosts_min == min_mem

    # Join a new small GPU and verify hosting set updates
    before_layer_id = top_load.layer_id
    before_mem = allocator.layer_to_load[before_layer_id].current_kv_size

    new_node = _build_node("rtx4090", model_info, id_suffix="-gap")
    allocator.join(new_node)

    after_mem = allocator.layer_to_load[before_layer_id].current_kv_size
    assert after_mem >= before_mem
    assert new_node.node_id in allocator.layer_to_load[before_layer_id].hosting_nodes

    allocator.leave(new_node.node_id)
    restored_mem = allocator.layer_to_load[before_layer_id].current_kv_size
    assert new_node.node_id not in allocator.layer_to_load[before_layer_id].hosting_nodes
    assert restored_mem == before_mem


@pytest.mark.parametrize(
    "num_layers,counts,expected_ranges,strategy",
    [
        # Six A100-80g: expect two pipelines, 12 each per stage in creation order
        (36, (6, 0, 0, 0), [(0, 12), (12, 24), (24, 36), (0, 12), (12, 24), (24, 36)], "greedy"),
        (36, (6, 0, 0, 0), [(0, 12), (12, 24), (24, 36), (0, 12), (12, 24), (24, 36)], "dp"),
        # 22 Layers, capacity (13, 13, 6, 6, 3, 3) -> greedy assigns (11, 11)
        (
            22,
            (2, 2, 0, 2),
            [
                (0, 11),
                (11, 22),
            ],
            "greedy",
        ),
        # For DP, we expect two pipelines, 13 each per stage in creation order
        (
            22,
            (2, 2, 0, 2),
            [
                (0, 13),
                (13, 19),
                (19, 22),
                (0, 13),
                (13, 19),
                (19, 22),
            ],
            "dp",
        ),
        # 14 Layers, capacity (13, 5, 5, 3, 3) -> greedy assigns (10, 4)
        (
            14,
            (1, 0, 2, 2),
            [
                (0, 10),
                (10, 14),
            ],
            "greedy",
        ),
        # 7 Layers, capacity (6, 5, 5, 3, 3) -> greedy assigns (5, 2, 4, 3)
        (
            7,
            (0, 1, 2, 2),
            [
                (0, 5),
                (5, 7),
                (0, 4),
                (4, 7),
            ],
            "greedy",
        ),
    ],
)
def test_allocator(
    num_layers: int,
    counts: tuple[int, int, int, int],
    expected_ranges: list[tuple[int, int]],
    strategy: Literal["greedy", "dp"],
):
    """Test allocator with greedy and dp strategies."""
    model = build_model_info(num_layers)

    n_a100_80g, n_a100_40g, n_5090, n_4090 = counts

    nodes: list[Node] = []
    for i in range(n_a100_80g):
        nodes.append(_build_node("a100-80g", model, id_suffix=f"-{i}"))
    for i in range(n_a100_40g):
        nodes.append(_build_node("a100-40g", model, id_suffix=f"-{i}"))
    for i in range(n_5090):
        nodes.append(_build_node("rtx5090", model, id_suffix=f"-{i}"))
    for i in range(n_4090):
        nodes.append(_build_node("rtx4090", model, id_suffix=f"-{i}"))

    allocator = (
        GreedyLayerAllocator(model, nodes, assign_left_over_nodes=False)
        if strategy == "greedy"
        else DynamicProgrammingLayerAllocator(model, nodes, assign_left_over_nodes=False)
    )
    allocator.global_allocation()
    _test_gap_patch_rebalance(allocator)

    # Collect (start,end) per node in creation order
    actual_ranges: list[tuple[int, int]] = []
    for node in nodes:
        if node.start_layer is None or node.end_layer is None:
            actual_ranges.append((0, 0))
        else:
            actual_ranges.append((node.start_layer, node.end_layer))

    # Trim trailing (0,0) if no assignment expected
    actual_trimmed = [r for r in actual_ranges if r != (0, 0)]
    expected_total = sum(e - s for (s, e) in expected_ranges)
    assert sum(e - s for (s, e) in actual_trimmed) == expected_total
    # Order-insensitive comparison: ranges represent stages; allow pipeline reordering
    assert Counter(actual_trimmed) == Counter(
        expected_ranges
    ), f"Stage ranges mismatch (order-insensitive):\nactual={actual_trimmed}\nexpected={expected_ranges}"


@pytest.mark.parametrize("strategy", ["greedy", "dp"])
def test_single_node_can_host_all_layers_greedy(strategy: Literal["greedy", "dp"]):
    """Small model fully hosted by a single strong node (A100-80g)."""
    model = build_model_info(5)
    node = _build_node("a100-80g", model)
    alloc = (
        GreedyLayerAllocator(model, [node])
        if strategy == "greedy"
        else DynamicProgrammingLayerAllocator(model, [node])
    )
    initialized = alloc.global_allocation()
    assert initialized is True
    assert alloc.has_full_pipeline() is True
    assert node.start_layer == 0 and node.end_layer == model.num_layers


@pytest.mark.parametrize("strategy", ["greedy", "dp"])
def test_mixed_pool_single_host_available(strategy: Literal["greedy", "dp"]):
    """Intermediate model: one A100 can host alone; others (4090) remain unused."""
    model = build_model_info(6)
    a100 = _build_node("a100-80g", model, id_suffix="-a")
    r1 = _build_node("rtx4090", model, id_suffix="-1")
    r2 = _build_node("rtx4090", model, id_suffix="-2")
    alloc = (
        GreedyLayerAllocator(model, [a100, r1, r2])
        if strategy == "greedy"
        else DynamicProgrammingLayerAllocator(model, [a100, r1, r2])
    )
    initialized = alloc.global_allocation()
    assert initialized is True
    # A100 should cover entire model
    assert a100.start_layer == 0 and a100.end_layer == model.num_layers
    assert r1.start_layer == 0 and r1.end_layer == 3
    assert r2.start_layer == 3 and r2.end_layer == model.num_layers


@pytest.mark.parametrize("strategy", ["greedy", "dp"])
def test_pipeline_required_with_midrange_only(strategy: Literal["greedy", "dp"]):
    """Model requires pipeline across multiple mid-range GPUs (RTX4090)."""
    model = build_model_info(7)
    nodes = [_build_node("rtx4090", model, id_suffix=f"-{i}") for i in range(3)]
    alloc = (
        GreedyLayerAllocator(model, nodes)
        if strategy == "greedy"
        else DynamicProgrammingLayerAllocator(model, nodes)
    )
    ok = alloc.global_allocation()
    assert ok is True
    # At least two nodes should be assigned to cover 7 layers
    assigned = [(n.node_id, n.start_layer, n.end_layer) for n in nodes if n.start_layer is not None]
    assert len(assigned) >= 2
    total = sum(e - s for _, s, e in assigned)
    assert total == model.num_layers


@pytest.mark.parametrize("strategy", ["greedy", "dp"])
def test_allocator_does_not_duplicate_leftover_nodes(strategy: Literal["greedy", "dp"]):
    """Both allocators should not duplicate self.nodes when left over after allocation.

    Greedy: builds multiple pipelines, leaves nodes that can't form another pipeline
    DP: optimizes for best pipeline configuration, may leave suboptimal nodes unallocated
    """
    model = build_model_info(12)

    if strategy == "greedy":
        # Two strong nodes form two complete pipelines (greedy maximizes pipelines)
        # One weak node left unallocated
        a100_1 = _build_node("a100-80g", model, id_suffix="-a1")
        a100_2 = _build_node("a100-80g", model, id_suffix="-a2")
        r1 = _build_node("rtx4090", model, id_suffix="-1")
        nodes = [a100_1, a100_2, r1]
        expected_node_count = 3
    else:
        # One strong node handles all layers (DP finds this optimal)
        # One weak node left unallocated
        a100 = _build_node("a100-80g", model, id_suffix="-a")
        r1 = _build_node("rtx4090", model, id_suffix="-1")
        nodes = [a100, r1]
        expected_node_count = 2

    alloc = (
        GreedyLayerAllocator(model, nodes)
        if strategy == "greedy"
        else DynamicProgrammingLayerAllocator(model, nodes)
    )
    ok = alloc.global_allocation()
    assert ok is True
    assert len(alloc.nodes) == expected_node_count, "Should not duplicate nodes during allocation"


================================================================================
File: tests/scheduler_tests/test_request_routing.py
Size: 10.16 kB
================================================================================

"""
Unit tests for request routing strategies.

Covers:
- Shard-level DP path using `Node` APIs (latency + RTT)
- Turning point detection via layer-level DP
- Parametrized scenarios with different splits/overlaps
- Round-robin baseline pipeline routing and overload skipping
"""

import pytest

from scheduling.node import Node
from scheduling.request_routing import (
    DynamicProgrammingRouting,
    RoundRobinPipelineRouting,
)

from .test_utils import build_model_info as build_model
from .test_utils import build_node, set_rtt_from_coords


def test_optimal_path_simple_chain():
    """Two-node chain [0, k) -> [k, L): verify path and latency sum."""
    num_layers = 12
    split = 7
    model = build_model(num_layers)
    n1 = build_node("n1", model, tflops=200.0, x=0.0, y=0.0)
    n2 = build_node("n2", model, tflops=200.0, x=1.0, y=0.0)
    n1.set_layer_allocation(0, split)
    n2.set_layer_allocation(split, num_layers)
    set_rtt_from_coords([n1, n2])

    router = DynamicProgrammingRouting()
    node_ids, latency = router.find_optimal_path([n1, n2], num_layers)

    assert node_ids == ["n1", "n2"]
    expected = float(n1.layer_latency_ms) + float(n1.get_rtt_to(n2)) + float(n2.layer_latency_ms)
    assert latency == pytest.approx(expected, rel=1e-6)


def test_optimal_path_single_node():
    """Single-node [0, L) path should have no hops and sum only node latency."""
    num_layers = 10
    model = build_model(num_layers)
    n = build_node("solo", model, tflops=250.0, x=0.0, y=0.0)
    n.set_layer_allocation(0, num_layers)

    router = DynamicProgrammingRouting()
    node_ids, latency = router.find_optimal_path([n], num_layers)

    assert node_ids == ["solo"]
    assert latency == pytest.approx(float(n.layer_latency_ms), rel=1e-6)


def test_optimal_path_missing_rtt():
    """If RTT is missing between two nodes in a path, it should be invalid."""
    num_layers = 12
    model = build_model(num_layers)
    n1 = build_node("n1", model, tflops=200.0, x=0.0, y=0.0)
    n2 = build_node("n2", model, tflops=200.0, x=1.0, y=0.0)
    n1.set_layer_allocation(0, 6)
    n2.set_layer_allocation(6, 12)
    nodes = [n1, n2]
    set_rtt_from_coords(nodes)

    # Manually remove the RTT info between n1 and n2
    if n2.node_id in n1.rtt_to_nodes:
        del n1.rtt_to_nodes[n2.node_id]

    router = DynamicProgrammingRouting()
    node_ids, latency = router.find_optimal_path(nodes, num_layers)

    assert node_ids == []
    assert latency == float("inf")


@pytest.mark.parametrize(
    "num_layers,segments,expected_path",
    [
        (
            15,
            [("a", 0, 5, 200.0, 0.0), ("b", 5, 10, 200.0, 1.0), ("c", 10, 15, 200.0, 2.0)],
            ["a", "b", "c"],
        ),
        (
            12,
            [("full", 0, 12, 150.0, 0.0), ("tail", 6, 12, 300.0, 1.0)],
            ["full"],
        ),
    ],
)
def test_optimal_path_parametrized(
    num_layers: int,
    segments: list[tuple[str, int, int, float, float]],
    expected_path: list[str],
):
    """Parameterized shard-level routing across contiguous and overlap cases."""
    model = build_model(num_layers)
    nodes: list[Node] = []
    for node_id, start, end, tflops, x in segments:
        n = build_node(node_id, model, tflops=tflops, x=x, y=0.0)
        n.set_layer_allocation(start, end)
        nodes.append(n)

    router = DynamicProgrammingRouting()
    set_rtt_from_coords(nodes)
    node_ids, latency = router.find_optimal_path(nodes, num_layers)
    assert node_ids == expected_path
    # Sanity: latency equals sum of node latencies along path plus RTTs
    total = 0.0
    for i, nid in enumerate(node_ids):
        n = next(n for n in nodes if n.node_id == nid)
        total += float(n.layer_latency_ms)
        if i > 0:
            prev = next(n for n in nodes if n.node_id == node_ids[i - 1])
            total += float(prev.get_rtt_to(n))
    assert latency == pytest.approx(total, rel=1e-6)


@pytest.mark.parametrize(
    "num_layers,segments,expected_turns",
    [
        # tail has dramatically larger I/O and close-enough y so small RTT
        # 'head' should 'turn' to the tail ASAP (tail truncation)
        (10, [("head", 0, 6, 1.0, 0.0), ("tail", 4, 10, 3000.0, 0.01)], [("head", 4, "tail")]),
        # front truncation: path first uses node 'mid' at layer 3, so drop [0,3) on 'mid'
        (
            12,
            [("head", 0, 6, 500.0, 0.0), ("mid", 2, 8, 1.0, 0.5), ("end", 7, 12, 500.0, 0.6)],
            [("mid", 7, "tail"), ("mid", 6, "head")],
        ),
        (8, [("solo", 0, 8, 250.0, 0.0)], []),
    ],
)
def test_turning_points(
    num_layers: int,
    segments: list[tuple[str, int, int, float, float]],
    expected_turns: list[tuple[str, int, str]],
):
    """Parameterized turning-point detection across contiguous and overlap cases."""
    model = build_model(num_layers)
    nodes: list[Node] = []
    for node_id, start, end, io, x in segments:
        n = build_node(node_id, model, mem_bandwidth_gbps=io, x=x, y=0.0)
        n.set_layer_allocation(start, end)
        nodes.append(n)

    router = DynamicProgrammingRouting()
    set_rtt_from_coords(nodes)
    turns = router.find_turning_points(nodes, num_layers)
    # Order-insensitive comparison: we only require the same set of truncation points
    assert set(turns) == set(expected_turns)


def test_round_robin_pipelines_cycle_between_two_complete_paths():
    """Two complete pipelines -> round-robin alternates between them deterministically."""
    num_layers = 12
    model = build_model(num_layers)
    # Pipelines: [a(0,6), b(6,12)] and [c(0,4), d(4,12)]
    a = build_node("a", model, tflops=200.0, x=0.0, y=0.0)
    b = build_node("b", model, tflops=200.0, x=1.0, y=0.0)
    c = build_node("c", model, tflops=220.0, x=0.0, y=1.0)
    d = build_node("d", model, tflops=220.0, x=1.0, y=1.0)
    a.set_layer_allocation(0, 6)
    b.set_layer_allocation(6, 12)
    c.set_layer_allocation(0, 4)
    d.set_layer_allocation(4, 12)
    nodes = [a, b, c, d]
    set_rtt_from_coords(nodes)

    rr = RoundRobinPipelineRouting()
    paths = []
    for _ in range(4):
        node_ids, latency = rr.find_optimal_path(nodes, num_layers)
        assert node_ids in (["a", "b"], ["c", "d"])
        # Latency equals sum of node latencies plus one RTT
        n0 = next(n for n in nodes if n.node_id == node_ids[0])
        n1 = next(n for n in nodes if n.node_id == node_ids[1])
        expected = (
            float(n0.layer_latency_ms) + float(n0.get_rtt_to(n1)) + float(n1.layer_latency_ms)
        )
        assert latency == pytest.approx(expected, rel=1e-6)
        paths.append(tuple(node_ids))

    # Expect strict alternation between the two pipelines
    assert paths[0] != paths[1]
    assert paths[0] == paths[2]
    assert paths[1] == paths[3]


def test_round_robin_skips_overloaded_pipeline():
    """If any node in a pipeline is overloaded, that entire pipeline is skipped."""
    num_layers = 10
    model = build_model(num_layers)
    # Two pipelines: [p1a, p1b] and [p2a, p2b]
    p1a = build_node("p1a", model, tflops=200.0, x=0.0, y=0.0)
    p1b = build_node("p1b", model, tflops=200.0, x=1.0, y=0.0)
    p2a = build_node("p2a", model, tflops=200.0, x=0.0, y=1.0)
    p2b = build_node("p2b", model, tflops=200.0, x=1.0, y=1.0)
    p1a.set_layer_allocation(0, 4)
    p1b.set_layer_allocation(4, 10)
    p2a.set_layer_allocation(0, 3)
    p2b.set_layer_allocation(3, 10)
    nodes = [p1a, p1b, p2a, p2b]
    set_rtt_from_coords(nodes)

    # Overload p1b to invalidate pipeline 1
    p1b.current_requests = p1b.max_requests

    rr = RoundRobinPipelineRouting()
    # Multiple calls should always pick the viable pipeline [p2a, p2b]
    for _ in range(3):
        node_ids, latency = rr.find_optimal_path(nodes, num_layers)
        assert node_ids == ["p2a", "p2b"]
        n0, n1 = p2a, p2b
        expected = (
            float(n0.layer_latency_ms) + float(n0.get_rtt_to(n1)) + float(n1.layer_latency_ms)
        )
        assert latency == pytest.approx(expected, rel=1e-6)

    # Now overload p2a as well -> no viable pipelines
    p2a.current_requests = p2a.max_requests
    node_ids, latency = rr.find_optimal_path(nodes, num_layers)
    assert node_ids == []
    assert latency == float("inf")


def test_round_robin_pipeline_discovery_overlapping_heads_and_tails():
    """Ensure pipeline_discovery finds multiple pipelines with overlapping heads/tails.

    Scenario layers [0, 64): nodes with ranges
        [0, 22), [0, 23), [22, 43), [23, 42), [23, 47), [43, 64), [47, 64)
    Expected pipelines:
        - 0 -> 22 -> 43 -> 47 -> 64
        - 0 -> 23 -> 47 -> 64
    """
    num_layers = 64
    model = build_model(num_layers)

    def N(nid: str, s: int, e: int) -> Node:
        n = build_node(nid, model, tflops=200.0, x=0.0, y=0.0)
        n.set_layer_allocation(s, e)
        return n

    nodes = [
        N("h1", 0, 22),
        N("h2", 0, 23),
        N("m1", 22, 43),
        N("m2", 23, 42),
        N("m3", 23, 47),
        N("t1", 43, 64),
        N("t2", 47, 64),
    ]

    rr = RoundRobinPipelineRouting()
    pipelines = rr.pipeline_discovery(nodes, num_layers)

    # Pipelines should be sequences of node ids
    assert len(pipelines) >= 2
    # Convert to layer ranges to validate coverage and specific two expected pipelines
    id_to_node = {n.node_id: n for n in nodes}
    ranges = [
        [(id_to_node[nid].start_layer, id_to_node[nid].end_layer) for nid in p] for p in pipelines
    ]

    expected1 = [
        (0, 22),
        (22, 43),
        (43, 64),
    ]  # path via h1 -> m1 -> t1/t2 (choose minimal ends greedily)
    # Given the greedy rule, 43->47->64 is also a possible chain; we accept either tail
    expected1_alt = [(0, 22), (22, 43), (47, 64)]
    expected2 = [(0, 23), (23, 47), (47, 64)]

    def matches_path(path, expected):
        return len(path) == len(expected) and all(a == b for a, b in zip(path, expected))

    has_expected1 = any(
        matches_path(r, expected1) or matches_path(r, expected1_alt) for r in ranges
    )
    has_expected2 = any(matches_path(r, expected2) for r in ranges)
    assert has_expected1 and has_expected2


================================================================================
File: tests/scheduler_tests/test_scheduler.py
Size: 9.85 kB
================================================================================

"""
Minimal tests for the Scheduler orchestrator.
"""

from __future__ import annotations

from scheduling.node import RequestSignal
from scheduling.scheduler import Scheduler

from .test_utils import build_model_info, build_node, set_rtt_from_coords


def test_scheduler_initialize_and_dispatch():
    """Allocate, then enqueue one request and dispatch it."""
    model = build_model_info(12)
    n1 = build_node("a100-0", model, tflops=312.0, mem_gb=80.0, x=0, y=0)
    n2 = build_node("a100-1", model, tflops=312.0, mem_gb=80.0, x=1, y=0)
    set_rtt_from_coords([n1, n2])

    sched = Scheduler(model, [n1, n2], strategy="greedy", min_nodes_bootstrapping=1)
    sched.layer_allocator.global_allocation()
    allocs = sched.list_node_allocations()
    assert allocs, "Allocator should assign at least one pipeline"
    # Check coverage equals total layers
    total = sum(e - s for _, s, e in allocs)
    assert total >= model.num_layers

    # Push a request and dispatch
    req = RequestSignal(request_id="req-1")
    sched.receive_request(req)
    assignment = sched.dispatch_next_request()
    assert assignment is not None
    req_id, path, latency = assignment
    assert req_id == req.request_id
    assert path, "Path should be non-empty"
    assert latency >= 0.0


def test_scheduler_join_and_leave():
    """New node can join and be assigned; leave removes it and may rebalance."""
    model = build_model_info(12)
    n1 = build_node("a100-0", model, tflops=312.0, mem_gb=80.0, x=0, y=0)
    n2 = build_node("a100-1", model, tflops=312.0, mem_gb=80.0, x=1, y=0)
    set_rtt_from_coords([n1, n2])
    sched = Scheduler(model, [n1, n2], strategy="greedy", min_nodes_bootstrapping=1)

    # Join a new node
    n3 = build_node("rtx4090-x", model, tflops=82.6, mem_gb=24.0, x=0, y=1)
    sched.join(n3)
    assert n3.start_layer is not None and n3.end_layer is not None

    # Leave
    sched.leave(n3.node_id)
    assert n3 not in sched.nodes


def test_scheduler_bootstrap_wait_and_dynamic_events():
    """Scheduler waits for min nodes, bootstraps, then handles join/leave events."""
    model = build_model_info(12)
    # Start with no nodes assigned yet; bootstrap needs 2
    n1 = build_node("a100-0", model, tflops=312.0, mem_gb=80.0, x=0, y=0)
    sched = Scheduler(model, [], strategy="dp", min_nodes_bootstrapping=2)

    # Enqueue one join; should not bootstrap yet (insufficient nodes)
    sched.enqueue_join(n1)
    # Process events once (simulate part of event loop)
    sched._process_joins()  # type: ignore[attr-defined]
    assert len(sched.nodes) == 1
    assert not sched.layer_allocator.has_full_pipeline()

    # Add second node and process join; now bootstrap should succeed
    n2 = build_node("5090-1", model, tflops=165.0, mem_gb=32.0, x=1, y=0)
    sched.enqueue_join(n2)
    sched._process_joins()  # type: ignore[attr-defined]
    # RTTs are needed for DP routing strategy
    set_rtt_from_coords(sched.nodes)
    ok = sched.bootstrap()
    assert ok
    assert sched.layer_allocator.has_full_pipeline()

    # Dynamic join after bootstrap should assign immediately
    n3 = build_node("rtx4090-x", model, tflops=82.6, mem_gb=24.0, x=0, y=1)
    sched.enqueue_join(n3)
    sched._process_joins()  # type: ignore[attr-defined]
    assert n3.start_layer is not None and n3.end_layer is not None
    print(sched.layer_allocator.list_node_allocations())

    # Leave a non-critical node; if still full pipeline, no global rebalance forced
    remaining_before = sched.layer_allocator.has_full_pipeline()
    sched.leave(n3.node_id)
    assert sched.layer_allocator.has_full_pipeline() == remaining_before

    print(sched.layer_allocator.list_node_allocations())

    for node in list(sched.nodes):
        if node.start_layer is not None and node.end_layer is not None:
            sched.layer_allocator.deallocate(node)
    # Re-allocate only first node to make pipeline incomplete
    sched.layer_allocator.allocate(sched.nodes[0], 0, model.num_layers - 1)
    # Now leave that node to break coverage and trigger global rebalance path
    core_id = sched.nodes[0].node_id
    sched.leave(core_id)


def test_scheduler_single_node_leave_then_rejoin_reassigns_layers():
    """With one node, after leave then re-join, layers should be re-assigned.

    Reproduction of observed issue: when `min_nodes_bootstrapping=1`, after killing the
    only node (leave) and re-joining it, the scheduler fails to re-assign layers.
    This test encodes the expected behavior (should re-assign), so it currently fails.
    """
    model = build_model_info(12)

    # Start with a single capable node and bootstrap successfully
    n1 = build_node("solo-0", model, tflops=312.0, mem_gb=80.0, x=0, y=0)
    set_rtt_from_coords([n1])
    sched = Scheduler(model, [n1], strategy="dp", min_nodes_bootstrapping=1)
    ok = sched.bootstrap()
    assert ok
    assert n1.start_layer is not None and n1.end_layer is not None

    # Simulate node leave (e.g., the process was killed)
    sched.leave(n1.node_id)
    assert n1 not in sched.nodes
    assert not sched.layer_allocator.has_full_pipeline()

    # Re-join the (same) node id; scheduler should re-assign layers
    n1_rejoin = build_node("solo-0", model, tflops=312.0, mem_gb=80.0, x=0, y=0)
    sched.enqueue_join(n1_rejoin)
    sched._process_joins()  # type: ignore[attr-defined]

    # Expected behavior: after re-join with min_nodes_bootstrapping=1, layers are assigned again
    assert (
        n1_rejoin.start_layer is not None and n1_rejoin.end_layer is not None
    ), "After re-join, single node should be assigned a full layer range"


def test_scheduler_three_nodes_sequential_join_leave_rejoin():
    """Test scheduler with 28-layer model, 3 nodes each capable of 22 layers.

    Scenario:
    - 28-layer model
    - n1, n2, n3 all can host 22 layers
    - min_nodes_bootstrapping=2
    - n1, n2, n3 join sequentially
    - n1 leaves and rejoins
    - n2 leaves and rejoins
    - n3 leaves and rejoins
    """
    model = build_model_info(28)

    # Create nodes that can each host 22 layers
    # Calculation: 100GB can host 16 layers, so 22 layers need ~137.5GB
    # Using 150GB to ensure capacity for 22 layers with some margin
    n1 = build_node("n1", model, tflops=312.0, mem_gb=138.0, x=0, y=0)
    n2 = build_node("n2", model, tflops=312.0, mem_gb=138.0, x=1, y=0)
    n3 = build_node("n3", model, tflops=312.0, mem_gb=138.0, x=2, y=0)

    # Verify nodes can host 22 layers
    assert n1.get_decoder_layer_capacity() >= 22, "n1 should be able to host 22 layers"
    assert n2.get_decoder_layer_capacity() >= 22, "n2 should be able to host 22 layers"
    assert n3.get_decoder_layer_capacity() >= 22, "n3 should be able to host 22 layers"

    # Initialize scheduler with min_nodes_bootstrapping=2, no nodes initially
    sched = Scheduler(model, [], strategy="dp", min_nodes_bootstrapping=2)

    # Step 1: n1 joins (not enough nodes yet)
    sched.enqueue_join(n1)
    sched._process_joins()  # type: ignore[attr-defined]
    assert len(sched.nodes) == 1
    assert not sched.layer_allocator.has_full_pipeline()

    # Step 2: n2 joins (now we have 2 nodes, should bootstrap)
    sched.enqueue_join(n2)
    sched._process_joins()  # type: ignore[attr-defined]
    set_rtt_from_coords(sched.nodes)
    ok = sched.bootstrap()
    assert ok, "Bootstrap should succeed with 2 nodes"
    assert sched.layer_allocator.has_full_pipeline()

    # Step 3: n3 joins (dynamic join after bootstrap)
    sched.enqueue_join(n3)
    sched._process_joins()  # type: ignore[attr-defined]
    set_rtt_from_coords(sched.nodes)
    assert n3.start_layer is not None and n3.end_layer is not None
    assert len(sched.nodes) == 3

    # Step 4: n1 leaves and rejoins
    n1_id = n1.node_id
    sched.leave(n1_id)
    assert n1 not in sched.nodes
    assert len(sched.nodes) == 2
    assert sched.layer_allocator.has_full_pipeline()

    # Rejoin n1
    n1_rejoin = build_node("n1", model, tflops=312.0, mem_gb=138.0, x=0, y=0)
    sched.enqueue_join(n1_rejoin)
    sched._process_joins()  # type: ignore[attr-defined]
    set_rtt_from_coords(sched.nodes)
    assert n1_rejoin.start_layer is not None and n1_rejoin.end_layer is not None
    assert len(sched.nodes) == 3
    assert sched.layer_allocator.has_full_pipeline()

    # Step 5: n2 leaves and rejoins
    n2_id = n2.node_id
    sched.leave(n2_id)
    assert n2 not in sched.nodes
    assert len(sched.nodes) == 2
    assert sched.layer_allocator.has_full_pipeline()

    # Rejoin n2
    n2_rejoin = build_node("n2", model, tflops=312.0, mem_gb=138.0, x=1, y=0)
    sched.enqueue_join(n2_rejoin)
    sched._process_joins()  # type: ignore[attr-defined]
    set_rtt_from_coords(sched.nodes)
    assert n2_rejoin.start_layer is not None and n2_rejoin.end_layer is not None
    assert len(sched.nodes) == 3
    assert sched.layer_allocator.has_full_pipeline()

    # Step 6: n3 leaves and rejoins
    n3_id = n3.node_id
    sched.leave(n3_id)
    assert n3 not in sched.nodes
    assert len(sched.nodes) == 2
    assert sched.layer_allocator.has_full_pipeline()

    # Rejoin n3
    n3_rejoin = build_node("n3", model, tflops=312.0, mem_gb=138.0, x=2, y=0)
    sched.enqueue_join(n3_rejoin)
    sched._process_joins()  # type: ignore[attr-defined]
    set_rtt_from_coords(sched.nodes)
    assert n3_rejoin.start_layer is not None and n3_rejoin.end_layer is not None
    assert len(sched.nodes) == 3
    assert sched.layer_allocator.has_full_pipeline()

    # Final verification: all nodes should have layer assignments
    allocations = sched.list_node_allocations()
    assert len(allocations) == 3, "All 3 nodes should have layer assignments"
    # Verify full pipeline coverage
    total_covered = sum(e - s for _, s, e in allocations)
    assert total_covered >= model.num_layers, "All layers should be covered"


================================================================================
File: tests/scheduler_tests/test_utils.py
Size: 5.28 kB
================================================================================

"""
Reusable test helpers for model/node builders and RTT utilities.
"""

from __future__ import annotations

from math import sqrt
from typing import Dict, Iterable, List, Tuple

from scheduling.model_info import ModelInfo
from scheduling.node import Node, NodeHardwareInfo

A100_80G = NodeHardwareInfo(
    node_id="a100-80g",
    num_gpus=1,
    tflops_fp16=312.0,
    gpu_name="",
    memory_gb=80.0,
    memory_bandwidth_gbps=2039,
    device="cuda",
)
A100_40G = NodeHardwareInfo(
    node_id="a100-40g",
    num_gpus=1,
    tflops_fp16=312.0,
    gpu_name="",
    memory_gb=40.0,
    memory_bandwidth_gbps=1935,
    device="cuda",
)
RTX5090 = NodeHardwareInfo(
    node_id="rtx5090",
    num_gpus=1,
    tflops_fp16=104.8,
    gpu_name="",
    memory_gb=32.0,
    memory_bandwidth_gbps=1792,
    device="cuda",
)
RTX4090 = NodeHardwareInfo(
    node_id="rtx4090",
    num_gpus=1,
    tflops_fp16=82.6,
    gpu_name="",
    memory_gb=24.0,
    memory_bandwidth_gbps=1008,
    device="cuda",
)


def build_model_info(num_layers: int) -> ModelInfo:
    """Build a model config used across tests (matches allocation tests)."""
    return ModelInfo(
        model_name=f"GPUOss-{num_layers}L",
        mlx_model_name=f"MLXOss-{num_layers}L",
        head_size=64,
        hidden_dim=2880,
        intermediate_dim=2880,
        num_attention_heads=64,
        num_kv_heads=8,
        vocab_size=201088,
        num_layers=num_layers,
        ffn_num_projections=3,
        num_local_experts=128,
        num_experts_per_tok=4,
        param_bytes_per_element=1,
        mlx_param_bytes_per_element=1,
        cache_bytes_per_element=2,
        embedding_bytes_per_element=2,
    )


def build_node(
    node_id: str,
    model: ModelInfo,
    tflops: float = 200.0,
    mem_gb: float = 80.0,
    x: float = 0.0,
    y: float = 0.0,
    mem_bandwidth_gbps: float = 100.0,
) -> Node:
    """Create a `Node` with hardware info and attach test-only coordinates/bandwidth."""
    hw = NodeHardwareInfo(
        node_id=node_id,
        num_gpus=1,
        tflops_fp16=tflops,
        gpu_name="",
        memory_gb=mem_gb,
        memory_bandwidth_gbps=mem_bandwidth_gbps,
        device="cuda",
    )
    n = Node(node_id=node_id, hardware=hw, model_info=model, _force_max_concurrent_requests=True)
    # Attach coordinates for RTT synthesis in tests
    setattr(n, "_x", float(x))
    setattr(n, "_y", float(y))
    # Ensure roofline uses a defined speedup
    setattr(n, "quantization_speedup", 1.0)
    return n


def compute_rtts_from_coords(nodes: Iterable[Node]) -> Dict[Tuple[str, str], float]:
    """Map Euclidean distances between nodes' (x, y) to RTTs in [10, 200] ms."""
    node_list = list(nodes)
    if not node_list:
        return {}
    coords: Dict[str, Tuple[float, float]] = {
        n.node_id: (
            float(getattr(n, "_x", 0.0)),
            float(getattr(n, "_y", 0.0)),
        )
        for n in node_list
    }
    ids = [n.node_id for n in node_list]

    max_dist = 0.0
    for i, aid in enumerate(ids):
        ax, ay = coords[aid]
        for bid in ids[i + 1 :]:
            bx, by = coords[bid]
            d = sqrt((ax - bx) ** 2 + (ay - by) ** 2)
            max_dist = max(max_dist, d)

    def to_latency(d: float) -> float:
        return 10.0 if max_dist <= 0 else 10.0 + 190.0 * (d / max_dist)

    rtts: Dict[Tuple[str, str], float] = {(nid, nid): 10.0 for nid in ids}
    for i, aid in enumerate(ids):
        ax, ay = coords[aid]
        for bid in ids[i + 1 :]:
            bx, by = coords[bid]
            d = sqrt((ax - bx) ** 2 + (ay - by) ** 2)
            lat = to_latency(d)
            rtts[(aid, bid)] = lat
            rtts[(bid, aid)] = lat
    return rtts


def set_rtt_from_coords(nodes: List[Node]) -> None:
    """Populate `rtt_to_nodes` on each node based on their coordinates."""
    all_rtts = compute_rtts_from_coords(nodes)
    node_map = {n.node_id: n for n in nodes}
    ids = list(node_map.keys())

    for aid in ids:
        node_a = node_map[aid]
        if node_a.rtt_to_nodes is None:
            node_a.rtt_to_nodes = {}
        for bid in ids:
            if aid == bid:
                continue
            rtt = all_rtts.get((aid, bid))
            if rtt is not None:
                node_a.rtt_to_nodes[bid] = rtt


def geo_rtt_provider(positions: Dict[str, Tuple[float, float]]):
    """Create an RTT provider mapping Euclidean distance to [10, 200] ms.

    Scales by the maximum pairwise distance among provided positions.
    """
    ids = list(positions.keys())
    # Compute max pairwise distance for scaling
    max_dist = 0.0
    for i, aid in enumerate(ids):
        ax, ay = positions[aid]
        for bid in ids[i + 1 :]:
            bx, by = positions[bid]
            max_dist = max(max_dist, ((ax - bx) ** 2 + (ay - by) ** 2) ** 0.5)

    def to_latency(d: float) -> float:
        return 10.0 if max_dist <= 0 else 10.0 + 190.0 * (d / max_dist)

    def provider(src: Node, dst: Node) -> float:
        if src.node_id == dst.node_id:
            return 0.0
        sx, sy = positions.get(src.node_id, (0.0, 0.0))
        dx, dy = positions.get(dst.node_id, (0.0, 0.0))
        dist = ((sx - dx) ** 2 + (sy - dy) ** 2) ** 0.5
        return to_latency(dist)

    return provider


================================================================================
File: tests/test_batch_scheduler.py
Size: 3.35 kB
================================================================================

from parallax.server.request import InitialRequest, Request, RequestStatus
from parallax.server.scheduler import Scheduler


class FakeKVCacheManager:
    def __init__(self, allow: bool = True):
        self.allow = allow
        self._reqs = set()

    def has_request(self, request_id: str) -> bool:
        return request_id in self._reqs

    def allocate_request(self, request_id: str, num_tokens: int) -> bool:
        """PagedKV interface."""
        if not self.allow:
            return False
        self._reqs.add(request_id)
        return True


def make_prefill(rid: str, prompt_len: int) -> InitialRequest:
    return InitialRequest(request_id=rid, input_ids=[0] * prompt_len)


def make_decode(rid: str, ready: bool = True) -> Request:
    r = Request(request_id=rid, status=RequestStatus.DECODING)
    r.ready_for_next_step = ready
    return r


def test_prefill_fifo_and_micro_batch():
    sched = Scheduler(max_batch_size=8, max_num_tokens_per_batch=10_000, micro_batch_ratio=1)
    # micro_batch_size = max_batch_size // ratio = 8
    # Enqueue 3 prefills in order
    r1 = make_prefill("r1", 5)
    r2 = make_prefill("r2", 6)
    r3 = make_prefill("r3", 7)
    sched.enque_request(r1)
    sched.enque_request(r2)
    sched.enque_request(r3)

    batch = sched.form_batch()
    ids = [r.request_id for r in batch]
    assert ids[:3] == ["r1", "r2", "r3"]


def test_decode_ready_order_and_prefill_first():
    # micro_batch_size = 3
    sched = Scheduler(max_batch_size=3, max_num_tokens_per_batch=10_000, micro_batch_ratio=1)

    # Two decodes already running
    d1 = make_decode("d1")
    d2 = make_decode("d2")
    sched._running_requests[d1.request_id] = d1
    sched._running_requests[d2.request_id] = d2

    # One prefill in queue
    p1 = make_prefill("p1", 8)
    sched.enque_request(p1)

    # Mark d1 ready first, then d2
    sched.enque_request(d1)  # sets ready_for_next_step + LRU move_to_end
    sched.enque_request(d2)

    sched.admit_requests()
    batch = sched.form_batch()
    ids = [r.request_id for r in batch]

    # Prefill first, then decodes in the order they became ready
    assert ids == ["p1", "d1", "d2"]


def test_token_budget_prefill_skipped_decode_taken():
    # Token budget too small for prefill, but enough for decodes (cost=1)
    sched = Scheduler(max_batch_size=2, max_num_tokens_per_batch=1, micro_batch_ratio=1)

    # One large prefill
    p_big = make_prefill("p_big", 5)
    sched.enque_request(p_big)

    # One ready decode already running
    d = make_decode("d")
    sched._running_requests[d.request_id] = d
    sched.enque_request(d)

    batch = sched.form_batch()
    ids = [r.request_id for r in batch]
    assert ids == ["d"]
    # ready flag should be reset after batching
    assert getattr(d, "ready_for_next_step", False) is False


def test_kv_cache_admission_guard_blocks_prefill():
    # A KV manager that rejects additions
    kv_mgr = FakeKVCacheManager(allow=False)
    sched = Scheduler(
        max_batch_size=2,
        max_num_tokens_per_batch=100,
        micro_batch_ratio=1,
        kv_cache_manager=kv_mgr,
    )
    p = make_prefill("p", 4)
    sched.enque_request(p)

    # Admission should fail and running set remains empty; batch should be empty
    batch = sched.form_batch()
    assert len(batch) == 0
    assert sched.num_running_requests == 0


================================================================================
File: tests/test_executor.py
Size: 8 kB
================================================================================

"""
Unit tests for the Executor class, using Qwen3-0.6B-bf16.
For ubuntu-GPU, test 3 pipelines
  - cuda -> cuda -> cuda
  - cuda -> mlx(cpu) -> cuda
  - mlx(cpu) -> cuda -> mlx(cpu)
For MAC, test 1 pipeline
  - mlx -> mlx -> mlx
"""

import pytest
from mlx_lm.generate import generate
from mlx_lm.utils import get_model_path, load_model

from parallax.p2p.message_util import proto_to_request, request_to_proto
from parallax.server.request import InitialRequest
from parallax.utils.tokenizer_utils import load_tokenizer
from parallax.utils.utils import get_current_device

MLX_MODEL_REPO = "mlx-community/Qwen3-0.6B-bf16"
CUDA_MODEL_REPO = "Qwen/Qwen3-0.6B"

model_path = get_model_path(MLX_MODEL_REPO)[0]
ref_model, ref_config = load_model(model_path)
ref_tokenizer = load_tokenizer(model_path, eos_token_ids=ref_config.get("eos_token_id", None))


def create_executor(start_layer, end_layer, device, kv_cache_memory_fraction=0.3):
    """Create a pipeline sharded executor

    Args:
        start_layer: Start layer index
        end_layer: End layer index
        device: Device type ("mlx" or "cuda")
        kv_cache_memory_fraction: Memory fraction for KV cache (will be divided by number of executors on same device)
    """
    if device == "mlx":
        model_repo = MLX_MODEL_REPO
        from parallax.server.executor.mlx_executor import MLXExecutor

        executor = MLXExecutor(
            model_repo=model_repo,
            start_layer=start_layer,
            end_layer=end_layer,
            kv_cache_memory_fraction=kv_cache_memory_fraction,
            dtype="bfloat16",
            device=device,
        )
    else:
        model_repo = CUDA_MODEL_REPO
        from parallax.server.executor.sglang_executor import SGLExecutor

        executor = SGLExecutor(
            model_repo=model_repo,
            start_layer=start_layer,
            end_layer=end_layer,
            kv_cache_memory_fraction=kv_cache_memory_fraction,
            dtype="bfloat16",
            device=device,
        )
    return executor


def run_executor_pipeline_stage(executor, requests, batch_type, is_last_peer):
    """Run executor pipeline stage. Input and output should be requests"""
    executor.handle_input_requests(requests)
    executor.scheduler.admit_requests()
    input_batch = executor.scheduler.form_batch()
    prepared_batch = executor.prepare_batch_inputs(input_batch)
    assert prepared_batch is not None, "Failed to prepare batch inputs"
    batch_data = prepared_batch[batch_type]
    hidden_states = executor.process_batch(batch_data, return_decoded_tokens=is_last_peer)
    output_reqs = executor.prepare_next_batch_requests(
        requests=batch_data["requests"],
        hidden_states=hidden_states,
        context_lengths=batch_data.get("context_lengths"),
    )
    return output_reqs, hidden_states


@pytest.mark.parametrize(
    "pipeline_devices",
    [
        ("cuda", "cuda", "cuda"),
        ("cuda", "mlx", "cuda"),
        ("mlx", "cuda", "mlx"),
        ("mlx", "mlx", "mlx"),
    ],
)
@pytest.mark.parametrize("pp_end_layers", [(10, 18, 28)])
@pytest.mark.parametrize("num_decode_steps", [8])
def test_decode_pipeline_multiple_steps(pipeline_devices, pp_end_layers, num_decode_steps):
    """Tests a multi-step decode pipeline with batched requests."""
    device = get_current_device()
    if device == "mlx" and "cuda" in pipeline_devices:
        return

    # 1. Setup executors
    # Calculate memory fraction for each executor based on how many executors share the same device
    # Total memory fraction to use across all executors
    # Note: We use a higher fraction (0.5) to account for model weights that are already loaded
    # The actual KV cache will use less due to the conservative calculation in _calculate_num_blocks

    # Calculate fraction per executor (divide by number of executors on same device)
    executor_peer1 = create_executor(
        start_layer=0,
        end_layer=pp_end_layers[0],
        device=pipeline_devices[0],
        kv_cache_memory_fraction=0.3,
    )
    executor_peer2 = create_executor(
        start_layer=pp_end_layers[0],
        end_layer=pp_end_layers[1],
        device=pipeline_devices[1],
        kv_cache_memory_fraction=0.3,
    )
    executor_peer3 = create_executor(
        start_layer=pp_end_layers[1],
        end_layer=pp_end_layers[2],
        device=pipeline_devices[2],
        kv_cache_memory_fraction=0.3,
    )

    # 2. Setup initial requests for multiple prompts
    prompts = [
        "The capital of China is",
        "Qwen is a large language model developed by",
    ]
    initial_requests = [
        InitialRequest(request_id=f"req{i}", input_ids=executor_peer1.tokenizer.encode(p))
        for i, p in enumerate(prompts)
    ]

    # 3. Prefill
    prefill_reqs_out1, _ = run_executor_pipeline_stage(
        executor_peer1, initial_requests, "prefill_batch", False
    )
    prefill_proto_p1 = request_to_proto(prefill_reqs_out1, device=pipeline_devices[0])

    prefill_reqs_in2 = proto_to_request(prefill_proto_p1, device=pipeline_devices[1])
    prefill_reqs_out2, _ = run_executor_pipeline_stage(
        executor_peer2, prefill_reqs_in2, "prefill_batch", False
    )
    prefill_proto_p2 = request_to_proto(prefill_reqs_out2, device=pipeline_devices[1])

    prefill_reqs_in3 = proto_to_request(prefill_proto_p2, device=pipeline_devices[2])
    prefill_reqs_out3, gen_tokens = run_executor_pipeline_stage(
        executor_peer3, prefill_reqs_in3, "prefill_batch", True
    )
    prefill_proto_p3 = request_to_proto(prefill_reqs_out3, device=pipeline_devices[2])

    generated_tokens_pipeline = [gen_tokens]
    print(f"Prefill done: generated_tokens_pipeline: {generated_tokens_pipeline}")
    first_rank_proto = prefill_proto_p3

    # 4. Decode
    for _ in range(num_decode_steps):
        decode_reqs_in1 = proto_to_request(first_rank_proto, device=pipeline_devices[0])
        decode_reqs_out1, _ = run_executor_pipeline_stage(
            executor_peer1, decode_reqs_in1, "decode_batch", False
        )
        decode_proto_p1 = request_to_proto(decode_reqs_out1, device=pipeline_devices[0])

        decode_reqs_in2 = proto_to_request(decode_proto_p1, device=pipeline_devices[1])
        decode_reqs_out2, _ = run_executor_pipeline_stage(
            executor_peer2, decode_reqs_in2, "decode_batch", False
        )
        decode_proto_p2 = request_to_proto(decode_reqs_out2, device=pipeline_devices[1])

        decode_reqs_in3 = proto_to_request(decode_proto_p2, device=pipeline_devices[2])
        decode_reqs_out3, next_gen_tokens = run_executor_pipeline_stage(
            executor_peer3, decode_reqs_in3, "decode_batch", True
        )
        decode_proto_p3 = request_to_proto(decode_reqs_out3, device=pipeline_devices[2])

        first_rank_proto = decode_proto_p3
        generated_tokens_pipeline.append(next_gen_tokens)

    # 5. Compare with reference
    total_tokens_to_generate = 1 + num_decode_steps
    for i, prompt in enumerate(prompts):
        # Generate reference tokens using mlx-lm's standard generation
        ref_output_text = generate(
            ref_model,
            ref_tokenizer,
            prompt,
            max_tokens=total_tokens_to_generate,
            verbose=False,
        )
        print(f"prompt: {prompt}")
        print(f"mlx-lm reference generation: {ref_output_text}")
        output_tokens_for_prompt = [
            gen_step_tokens[i].item() for gen_step_tokens in generated_tokens_pipeline
        ]

        # Decode the token IDs into a string
        output_text = executor_peer1.tokenizer.decode(output_tokens_for_prompt)
        print(f"parallax test generation: {output_text}")

        # Trim the first whitespace in our output
        assert ref_output_text[:5] == output_text[1:6]

    # 6. Release resources for next tests
    executor_peer1.shutdown()
    executor_peer2.shutdown()
    executor_peer3.shutdown()
    del executor_peer1
    del executor_peer2
    del executor_peer3


================================================================================
File: tests/test_http_handler.py
Size: 2.82 kB
================================================================================

import asyncio
from http import HTTPStatus

try:
    import torch  # type: ignore
except Exception:  # pragma: no cover - torch might be unavailable in CI
    import importlib.machinery
    import sys
    import types

    torch_stub = types.ModuleType("torch")
    torch_stub.__spec__ = importlib.machinery.ModuleSpec("torch", loader=None)

    class _DeviceStatus:
        @staticmethod
        def is_available():
            return False

    torch_stub.cuda = _DeviceStatus()
    torch_stub.mps = _DeviceStatus()
    torch_stub.float16 = "float16"
    torch_stub.bfloat16 = "bfloat16"
    torch_stub.float32 = "float32"
    sys.modules.setdefault("torch", torch_stub)

from parallax.server.http_server import HTTPHandler, HTTPRequestInfo


def test_http_handler_marks_non_stream_error():
    async def scenario():
        handler = HTTPHandler.__new__(HTTPHandler)
        handler.processing_requests = {}

        rid = "req-non-stream"
        request_info = HTTPRequestInfo(id=rid, stream=False)
        handler.processing_requests[rid] = request_info

        await handler._handle_executor_error(
            rid,
            {
                "error": "Invalid template",
                "error_type": "TemplateError",
                "status_code": HTTPStatus.BAD_REQUEST.value,
            },
        )
        return request_info

    request_info = asyncio.run(scenario())

    assert request_info.is_finish is True
    assert request_info.finish_reason == "error"
    assert request_info.error_message == "Invalid template"
    assert request_info.error_type == "TemplateError"
    assert request_info.error_status == HTTPStatus.BAD_REQUEST


def test_http_handler_stream_error_pushes_queue_event():
    async def scenario():
        handler = HTTPHandler.__new__(HTTPHandler)
        handler.processing_requests = {}

        rid = "req-stream"
        request_info = HTTPRequestInfo(id=rid, stream=True)
        request_info.token_queue = asyncio.Queue()
        handler.processing_requests[rid] = request_info

        await handler._handle_executor_error(
            rid,
            {
                "error": "Executor failure",
                "error_type": "InternalServerError",
                "status_code": HTTPStatus.INTERNAL_SERVER_ERROR.value,
            },
        )

        error_chunk = await request_info.token_queue.get()
        sentinel = await request_info.token_queue.get()
        return request_info, error_chunk, sentinel

    request_info, error_chunk, sentinel = asyncio.run(scenario())

    assert error_chunk["type"] == "error"
    assert error_chunk["payload"]["message"] == "Executor failure"
    assert error_chunk["payload"]["type"] == "InternalServerError"
    assert error_chunk["payload"]["code"] == HTTPStatus.INTERNAL_SERVER_ERROR.value
    assert sentinel is None


================================================================================
File: tests/test_message_util.py
Size: 9.85 kB
================================================================================

"""
Test the message utility functions.
"""

import mlx.core as mx
import numpy as np
import pytest

from parallax.p2p.message_util import (
    abort_request_to_proto,
    bytes_to_tensor,
    proto_to_abort_request,
    proto_to_request,
    proto_to_sampling_params,
    request_to_proto,
    sampling_params_to_proto,
    tensor_to_bytes,
)
from parallax.p2p.proto import forward_pb2
from parallax.server.request import IntermediateRequest, Request, RequestStatus
from parallax.server.sampling.sampling_params import SamplingParams


class TestMessageUtil:
    """
    Test the message utility functions.
    """

    def setup_method(self):
        """Set up test fixtures."""
        self.request_id = "test_request_123"
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_new_tokens=100,
            stop_strs=["\n"],
        )

    def test_request_to_proto_prefilling(self):
        """Test converting a PREFILLING IntermediateRequest to protobuf."""
        hidden_states = mx.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=mx.float32)
        request = IntermediateRequest(
            request_id=self.request_id,
            input_ids=[1, 2, 3],
            current_position=3,
            status=RequestStatus.PREFILLING,
            hidden_states=hidden_states,
            sampling_params=self.sampling_params,
            routing_table=["layer1", "layer2"],
            lora_path=None,
        )

        forward_request = request_to_proto([request])

        assert forward_request.forward_mode == forward_pb2.ForwardMode.EXTEND
        assert len(forward_request.reqs) == 1
        proto_req = forward_request.reqs[0]
        assert proto_req.rid == self.request_id
        assert list(proto_req.input_ids) == [1, 2, 3]
        assert proto_req.output_length == 0
        assert proto_req.sampling_params.temperature == pytest.approx(0.7)
        assert list(proto_req.routing_table) == ["layer1", "layer2"]

        # Verify hidden_states
        deserialized_hs = bytes_to_tensor(proto_req.hidden_states)
        np.testing.assert_array_equal(
            np.array(deserialized_hs.tolist()), np.array(hidden_states.tolist())
        )

    def test_request_to_proto_decoding(self):
        """Test converting a DECODING IntermediateRequest to protobuf."""
        # The IntermediateRequest requires hidden_states for initialization.
        # For DECODING status, it should be serialized.
        hidden_states = mx.array([[0.0]], dtype=mx.float32)
        request = IntermediateRequest(
            request_id=self.request_id,
            input_ids=[],
            current_position=10,
            status=RequestStatus.DECODING,
            hidden_states=hidden_states,
            next_token_id=42,
            sampling_params=self.sampling_params,
            lora_path=None,
        )

        forward_request = request_to_proto([request])

        assert forward_request.forward_mode == forward_pb2.ForwardMode.DECODE
        assert len(forward_request.reqs) == 1
        proto_req = forward_request.reqs[0]
        assert proto_req.rid == self.request_id
        assert proto_req.next_token_id == 42
        assert proto_req.hidden_states  # hidden_states should be serialized

    def test_proto_to_request_conversion(self):
        """Test the round-trip conversion from request to proto and back."""
        hidden_states = mx.array([[1.0, 2.0]], dtype=mx.bfloat16)
        original_request = IntermediateRequest(
            request_id=self.request_id,
            input_ids=[10, 20],
            current_position=12,
            status=RequestStatus.PREFILLING,
            hidden_states=hidden_states,
            next_token_id=50,
            sampling_params=self.sampling_params,
            routing_table=["nodeA"],
            lora_path=None,
        )

        proto_request = request_to_proto([original_request])
        converted_requests = proto_to_request(proto_request)

        assert len(converted_requests) == 1
        converted_request = converted_requests[0]

        assert converted_request.request_id == original_request.request_id
        assert converted_request.status == original_request.status
        assert converted_request.input_ids == original_request.input_ids
        # current_position = output_length + len(input_ids)
        assert converted_request.current_position == original_request.current_position
        assert converted_request.next_token_id == original_request.next_token_id

        # Compare sampling params field by field since __eq__ is not implemented
        assert converted_request.sampling_params.temperature == pytest.approx(
            original_request.sampling_params.temperature
        )
        assert converted_request.sampling_params.top_p == pytest.approx(
            original_request.sampling_params.top_p
        )
        assert (
            converted_request.sampling_params.max_new_tokens
            == original_request.sampling_params.max_new_tokens
        )
        assert (
            converted_request.sampling_params.stop_strs
            == original_request.sampling_params.stop_strs
        )

        assert converted_request.routing_table == original_request.routing_table

        np.testing.assert_array_equal(
            np.array(converted_request.hidden_states.tolist()),
            np.array(original_request.hidden_states.tolist()),
        )

    def test_multiple_requests(self):
        """Test conversion of multiple requests."""
        req1 = IntermediateRequest(
            request_id="req1",
            input_ids=[1],
            current_position=1,
            status=RequestStatus.PREFILLING,
            hidden_states=mx.array([[1.0]], dtype=mx.float32),
            lora_path=None,
        )
        req2 = IntermediateRequest(
            request_id="req2",
            input_ids=[2],
            current_position=1,
            status=RequestStatus.PREFILLING,
            hidden_states=mx.array([[2.0]], dtype=mx.float32),
            lora_path=None,
        )

        forward_request = request_to_proto([req1, req2])
        assert len(forward_request.reqs) == 2

        # Test that hidden_states are not concatenated
        deserialized_reqs = proto_to_request(forward_request)
        assert len(deserialized_reqs) == 2
        np.testing.assert_array_equal(
            np.array(deserialized_reqs[0].hidden_states.tolist()), np.array([[1.0]])
        )
        np.testing.assert_array_equal(
            np.array(deserialized_reqs[1].hidden_states.tolist()), np.array([[2.0]])
        )

    @pytest.mark.parametrize(
        "dtype,shape",
        [
            (mx.float32, (2, 2)),
            (mx.bfloat16, (1, 8)),
            (mx.int32, (5,)),
        ],
    )
    def test_tensor_serialization(self, dtype, shape):
        """Test tensor serialization and deserialization."""
        original_tensor = mx.arange(int(np.prod(shape)), dtype=dtype).reshape(shape)

        serialized_bytes = tensor_to_bytes(original_tensor)
        deserialized_tensor = bytes_to_tensor(serialized_bytes)

        assert deserialized_tensor.shape == original_tensor.shape
        assert deserialized_tensor.dtype == original_tensor.dtype
        np.testing.assert_array_equal(
            np.array(deserialized_tensor.tolist()),
            np.array(original_tensor.tolist()),
        )

    def test_sampling_params_conversion(self):
        """Test SamplingParams conversion to and from proto."""
        params = SamplingParams(
            max_new_tokens=200,
            min_new_tokens=10,
            temperature=0.5,
            top_p=0.8,
            min_p=0.1,
            top_k=40,
            stop_strs=["\n", "stop"],
            stop_token_ids=[0, 1],
            ignore_eos=True,
            repetition_penalty=1.2,
            presence_penalty=0.2,
            frequency_penalty=0.3,
            json_schema='{"type": "string"}',
        )

        proto = sampling_params_to_proto(params)
        converted_params = proto_to_sampling_params(proto)

        assert converted_params.max_new_tokens == params.max_new_tokens
        assert converted_params.min_new_tokens == params.min_new_tokens
        assert pytest.approx(converted_params.temperature) == params.temperature
        assert pytest.approx(converted_params.top_p) == params.top_p
        assert pytest.approx(converted_params.min_p) == params.min_p
        assert converted_params.top_k == params.top_k
        assert converted_params.stop_strs == params.stop_strs
        assert converted_params.stop_token_ids == params.stop_token_ids
        assert converted_params.ignore_eos == params.ignore_eos
        assert pytest.approx(converted_params.repetition_penalty) == params.repetition_penalty
        assert pytest.approx(converted_params.presence_penalty) == params.presence_penalty
        assert pytest.approx(converted_params.frequency_penalty) == params.frequency_penalty
        assert converted_params.json_schema == params.json_schema

    def test_abort_request(self):
        """Test abort request conversion."""
        req1 = Request(request_id="abort1", routing_table=["nodeA", "nodeB"])
        req2 = Request(request_id="abort2", routing_table=["nodeC"])

        abort_proto = abort_request_to_proto([req1, req2])
        assert len(abort_proto.reqs) == 2
        assert abort_proto.reqs[0].rid == "abort1"
        assert list(abort_proto.reqs[0].routing_table) == ["nodeA", "nodeB"]

        intermediate_reqs = proto_to_abort_request(abort_proto)
        assert len(intermediate_reqs) == 2
        assert intermediate_reqs[0].request_id == "abort1"
        assert intermediate_reqs[0].status == RequestStatus.FINISHED_EOS
        assert intermediate_reqs[0].routing_table == ["nodeA", "nodeB"]
        assert intermediate_reqs[1].request_id == "abort2"


================================================================================
File: tests/test_model.py
Size: 5.56 kB
================================================================================

"""
Tests for the ShardedModel loader utilities.
"""

from typing import List, Tuple

import mlx.core as mx
import pytest
from mlx_lm.models.base import create_attention_mask
from mlx_lm.utils import get_model_path, load_model

from parallax.server.paged_kv_cache import PagedKVCacheManager
from parallax.server.shard_loader import MLXModelLoader
from parallax.utils.tokenizer_utils import load_tokenizer
from parallax.utils.utils import pad_inputs

REPO_ID = "mlx-community/Qwen3-0.6B-bf16"
TOTAL_LAYERS = 28


model_path = get_model_path(REPO_ID)[0]
ref_model, ref_config = load_model(model_path)
ref_tokenizer = load_tokenizer(model_path, eos_token_ids=ref_config.get("eos_token_id", None))


@pytest.mark.parametrize(
    "layers_config",
    [
        [(0, 12), (12, TOTAL_LAYERS)],
        [(0, 8), (8, 16), (16, TOTAL_LAYERS)],
    ],
)
def test_shard_prefill(layers_config: List[Tuple[int, int]]) -> None:
    """Load sharded model based on layers_config and
    compare its forward pass with a full reference model.
    """
    dtype = mx.bfloat16

    # Load sharded models
    model_shards = []
    for layer_from, layer_to in layers_config:
        loader = MLXModelLoader(
            model_path_or_hf_repo=REPO_ID,
            start_layer=layer_from,
            end_layer=layer_to,
        )
        model_shard_instance, _, _ = loader.load()
        model_shards.append(model_shard_instance)

    # Prepare test inputs
    texts = [
        "This is a test.",
        "This is yet another test.",
        "what color is Mars",
        "what color is Moon",
    ]
    ref_ids = [ref_tokenizer.encode(text) for text in texts]
    ref_pad_token_id = ref_tokenizer.pad_token_id or ref_tokenizer.eos_token_id
    ref_ids, ref_mask = pad_inputs(ref_pad_token_id, ref_ids, dtype=dtype)

    # Run reference model
    def _call_with_mask(self, inputs, cache=None, mask=None):
        h = self.model.embed_tokens(inputs)
        if cache is None:
            cache = [None] * len(self.layers)
        if mask is None:
            mask_inner = create_attention_mask(h, cache[0])
        else:
            mask_inner = mask
        for layer, c in zip(self.layers, cache):
            h = layer(h, mask_inner, c)
        h = self.model.norm(h)
        if self.args.tie_word_embeddings:
            h = self.model.embed_tokens.as_linear(h)
        else:
            h = self.lm_head(h)
        return h

    type(ref_model).__call__ = _call_with_mask
    ref_out = ref_model(ref_ids, mask=ref_mask)

    # Prepare PagedKV cache managers for each shard
    batch_size = len(texts)
    max_seq_len = ref_ids.shape[1]
    num_kv_heads = ref_config.get("num_key_value_heads")
    head_dim = ref_config.get("head_dim") or ref_config.get("hidden_size") // ref_config.get(
        "num_attention_heads"
    )

    kv_cache_managers = []
    cache_memory_fraction = 0
    for shard in model_shards:
        num_shard_layers = shard.end_layer - shard.start_layer
        cache_memory_fraction += 0.1
        kv_mgr = PagedKVCacheManager(
            num_layers=num_shard_layers,
            num_kv_heads=num_kv_heads,
            head_dim=head_dim,
            dtype=dtype,
            block_size=64,
            num_gpu_blocks=200,
        )
        kv_cache_managers.append(kv_mgr)

    # Prepare common inputs
    padding_mask = (ref_ids != ref_pad_token_id).astype(dtype)
    actual_seq_lengths = [int((padding_mask[i] > 0).sum()) for i in range(batch_size)]

    # Run sharded models
    x = None
    for shard_idx, shard in enumerate(model_shards):
        kv_cache_manager = kv_cache_managers[shard_idx]

        # Allocate blocks and prepare metadata
        block_tables_list = []
        context_lengths_list = []
        slot_mapping_flat = []

        for i in range(batch_size):
            req_id = f"req_{i}_shard_{shard_idx}"
            seq_len = actual_seq_lengths[i]
            context_lengths_list.append(seq_len)

            success = kv_cache_manager.allocate_request(req_id, seq_len)
            assert success, f"Failed to allocate blocks for request {i} in shard {shard_idx}"

            block_table = kv_cache_manager.get_block_table(req_id)
            block_tables_list.append(block_table)

            # Generate slot mapping
            for seq_idx in range(max_seq_len):
                if seq_idx < seq_len:
                    block_idx = seq_idx // kv_cache_manager.block_size
                    block_offset = seq_idx % kv_cache_manager.block_size
                    physical_block = block_table[block_idx]
                    slot = physical_block * kv_cache_manager.block_size + block_offset
                    slot_mapping_flat.append(slot)
                else:
                    slot_mapping_flat.append(-1)

        # Pad block tables
        max_blocks = max(len(bt) for bt in block_tables_list)
        padded_block_tables = [bt + [0] * (max_blocks - len(bt)) for bt in block_tables_list]

        block_tables = mx.array(padded_block_tables, dtype=mx.int32)
        context_lengths = mx.array(context_lengths_list, dtype=mx.int32)
        slot_mapping = mx.array(slot_mapping_flat, dtype=mx.int64)
        cache = kv_cache_manager.get_cache()

        # Forward pass
        input_data = ref_ids if shard.start_layer == 0 else x
        x = shard(
            input_data,
            cache=cache,
            mask=ref_mask,
            block_tables=block_tables,
            context_lengths=context_lengths,
            slot_mapping=slot_mapping,
        )

    assert mx.allclose(x, ref_out, atol=1e-3, rtol=1e-3)


================================================================================
File: tests/test_paged_attention.py
Size: 13.17 kB
================================================================================

import math
import time

import mlx.core as mx
import numpy as np
import pytest

from parallax.metal.paged_attention.kernel import paged_attention, reshape_and_cache


def ref_masked_attention(q, k, v, scale):
    """
    Reference implementation of attention for verification.
    Used for basic tests.
    q: (batch, n_heads, head_dim)
    k: (batch, n_heads, seq_len, head_dim)
    v: (batch, n_heads, seq_len, head_dim)
    """
    # (batch, n_heads, 1, head_dim) @ (batch, n_heads, head_dim, seq_len) -> (batch, n_heads, 1, seq_len)
    scores = (q[:, :, None, :] @ k.transpose(0, 1, 3, 2)) * scale
    probs = mx.softmax(scores, axis=-1)
    # (batch, n_heads, 1, seq_len) @ (batch, n_heads, seq_len, head_dim) -> (batch, n_heads, 1, head_dim)
    output = probs @ v
    return output.squeeze(2)  # (batch, n_heads, head_dim)


def ref_attention_large(q, k, v, scale):
    """
    Reference implementation handling GQA/MQA.
    Used for large scale tests.
    q: (batch, n_heads, 1, head_dim)
    k: (batch, n_kv_heads, seq_len, head_dim)
    v: (batch, n_kv_heads, seq_len, head_dim)
    """
    n_heads = q.shape[1]
    n_kv_heads = k.shape[1]
    n_rep = n_heads // n_kv_heads

    if n_rep > 1:
        k = mx.repeat(k[:, :, None, :, :], n_rep, axis=2).reshape(
            k.shape[0], n_heads, k.shape[2], k.shape[3]
        )
        v = mx.repeat(v[:, :, None, :, :], n_rep, axis=2).reshape(
            v.shape[0], n_heads, v.shape[2], v.shape[3]
        )

    scores = (q @ k.transpose(0, 1, 3, 2)) * scale
    probs = mx.softmax(scores, axis=-1)
    output = probs @ v
    return output.squeeze(2)  # (B, H, D)


class TestPagedAttention:

    @pytest.mark.parametrize("dtype", [mx.float32, mx.float16, mx.bfloat16])
    def test_basic_functionality(self, dtype):
        """Test reshape_and_cache and paged_attention with different dtypes on small data."""
        # Check for bfloat16 support
        if dtype == mx.bfloat16:
            try:
                mx.array(1.0, dtype=mx.bfloat16)
            except ValueError:
                pytest.skip("bfloat16 not supported")

        # Constants
        BATCH_SIZE = 2
        NUM_HEADS = 4
        NUM_KV_HEADS = 4
        HEAD_DIM = 32
        BLOCK_SIZE = 16
        NUM_LAYERS = 1
        NUM_BLOCKS = 1024
        LAYER_IDX = 0
        SCALE = 1.0 / math.sqrt(HEAD_DIM)
        atol = 1e-2 if dtype != mx.float32 else 1e-4

        # Setup Memory
        key_cache = mx.zeros(
            (NUM_LAYERS, NUM_BLOCKS, NUM_KV_HEADS, BLOCK_SIZE, HEAD_DIM), dtype=dtype
        )
        value_cache = mx.zeros(
            (NUM_LAYERS, NUM_BLOCKS, NUM_KV_HEADS, BLOCK_SIZE, HEAD_DIM), dtype=dtype
        )

        # Mock Block Tables
        max_blocks_per_req = 2
        block_tables_np = np.zeros((BATCH_SIZE, max_blocks_per_req), dtype=np.int32)
        block_tables_np[0, :] = [0, 1]
        block_tables_np[1, 0] = 2
        block_tables = mx.array(block_tables_np)

        # Context Lengths
        context_lengths = mx.array([20, 5], dtype=mx.int32)

        # --- Step 1: Test reshape_and_cache ---
        k_new = mx.random.uniform(shape=(BATCH_SIZE, NUM_KV_HEADS, 1, HEAD_DIM)).astype(dtype)
        v_new = mx.random.uniform(shape=(BATCH_SIZE, NUM_KV_HEADS, 1, HEAD_DIM)).astype(dtype)

        new_k_cache, new_v_cache = reshape_and_cache(
            k_new,
            v_new,
            key_cache,
            value_cache,
            block_tables,
            context_lengths,
            BLOCK_SIZE,
            LAYER_IDX,
        )
        mx.eval(new_k_cache, new_v_cache)

        # Verify Data in Cache
        # Req 0 (len 20) -> Block 1, Offset 3
        cached_k_0 = new_k_cache[0, 1, :, 3, :]
        input_k_0 = k_new[0].squeeze(1)
        assert mx.allclose(cached_k_0, input_k_0, atol=atol).item(), "Cache update failed for Req 0"

        # Req 1 (len 5) -> Block 2, Offset 4
        cached_k_1 = new_k_cache[0, 2, :, 4, :]
        input_k_1 = k_new[1].squeeze(1)
        assert mx.allclose(
            cached_k_1, input_k_1, atol=atol
        ).item(), "Cache update failed for Req 1 (Key)"

        cached_v_1 = new_v_cache[0, 2, :, 4, :]
        input_v_1 = v_new[1].squeeze(1)
        assert mx.allclose(
            cached_v_1, input_v_1, atol=atol
        ).item(), "Cache update failed for Req 1 (Value)"

        # --- Step 2: Test paged_attention ---
        q = mx.random.uniform(shape=(BATCH_SIZE, NUM_HEADS, 1, HEAD_DIM)).astype(dtype)

        output = paged_attention(
            q,
            new_k_cache,
            new_v_cache,
            block_tables,
            context_lengths,
            BLOCK_SIZE,
            SCALE,
            NUM_KV_HEADS,
            LAYER_IDX,
        )
        mx.eval(output)

        # Verify against Reference
        # Construct inputs for reference
        k_part1 = mx.zeros((NUM_KV_HEADS, 4, HEAD_DIM), dtype=dtype)
        k_part2 = k_new[1]
        k_full_1 = mx.concatenate([k_part1, k_part2], axis=1)

        v_part1 = mx.zeros((NUM_KV_HEADS, 4, HEAD_DIM), dtype=dtype)
        v_part2 = v_new[1]
        v_full_1 = mx.concatenate([v_part1, v_part2], axis=1)

        k_ref_input = k_full_1[None, :, :, :]
        v_ref_input = v_full_1[None, :, :, :]
        q_input = q[1:2].squeeze(2)

        ref_out = ref_masked_attention(
            q_input.astype(mx.float32),
            k_ref_input.astype(mx.float32),
            v_ref_input.astype(mx.float32),
            SCALE,
        )
        kernel_out_1 = output[1].squeeze(1).astype(mx.float32)

        assert mx.allclose(
            kernel_out_1, ref_out[0], atol=atol
        ).item(), f"Paged Attention mismatch for Req 1 with {dtype}"

    @pytest.mark.parametrize(
        "params",
        [
            {
                "bs": 8,
                "len": 2048,
                "heads": 32,
                "kv_heads": 32,
                "dim": 128,
                "desc": "MHA Llama-2-7B Style",
            },
            {
                "bs": 4,
                "len": 2048,
                "heads": 32,
                "kv_heads": 8,
                "dim": 128,
                "desc": "GQA Custom Style",
            },
        ],
    )
    def test_large_scale_correctness(self, params):
        """
        Test paged_attention correctness on larger scales with MHA/GQA.
        Uses float16 for reasonable memory usage/precision check.
        """
        batch_size = params["bs"]
        seq_len = params["len"]
        num_heads = params["heads"]
        num_kv_heads = params["kv_heads"]
        head_dim = params["dim"]
        block_size = 16
        dtype = mx.float16

        scale = 1.0 / (head_dim**0.5)
        num_blocks_per_req = (seq_len + block_size - 1) // block_size
        total_blocks = num_blocks_per_req * batch_size

        # Setup Cache
        key_cache = mx.zeros((1, total_blocks, num_kv_heads, block_size, head_dim), dtype=dtype)
        value_cache = mx.zeros((1, total_blocks, num_kv_heads, block_size, head_dim), dtype=dtype)

        all_blocks = np.arange(total_blocks, dtype=np.int32).reshape(batch_size, num_blocks_per_req)
        block_tables = mx.array(all_blocks)
        context_lengths = mx.array([seq_len] * batch_size, dtype=mx.int32)

        # Generate Data
        k_history = mx.random.uniform(shape=(batch_size, num_kv_heads, seq_len, head_dim)).astype(
            dtype
        )
        v_history = mx.random.uniform(shape=(batch_size, num_kv_heads, seq_len, head_dim)).astype(
            dtype
        )
        q = mx.random.uniform(shape=(batch_size, num_heads, 1, head_dim)).astype(dtype)

        # Populate Cache (Simulated via reshape/assign)
        padded_len = num_blocks_per_req * block_size
        if padded_len > seq_len:
            padding = mx.zeros(
                (batch_size, num_kv_heads, padded_len - seq_len, head_dim), dtype=dtype
            )
            k_padded = mx.concatenate([k_history, padding], axis=2)
            v_padded = mx.concatenate([v_history, padding], axis=2)
        else:
            k_padded = k_history
            v_padded = v_history

        k_reshaped = k_padded.reshape(
            batch_size, num_kv_heads, num_blocks_per_req, block_size, head_dim
        )
        v_reshaped = v_padded.reshape(
            batch_size, num_kv_heads, num_blocks_per_req, block_size, head_dim
        )

        k_ready = k_reshaped.transpose(0, 2, 1, 3, 4).reshape(
            total_blocks, num_kv_heads, block_size, head_dim
        )
        v_ready = v_reshaped.transpose(0, 2, 1, 3, 4).reshape(
            total_blocks, num_kv_heads, block_size, head_dim
        )

        key_cache = k_ready[None, ...]
        value_cache = v_ready[None, ...]
        mx.eval(key_cache, value_cache)

        # Run Kernel
        out = paged_attention(
            q,
            key_cache,
            value_cache,
            block_tables,
            context_lengths,
            block_size,
            scale,
            num_kv_heads,
            0,
        )
        mx.eval(out)

        # Run Reference
        ref_out = ref_attention_large(
            q.astype(mx.float32), k_history.astype(mx.float32), v_history.astype(mx.float32), scale
        )
        kernel_out = out.astype(mx.float32).squeeze(2)

        # Check Correctness
        # Relax tolerance slightly for large scale FP16 accumulations
        diff = mx.abs(ref_out - kernel_out)
        max_diff = mx.max(diff).item()

        assert (
            max_diff < 1e-2
        ), f"Large scale test failed for {params['desc']}, max_diff: {max_diff}"

    def test_benchmark_paged_vs_native(self):
        """
        Benchmark PagedAttention vs Native MLX SDPA.
        """
        # Config
        batch_size = 8
        num_heads = 32
        num_kv_heads = 32
        head_dim = 128
        seq_len = 200  # Length
        block_size = 1024
        dtype = mx.float16
        scale = 1.0 / math.sqrt(head_dim)

        # Data
        q = mx.random.uniform(shape=(batch_size, num_heads, 1, head_dim)).astype(dtype)
        k_cont = mx.random.uniform(shape=(batch_size, num_kv_heads, seq_len, head_dim)).astype(
            dtype
        )
        v_cont = mx.random.uniform(shape=(batch_size, num_kv_heads, seq_len, head_dim)).astype(
            dtype
        )

        # Setup Paged
        num_blocks = (seq_len + block_size - 1) // block_size
        total_blocks = num_blocks * batch_size
        # Quick setup without manager for pure kernel benchmark
        key_cache = mx.zeros((1, total_blocks, num_kv_heads, block_size, head_dim), dtype=dtype)
        value_cache = mx.zeros((1, total_blocks, num_kv_heads, block_size, head_dim), dtype=dtype)
        all_blocks = np.arange(total_blocks, dtype=np.int32).reshape(batch_size, num_blocks)
        block_tables = mx.array(all_blocks)
        context_lengths = mx.array([seq_len] * batch_size, dtype=mx.int32)
        # Force materialize paged cache
        mx.eval(key_cache, value_cache)

        # Benchmark Native (using mx.fast.scaled_dot_product_attention if available or manual)
        # For fair comparison, we assume native implementation is fast.
        # Manual implementation is slow in python.
        # Let's check if mx.fast.scaled_dot_product_attention exists
        has_sdpa = hasattr(mx.fast, "scaled_dot_product_attention")

        print(f"\n[Benchmark BS={batch_size}, Len={seq_len}]")

        if has_sdpa:
            # K needs to be (B, H, L, D) for SDPA?
            # SDPA docs: queries, keys, values.
            # Usually expects (B, H, L, D) or (B, L, H, D) depending on layout.
            # MLX SDPA: (q, k, v, ...).
            # Let's assume (B, H, L, D) for simplicity or transpose.
            # Native SDPA is highly optimized.
            start = time.perf_counter()
            for _ in range(100):
                # Transpose to match SDPA expectations if needed
                _ = mx.fast.scaled_dot_product_attention(q, k_cont, v_cont, scale=scale)
                mx.eval(_)
            end = time.perf_counter()
            native_time = (end - start) / 100 * 1000
            print(f"Native SDPA:     {native_time:.3f} ms")
        else:
            print("Native SDPA:     N/A (Not available in this MLX version)")

        # Benchmark Paged
        # Warmup
        for _ in range(5):
            _ = paged_attention(
                q,
                key_cache,
                value_cache,
                block_tables,
                context_lengths,
                block_size,
                scale,
                num_kv_heads,
                0,
            )
            mx.eval(_)

        start = time.perf_counter()
        for _ in range(100):
            out = paged_attention(
                q,
                key_cache,
                value_cache,
                block_tables,
                context_lengths,
                block_size,
                scale,
                num_kv_heads,
                0,
            )
            mx.eval(out)
        end = time.perf_counter()
        paged_time = (end - start) / 100 * 1000
        print(f"Paged Attention: {paged_time:.3f} ms")


if __name__ == "__main__":
    unittest.main()


================================================================================
File: tests/test_paged_kv_integration.py
Size: 5.87 kB
================================================================================

import unittest

import mlx.core as mx
import numpy as np

from parallax.metal.paged_attention.kernel import reshape_and_cache
from parallax.server.paged_kv_cache import PagedKVCacheManager


class TestPagedKVIntegration(unittest.TestCase):
    def setUp(self):
        self.num_layers = 1
        self.num_kv_heads = 4
        self.head_dim = 16
        self.block_size = 16
        self.dtype = mx.float32

        # Initialize Cache Manager
        # Mocking device info to avoid OOM or device dependency in test env
        # Assuming cache_memory_fraction results in enough blocks
        # We will manually override num_gpu_blocks if needed or rely on default fallback
        self.cache_manager = PagedKVCacheManager(
            num_layers=self.num_layers,
            num_kv_heads=self.num_kv_heads,
            head_dim=self.head_dim,
            dtype=self.dtype,
            block_size=self.block_size,
            cache_memory_fraction=0.5,
        )

        # Ensure we have enough blocks for testing
        if self.cache_manager.num_gpu_blocks < 100:
            # Manually resize if auto-calc gave too few (e.g. on small device)
            pass

    def test_prefill_slot_mapping(self):
        """
        Test that slot_mapping is correctly generated and used in reshape_and_cache
        for a batch of sequences with different lengths (Prefill phase).
        """
        # 1. Setup Requests
        req_ids = ["req1", "req2"]
        seq_lens = [10, 20]  # Both < block_size * 2 for simplicity

        # Allocate blocks
        for rid, slen in zip(req_ids, seq_lens):
            self.cache_manager.allocate_request(rid, slen)

        block_tables = []
        for rid in req_ids:
            block_tables.append(self.cache_manager.get_block_table(rid))

        # Pad block tables
        max_blocks = max(len(bt) for bt in block_tables)
        padded_block_tables = []
        for bt in block_tables:
            padded_block_tables.append(bt + [0] * (max_blocks - len(bt)))
        block_tables_tensor = mx.array(padded_block_tables, dtype=mx.int32)

        context_lengths_tensor = mx.array(seq_lens, dtype=mx.int32)

        # 2. Generate Input Data (Batch, MaxLen, KV_Heads, Head_Dim)
        # We need to flatten this for reshape_and_cache
        max_len = 20
        batch_size = 2

        # Create dummy key/value data
        # We use a pattern to easily verify: key[b, l] = b * 1000 + l
        keys_np = np.zeros(
            (batch_size, max_len, self.num_kv_heads, self.head_dim), dtype=np.float32
        )
        values_np = np.zeros(
            (batch_size, max_len, self.num_kv_heads, self.head_dim), dtype=np.float32
        )

        for b in range(batch_size):
            for l in range(seq_lens[b]):
                keys_np[b, l, :, :] = b * 1000 + l
                values_np[b, l, :, :] = -(b * 1000 + l)

        keys = mx.array(keys_np)
        values = mx.array(values_np)

        # Flatten inputs for kernel: (Batch * MaxLen, ...)
        keys_flat = keys.reshape(-1, self.num_kv_heads, self.head_dim)
        values_flat = values.reshape(-1, self.num_kv_heads, self.head_dim)

        # 3. Generate Slot Mapping (Logic from Executor)
        slot_mapping_flat = []
        for i in range(batch_size):
            block_table = block_tables[i]
            length = seq_lens[i]

            for seq_idx in range(max_len):
                if seq_idx < length:
                    block_idx = seq_idx // self.block_size
                    block_offset = seq_idx % self.block_size
                    physical_block = block_table[block_idx]
                    slot = physical_block * self.block_size + block_offset
                    slot_mapping_flat.append(slot)
                else:
                    slot_mapping_flat.append(-1)

        slot_mapping_tensor = mx.array(slot_mapping_flat, dtype=mx.int64)

        # 4. Run Kernel
        key_cache, value_cache = self.cache_manager.get_cache()

        reshape_and_cache(
            keys_flat,
            values_flat,
            key_cache,
            value_cache,
            block_tables_tensor,
            context_lengths_tensor,
            self.block_size,
            layer_idx=0,
            slot_mapping=slot_mapping_tensor,
        )

        mx.eval(key_cache, value_cache)

        # 5. Verify Cache Content
        # Check req1 (batch 0, len 10)
        # req1 fits in 1 block (block_size=16)
        block_idx_req1 = block_tables[0][0]
        # Check first token
        cached_k_0 = key_cache[0, block_idx_req1, :, 0, :]  # layer 0, block, heads, offset 0, dim
        expected_k_0 = mx.array(keys_np[0, 0, :, :])
        self.assertTrue(mx.allclose(cached_k_0, expected_k_0).item(), "Req1 Token 0 Key Mismatch")

        # Check last token of req1
        cached_k_9 = key_cache[0, block_idx_req1, :, 9, :]
        expected_k_9 = mx.array(keys_np[0, 9, :, :])
        self.assertTrue(mx.allclose(cached_k_9, expected_k_9).item(), "Req1 Token 9 Key Mismatch")

        # Check req2 (batch 1, len 20)
        # req2 spans 2 blocks. 0-15 in block 0, 16-19 in block 1
        block_idx_req2_0 = block_tables[1][0]
        block_idx_req2_1 = block_tables[1][1]

        # Check token 0 (in first block)
        cached_k_b1_0 = key_cache[0, block_idx_req2_0, :, 0, :]
        expected_k_b1_0 = mx.array(keys_np[1, 0, :, :])
        self.assertTrue(
            mx.allclose(cached_k_b1_0, expected_k_b1_0).item(), "Req2 Token 0 Key Mismatch"
        )

        # Check token 16 (in second block, offset 0)
        cached_k_b1_16 = key_cache[0, block_idx_req2_1, :, 0, :]
        expected_k_b1_16 = mx.array(keys_np[1, 16, :, :])
        self.assertTrue(
            mx.allclose(cached_k_b1_16, expected_k_b1_16).item(),
            "Req2 Token 16 Key Mismatch (Cross Block)",
        )


if __name__ == "__main__":
    unittest.main()


================================================================================
File: tests/test_prefix_cache.py
Size: 894 B
================================================================================

"""
Tests for the radix tree.
"""

import mlx.core as mx

from parallax.server.radix_cache import RadixCache

if __name__ == "__main__":
    DATA_TYPE = mx.bfloat16
    tree = RadixCache(
        num_kv_heads=1,
        head_dim=4,
        num_layers=10,
        dtype=DATA_TYPE,
        page_size=1,
        max_num_tokens=10000,
    )
    arr_for_test = mx.zeros([tree.num_layers, tree.num_kv_heads, 1, tree.head_dim], dtype=DATA_TYPE)

    tree.insert("Hello", None, arr_for_test, arr_for_test)
    tree.insert("Hello", None, arr_for_test, arr_for_test)
    tree.insert("Hello_L.A.!", None, arr_for_test, arr_for_test)
    tree.insert("Hello_world! Happy", None, arr_for_test, arr_for_test)
    tree.insert("I love you!", None, arr_for_test, arr_for_test)
    tree.pretty_print()

    print(tree.match_prefix("I love you! aha"))

    tree.evict(5)
    tree.evict(10)
    tree.pretty_print()


================================================================================
File: tests/test_sampler.py
Size: 1.73 kB
================================================================================

"""
Test for the Sampler class
"""

import unittest

import mlx.core as mx
from mlx_lm.sample_utils import apply_min_p, apply_top_k, apply_top_p

from parallax.server.sampling.sampler import Sampler, SamplingBatchInfo


class TestSampler(unittest.TestCase):
    """Tests the correctness of topk/topp/minp sampling"""

    def test_sampling(self):
        """Sampling test method"""
        temperatures = mx.array([0.5, 0.95, 1.0], dtype=mx.float32)
        top_ks = mx.array([3, 3, 1], dtype=mx.int32)
        top_ps = mx.array([0.8, 0.9, 1.0], dtype=mx.float32)
        min_ps = mx.array([0.2, 0.05, 0.05], dtype=mx.float32)

        probs = mx.array([[0.2, 0.0, 0.7, 0.1], [0.1, 0.0, 0.0, 0.9], [0.5, 0.3, 0.1, 0.1]])
        logits = mx.log(probs)

        # test sampling
        mx.random.seed(42)
        sampling_info = SamplingBatchInfo(
            temperatures=temperatures,
            top_ps=top_ps,
            top_ks=top_ks,
            min_ps=min_ps,
            is_all_greedy=False,
            need_min_p_sampling=True,
        )
        sampler = Sampler()
        batch_next_token_ids = sampler(logits, sampling_info)

        # calculate mx refs
        mx.random.seed(42)
        logits = mx.array([apply_top_k(logits[i], int(top_ks[i])) for i in range(3)])
        logits = mx.array([apply_top_p(logits[i], float(top_ps[i])) for i in range(3)])
        logits = mx.array(
            [apply_min_p(logits[i].reshape(1, -1), float(min_ps[i])).reshape(-1) for i in range(3)]
        )
        logits = logits / temperatures.reshape(-1, 1)
        next_token_ids_ref = mx.random.categorical(logits)

        mx.allclose(batch_next_token_ids, next_token_ids_ref)


if __name__ == "__main__":
    unittest.main()


================================================================================
File: tests/test_server_args.py
Size: 5.82 kB
================================================================================

"""
Tests for the server_args module.
"""

import argparse
from unittest.mock import patch

import pytest

from parallax.server.executor.factory import create_executor_config
from parallax.server.server_args import parse_args, validate_args


class TestValidateArgs:
    """Test argument validation functionality."""

    def test_valid_layer_indices(self):
        """Test valid layer indices."""
        args = argparse.Namespace(
            start_layer=0,
            end_layer=10,
            dtype="bfloat16",
            kv_cache_memory_fraction=0.5,
            max_batch_size=16,
            max_num_tokens_per_batch=1024,
            kv_block_size=16,
            micro_batch_ratio=2,
            scheduler_wait_ms=500,
        )

        # Should not raise any exception
        validate_args(args)

    def test_invalid_start_layer(self):
        """Test invalid start layer."""
        args = argparse.Namespace(
            start_layer=-1,
            end_layer=10,
            dtype="bfloat16",
            kv_cache_memory_fraction=0.5,
            max_batch_size=16,
            max_num_tokens_per_batch=1024,
            kv_block_size=16,
            micro_batch_ratio=2,
            scheduler_wait_ms=500,
        )

        with pytest.raises(ValueError, match="start_layer must be non-negative"):
            validate_args(args)

    def test_invalid_end_layer(self):
        """Test invalid end layer."""
        args = argparse.Namespace(
            start_layer=10,
            end_layer=5,
            dtype="bfloat16",
            kv_cache_memory_fraction=0.5,
            max_batch_size=16,
            max_num_tokens_per_batch=1024,
            kv_block_size=16,
            micro_batch_ratio=2,
            scheduler_wait_ms=500,
        )

        with pytest.raises(ValueError, match="end_layer must be greater than start_layer"):
            validate_args(args)


class TestCreateExecutorConfig:
    """Test executor configuration creation."""

    def test_create_config(self):
        """Test creating executor configuration."""
        args = argparse.Namespace(
            model_path="mlx-community/Qwen3-0.6B-bf16",
            start_layer=0,
            end_layer=10,
            dtype="float16",
            gpu_backend="sglang",
            max_sequence_length=2048,
            max_batch_size=8,
            kv_block_size=64,
            kv_cache_memory_fraction=0.8,
            enable_prefix_cache=False,
            max_num_tokens_per_batch=1024,
            prefill_priority=0,
            micro_batch_ratio=2,
            scheduler_wait_ms=500,
            send_to_peer_addr=None,
            recv_from_peer_addr=None,
            executor_input_ipc="ipc://test_input",
            executor_output_ipc="ipc://test_output",
            attention_backend="flashinfer",
            moe_runner_backend="auto",
            tp_rank=0,
            tp_size=1,
            nccl_port=4000,
            use_hfcache=False,
            enable_lora=False,
            max_lora_rank=None,
            lora_target_modules=None,
            lora_paths=None,
            max_loras_per_batch=1,
            max_loaded_loras=8,
            lora_eviction_policy="lru",
            lora_backend="triton",
            max_lora_chunk_size=128,
        )
        args = argparse.Namespace(
            model_path="mlx-community/Qwen3-0.6B-bf16",
            start_layer=0,
            end_layer=14,
            dtype="bfloat16",
            max_batch_size=16,
            kv_block_size=16,
            kv_cache_memory_fraction=0.8,
            max_num_tokens_per_batch=1024,
            prefill_priority=0,
            micro_batch_ratio=2,
            scheduler_wait_ms=500,
            enable_prefix_cache=True,
            executor_input_ipc="///ipc/1",
            executor_output_ipc="///ipc/2",
            attention_backend="torch_native",
            moe_runner_backend="auto",
            tp_rank=0,
            tp_size=1,
            nccl_port=4001,
            use_hfcache=False,
            enable_lora=False,
            max_lora_rank=None,
            lora_target_modules=None,
            lora_paths=None,
            max_loras_per_batch=1,
            max_loaded_loras=8,
            lora_eviction_policy="lru",
            lora_backend="triton",
            max_lora_chunk_size=128,
        )

        config = create_executor_config(args)

        assert config["model_repo"] == "mlx-community/Qwen3-0.6B-bf16"
        assert config["start_layer"] == 0
        assert config["end_layer"] == 14
        assert config["dtype"] == "bfloat16"
        assert config["kv_cache_memory_fraction"] == 0.8


class TestParseArgs:
    """Test argument parsing with mocked sys.argv."""

    @patch(
        "sys.argv",
        [
            "test_server_args.py",
            "--model-path",
            "mlx-community/Qwen3-0.6B-bf16",
            "--start-layer",
            "0",
            "--end-layer",
            "14",
            "--dtype",
            "bfloat16",
            "--kv-cache-memory-fraction",
            "0.8",
            "--max-sequence-length",
            "1024",
        ],
    )
    def test_parse_valid_args(self):
        """Test parsing valid arguments."""
        args = parse_args()

        assert args.model_path == "mlx-community/Qwen3-0.6B-bf16"
        assert args.start_layer == 0
        assert args.end_layer == 14
        assert args.dtype == "bfloat16"
        assert args.kv_cache_memory_fraction == 0.8

    @patch(
        "sys.argv",
        ["test_server_args.py", "--model-path", "test", "--start-layer", "10", "--end-layer", "5"],
    )
    def test_parse_invalid_args(self):
        """Test parsing invalid arguments."""
        with pytest.raises(ValueError, match="end_layer must be greater than start_layer"):
            parse_args()


================================================================================
File: tests/test_shard_loader.py
Size: 7.66 kB
================================================================================

"""
Tests for the shard_loader module.
"""

from unittest.mock import Mock, patch

from parallax.server.shard_loader import MLXModelLoader


class TestMLXModelLoader:
    """Test MLXModelLoader functionality."""

    def test_register_block_class_success(self):
        """Test successful registration of block classes from models directory."""
        loader = MLXModelLoader("test_model_path")

        # Check that block_class_map is populated
        assert hasattr(loader, "block_class_map")
        assert isinstance(loader.block_class_map, dict)

        # Check that expected architectures are registered
        expected_architectures = ["Qwen2ForCausalLM", "Qwen3ForCausalLM"]
        for architecture in expected_architectures:
            assert architecture in loader.block_class_map
            assert hasattr(loader.block_class_map[architecture], "get_architecture")
            assert loader.block_class_map[architecture].get_architecture() == architecture

    def test_register_block_class_with_missing_get_architecture(self):
        """Test registration when EntryClass doesn't have get_architecture method."""
        # Create a mock module with EntryClass but no get_architecture method
        mock_entry_class = Mock()
        mock_entry_class.__name__ = "TestBlock"
        # Don't add get_architecture method to mock_entry_class

        mock_module = Mock()
        mock_module.EntryClass = mock_entry_class

        with patch(
            "parallax.server.shard_loader.importlib.import_module", return_value=mock_module
        ):
            with patch("parallax.server.shard_loader.pathlib.Path.glob") as mock_glob:
                # Mock a single model file
                mock_file = Mock()
                mock_file.name = "test_model.py"
                mock_file.stem = "test_model"
                mock_glob.return_value = [mock_file]

                # This should not raise an exception, just log a warning
                loader = MLXModelLoader("test_model_path")
                # The mock will have a get_architecture method by default, so we need to check differently
                # The test should verify that the method exists but doesn't have the expected behavior
                assert (
                    len(loader.block_class_map) >= 0
                )  # At least 0 (could be more due to real models)

    def test_register_block_class_with_no_entry_class(self):
        """Test registration when module doesn't have EntryClass."""
        mock_module = Mock()
        # Don't add EntryClass to mock_module

        with patch(
            "parallax.server.shard_loader.importlib.import_module", return_value=mock_module
        ):
            with patch("parallax.server.shard_loader.pathlib.Path.glob") as mock_glob:
                # Mock a single model file
                mock_file = Mock()
                mock_file.name = "no_entry_model.py"
                mock_file.stem = "no_entry_model"
                mock_glob.return_value = [mock_file]

                # This should not raise an exception, just skip the module
                loader = MLXModelLoader("test_model_path")
                # The real models will still be loaded, so we can't assert empty map
                # Instead, verify that the loader was created successfully
                assert hasattr(loader, "block_class_map")

    def test_register_block_class_excludes_init_py(self):
        """Test that __init__.py files are excluded from registration."""
        with patch("parallax.server.shard_loader.pathlib.Path.glob") as mock_glob:
            # Mock files including __init__.py
            mock_init_file = Mock()
            mock_init_file.name = "__init__.py"
            mock_init_file.stem = "__init__"

            mock_model_file = Mock()
            mock_model_file.name = "test_model.py"
            mock_model_file.stem = "test_model"

            mock_glob.return_value = [mock_init_file, mock_model_file]

            # Mock successful import for the model file
            mock_entry_class = Mock()
            mock_entry_class.__name__ = "TestBlock"
            mock_entry_class.get_architecture.return_value = "TestArchitecture"

            mock_module = Mock()
            mock_module.EntryClass = mock_entry_class

            with patch(
                "parallax.server.shard_loader.importlib.import_module", return_value=mock_module
            ):
                loader = MLXModelLoader("test_model_path")
                # Should only register the non-__init__.py file
                assert "TestArchitecture" in loader.block_class_map

    def test_register_block_class_architecture_mapping(self):
        """Test that architecture names are correctly mapped to EntryClass."""
        loader = MLXModelLoader("test_model_path")

        # Test Qwen2 architecture
        if "Qwen2ForCausalLM" in loader.block_class_map:
            qwen2_class = loader.block_class_map["Qwen2ForCausalLM"]
            assert qwen2_class.get_architecture() == "Qwen2ForCausalLM"

        # Test Qwen3 architecture
        if "Qwen3ForCausalLM" in loader.block_class_map:
            qwen3_class = loader.block_class_map["Qwen3ForCausalLM"]
            assert qwen3_class.get_architecture() == "Qwen3ForCausalLM"

    def test_register_block_class_multiple_models(self):
        """Test registration with multiple model files."""
        # This test verifies that multiple models can be registered
        loader = MLXModelLoader("test_model_path")

        # Should have registered at least the expected models
        registered_architectures = list(loader.block_class_map.keys())
        assert len(registered_architectures) >= 0  # At least 0 (could be more in future)

        # Each registered architecture should have a valid EntryClass
        for architecture, entry_class in loader.block_class_map.items():
            assert hasattr(entry_class, "get_architecture")
            assert entry_class.get_architecture() == architecture

    def test_register_block_class_initialization(self):
        """Test that register_block_class is called during initialization."""
        with patch.object(MLXModelLoader, "register_block_class") as mock_register:
            MLXModelLoader("test_model_path")
            mock_register.assert_called_once()

    def test_register_block_class_empty_models_directory(self):
        """Test registration when models directory is empty."""
        with patch("parallax.server.shard_loader.pathlib.Path.glob", return_value=[]):
            loader = MLXModelLoader("test_model_path")
            assert not loader.block_class_map

    def test_register_block_class_with_exception_in_get_architecture(self):
        """Test registration when get_architecture method raises an exception."""
        mock_entry_class = Mock()
        mock_entry_class.__name__ = "TestBlock"
        mock_entry_class.get_architecture.side_effect = Exception("Test exception")

        mock_module = Mock()
        mock_module.EntryClass = mock_entry_class

        with patch(
            "parallax.server.shard_loader.importlib.import_module", return_value=mock_module
        ):
            with patch("parallax.server.shard_loader.pathlib.Path.glob") as mock_glob:
                # Mock a single model file
                mock_file = Mock()
                mock_file.name = "exception_model.py"
                mock_file.stem = "exception_model"
                mock_glob.return_value = [mock_file]

                # This should not raise an exception, just log a warning
                loader = MLXModelLoader("test_model_path")
                assert not loader.block_class_map

